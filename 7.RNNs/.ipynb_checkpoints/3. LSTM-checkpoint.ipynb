{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T01:19:36.267283Z",
     "start_time": "2020-11-07T01:19:36.247685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T01:19:36.376163Z",
     "start_time": "2020-11-07T01:19:36.373941Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T01:19:42.307969Z",
     "start_time": "2020-11-07T01:19:42.104757Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from data_prepare import load_data_jay_song, data_iter_random, data_iter_consecutive, to_onehot\n",
    "from model_train import train_rnn, train_rnn_pytorch\n",
    "from set_seed import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T01:56:11.636108Z",
     "start_time": "2020-11-07T01:56:11.625273Z"
    }
   },
   "source": [
    "LSTM有输入门，遗忘门，输出门\n",
    "- 输入门：$i_t = \\delta(x_tw_{xi} + h_{t-1}w_{hi} + b_i)$\n",
    "- 遗忘门：$f_t = \\delta(x_tw_{xf} + h_{t-1}w_{hf} + b_f)$\n",
    "- 候选元胞状态：$\\widetilde c = tanh(x_tw_{xc} + h_{t-1}w_{hc} + b_c)$\n",
    "- 元胞状态：$c_t = i_t*\\widetilde c + o_t*c_{t-1}$\n",
    "- 输出门：$o_t = \\delta(x_tw_{xo} + h_{t-1}w_{ho} + b_o)$\n",
    "- 隐藏层状态：$h_t = o_t*tanh(c_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T02:36:56.572748Z",
     "start_time": "2020-11-07T02:36:56.557315Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_params(input_num, hidden_num, output_num, device):\n",
    "    def _ones(shape):\n",
    "        weight = nn.Parameter(torch.normal(0, 0.01, size=shape, device=device), requires_grad=True)\n",
    "        return weight\n",
    "    \n",
    "    def _zeros(shape):\n",
    "        bias = nn.Parameter(torch.zeros(shape, device=device), requires_grad=True)\n",
    "        return bias\n",
    "    \n",
    "    def _three():\n",
    "        return (\n",
    "            _ones((input_num, hidden_num)),\n",
    "            _ones((hidden_num, hidden_num)),\n",
    "            _zeros(hidden_num)\n",
    "        )\n",
    "    \n",
    "    # 输入门/遗忘门/输出门\n",
    "    w_xi, w_hi, b_i = _three()\n",
    "    w_xf, w_hf, b_f = _three()\n",
    "    w_xo, w_ho, b_o = _three()\n",
    "    # 元胞状态\n",
    "    w_xc, w_hc, b_c = _three()\n",
    "    # 输出层\n",
    "    w_hy = _ones((hidden_num, output_num))\n",
    "    b_y = _zeros(output_num)\n",
    "    \n",
    "    return nn.ParameterList([w_xi, w_hi, b_i, w_xf, w_hf, b_f, w_xo, w_ho, b_o, w_xc, w_hc, b_c, w_hy, b_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T02:43:56.458220Z",
     "start_time": "2020-11-07T02:43:56.448119Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def init_hidden_state(batch_size, hidden_num, device):\n",
    "    return (torch.zeros(batch_size, hidden_num, device=device), \n",
    "            torch.zeros(batch_size, hidden_num, device=device))\n",
    "\n",
    "\n",
    "def lstm(inputs, h_state, params):\n",
    "    w_xi, w_hi, b_i, w_xf, w_hf, b_f, w_xo, w_ho, b_o, w_xc, w_hc, b_c, w_hy, b_y = params\n",
    "    outputs = []\n",
    "    h, c = h_state\n",
    "    \n",
    "    # inputs.shape is (num_step, batch_size, vocab_size)\n",
    "    for x in inputs:\n",
    "        it = torch.sigmoid(torch.mm(x, w_xi) + torch.mm(h, w_hi) + b_i)\n",
    "        ft = torch.sigmoid(torch.mm(x, w_xf) + torch.mm(h, w_hf) + b_f)\n",
    "        ot = torch.sigmoid(torch.mm(x, w_xo) + torch.mm(h, w_ho) + b_o)\n",
    "        c_candidate = torch.tanh(torch.mm(x, w_xc) + torch.mm(h, w_hc) + b_c)\n",
    "        c = it*c_candidate + ft*c\n",
    "        h = ot*torch.tanh(c)\n",
    "        y = torch.mm(h, w_hy) + b_y\n",
    "        outputs.append(y.unsqueeze(0))\n",
    "        \n",
    "    return reduce(lambda x, y: torch.cat((x, y)), outputs), (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T02:43:56.999964Z",
     "start_time": "2020-11-07T02:43:56.974782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2, 15]), torch.Size([2, 10]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证\n",
    "hidden_num, vocab_size, device = 10, 15, \"cuda\"\n",
    "x = torch.arange(10).view(2, 5)\n",
    "inputs = to_onehot(x, vocab_size, device)\n",
    "h_state = init_hidden_state(inputs.shape[1], hidden_num, device)\n",
    "params = get_params(vocab_size, hidden_num, vocab_size, device)\n",
    "outputs, h_state = lstm(inputs, h_state, params)\n",
    "# 输出/隐藏状态/元胞状态\n",
    "outputs.shape, h_state[0].shape, h_state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T02:45:17.466245Z",
     "start_time": "2020-11-07T02:45:17.454431Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, pred_num, model, init_hidden_state, hidden_num, \n",
    "                params, char_to_idx, vocab_set, vocab_size, device):\n",
    "    outputs = [char_to_idx[prefix[0]]]\n",
    "    h_state = init_hidden_state(1, hidden_num, device)\n",
    "    \n",
    "    for i in range(len(prefix) + pred_num - 1):\n",
    "        # inputs.shape is (1, 1, vocab_size)\n",
    "        inputs = to_onehot(torch.tensor(outputs[-1]).view(-1, 1), vocab_size, device)\n",
    "        # y.shape is (1, 1, vocab_size), h_state.shape is (1, hidden_num)\n",
    "        y, h_state = model(inputs, h_state, params)\n",
    "        \n",
    "        if i + 1 < len(prefix):\n",
    "            outputs.append(char_to_idx[prefix[i+1]])\n",
    "        else:\n",
    "            outputs.append(y.argmax(dim=2).item())\n",
    "            \n",
    "    return \"\".join(vocab_set[i] for i in outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T02:45:42.933491Z",
     "start_time": "2020-11-07T02:45:42.877632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开顽繁曲狈耿跡掏狞墙台'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证\n",
    "# load data\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "# params\n",
    "hidden_num, device = 256, \"cuda\"\n",
    "params = get_params(vocab_size, hidden_num, vocab_size, device)\n",
    "predict_rnn(\"分开\", 10, lstm, init_hidden_state, hidden_num, params, char_to_idx, vocab_set, vocab_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T03:13:04.225633Z",
     "start_time": "2020-11-07T02:59:07.724990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "989/989 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 345.9668, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开 我们的爱情 我的爱爱不出 我的爱爱不出 我的爱爱不出 我的爱爱不出 我的爱爱不出 我的爱爱不出 我\n",
      "prefix-不分开:  不分开 我们的爱情 我的爱爱不出 我的爱爱不出 我的爱爱不出 我的爱爱不出 我的爱爱不出 我的爱爱不出 我\n",
      "\n",
      "\n",
      "Epoch [2/5]\n",
      "989/989 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 231.9450, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我\n",
      "prefix-不分开:  不分开 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我们的感觉 我\n",
      "\n",
      "\n",
      "Epoch [3/5]\n",
      "989/989 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 166.0830, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开 我不能够想 我不要再想 我不能再想 我不要我 我不要我 我不要我 我不要我 我不要我 我不要我 我\n",
      "prefix-不分开:  不分开 我用第一人称 在我的等待 你说我爱你 你说我爱你 你说我爱你 你说我爱你 你说我爱你 你说我爱你 \n",
      "\n",
      "\n",
      "Epoch [4/5]\n",
      "989/989 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 125.3085, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开 我不能再想 我不能再想 我不再再想 我不再再想 我不再再想 我不再再想 我不再再想 我不再再想 我\n",
      "prefix-不分开:  不分开 我知道你不要我  我不再再想 我不是我要的天堂景象 你只会感到更加 我不想            \n",
      "\n",
      "\n",
      "Epoch [5/5]\n",
      "989/989 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 99.4540, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "prefix-分开:  分开 我不能再想 我不要我想 我不要我 我我我 我我我 我我我 我我我 我我我 我我我 我我我 我我我 \n",
      "prefix-不分开:  不分开 我会发著一个人 我不能就这样牵着我 不要我 我我我 我我我 我不是我不要我 我我我 我我我 我我我\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "\n",
    "super_params = {\n",
    "        \"epoch_num\": 5,\n",
    "        \"rnn\": lstm,\n",
    "        \"loss\": nn.CrossEntropyLoss(),\n",
    "        \"init_hidden_state\": init_hidden_state,\n",
    "        \"hidden_num\": 256,\n",
    "        \"get_params\": get_params,\n",
    "        \"batch_size\": 2,\n",
    "        \"num_step\": 32,\n",
    "        \"corpus_index\": corpus_index,\n",
    "        \"data_iter\": data_iter_random,\n",
    "        \"lr\": 100,\n",
    "        \"char_to_idx\": char_to_idx,\n",
    "        \"vocab_set\": vocab_set,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"predict_rnn\": predict_rnn,\n",
    "        \"pred_num\": 50,\n",
    "        \"prefixs\": [\"分开\", \"不分开\"],\n",
    "        #     \"random_sample\": False\n",
    "    }\n",
    "\n",
    "super_params[\"batch_num\"] = len(list(data_iter_random(corpus_index, super_params[\"batch_size\"],\n",
    "                                                      super_params[\"num_step\"], \"cpu\")))\n",
    "\n",
    "train_rnn(**super_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T03:13:04.368281Z",
     "start_time": "2020-11-07T03:13:04.362366Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.hidden_num = self.rnn.hidden_size * (2 if self.rnn.bidirectional else 1)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc = nn.Linear(self.hidden_numden_numden_num, vocab_size)\n",
    "        self.h_state = None\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        y, self.h_state = self.rnn(x, h_state)\n",
    "        return self.fc(y), self.h_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "def train_rnn_pytorch(prefix, pred_num, model, char_to_idx, vocab_size, vocab_set, device):\n",
    "    outputs = [char_to_idx[prefix[0]]]\n",
    "    h_state = None\n",
    "    \n",
    "    for i in range(len(prefix) + pred_num - 1):\n",
    "        inputs = to_onehot(torch.tensor(outputs[-1]).view(-1, 1), vocab_size, device)\n",
    "        if h_state is not None:\n",
    "            if isinstance(h_state, tuple): # lstm , (h,c)\n",
    "                h_state = (h_state[0].to(device), h_state[1].to(device))\n",
    "            else:\n",
    "                h_state = h_state.to(device)\n",
    "                \n",
    "        y, h_state = model(inputs, h_state)\n",
    "        if i + 1 < len(prefix):\n",
    "            outputs.append(char_to_idx[prefix[i+1]])\n",
    "        else:\n",
    "            outputs.append(y.argmax(dim=2).item())\n",
    "            \n",
    "    return \"\".join(vocab_set[i] for i in outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T03:13:04.449479Z",
     "start_time": "2020-11-07T03:13:04.370792Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_rnn_pytorch() missing 8 required positional arguments: 'num_step', 'batch_size', 'char_to_idx', 'vocab_set', 'vocab_size', 'prefixs', 'pred_num', and 'predict_rnn_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e790934b68e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_rnn_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"分开\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train_rnn_pytorch() missing 8 required positional arguments: 'num_step', 'batch_size', 'char_to_idx', 'vocab_set', 'vocab_size', 'prefixs', 'pred_num', and 'predict_rnn_pytorch'"
     ]
    }
   ],
   "source": [
    "# 验证\n",
    "# load data\n",
    "hidden_num = 256\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "rnn_layer = nn.GRU(vocab_size, hidden_num)\n",
    "model = RNNModel(rnn_layer, vocab_size)\n",
    "model = model.cuda()\n",
    "train_rnn_pytorch(\"分开\", 10, model, char_to_idx, vocab_size, vocab_set, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_num = 256\n",
    "rnn_layer = nn.LSTM(vocab_size, hidden_num)\n",
    "model = RNNModel(rnn_layer, vocab_size)\n",
    "model = model.cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "params = {\n",
    "    \"epoch_num\": 250,\n",
    "    \"model\": model,\n",
    "    \"loss\": loss,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_step\": 32,\n",
    "    \"corpus_index\": corpus_index,\n",
    "    \"data_iter\": data_iter_consecutive,\n",
    "    \"char_to_idx\": char_to_idx,\n",
    "    \"vocab_set\": vocab_set,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"predict_rnn_pytorch\": predict_rnn_pytorch,\n",
    "    \"pred_num\": 50,\n",
    "    \"prefixs\": [\"分开\", \"不分开\"],\n",
    "    \"random_sample\": False\n",
    "}\n",
    "\n",
    "params[\"batch_num\"] = len(list(data_iter_consecutive(corpus_index, params[\"batch_size\"],\n",
    "                                                     params[\"num_step\"], \"cpu\")))\n",
    "\n",
    "train_rnn_pytorch(**params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
