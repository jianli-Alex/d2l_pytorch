{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T00:52:02.227868Z",
     "start_time": "2020-11-07T00:52:02.200260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T00:52:02.390307Z",
     "start_time": "2020-11-07T00:52:02.387987Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T00:52:03.236821Z",
     "start_time": "2020-11-07T00:52:02.533525Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from data_prepare import load_data_jay_song, data_iter_random, data_iter_consecutive, to_onehot\n",
    "from model_train import train_rnn, train_rnn_pytorch\n",
    "from set_seed import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T13:40:25.268476Z",
     "start_time": "2020-11-06T13:40:25.257283Z"
    }
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当时间步过小或者过大时，RNN往往会出现梯度爆炸或者梯度消失的问题\n",
    "- 梯度爆炸：可以通过梯度剪裁来解决\n",
    "- 梯度消失：可以通过GRU/LSTM等来进行缓解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T13:45:48.219163Z",
     "start_time": "2020-11-06T13:45:48.104938Z"
    }
   },
   "source": [
    "GRU包含重置门和更新门\n",
    "- 重置门用于捕捉短期依赖关系\n",
    "- 更新门用于捕捉长期依赖关系\n",
    "- 重置门：$r_t = \\delta(x_tw_{xr} + h_{t-1}w_{hr} + b_r)$\n",
    "- 更新门：$z_t = \\delta(x_tw_{xz} + h_{t-1}w_{hz} + b_z)$\n",
    "- 候选隐藏层状态：$\\widetilde h_t = tanh(x_tw_{xh} + r_t*(h_{t-1}w_hh) + b_h)$\n",
    "- 当前时间步的隐藏层：$h_t = z*h_{t-1} + (1-z)\\widetilde h_t$\n",
    "- 输出层：$y_t = h_tw_{hy} + b_y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T15:44:02.915391Z",
     "start_time": "2020-11-06T15:44:02.910747Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_params(input_num, hidden_num, output_num, device):\n",
    "    def _ones(shape):\n",
    "        weight = nn.Parameter(torch.normal(0, 0.01, size=shape, device=device), requires_grad=True)\n",
    "        return weight\n",
    "    \n",
    "    def _zeros(shape):\n",
    "        bias = nn.Parameter(torch.zeros(shape, device=device), requires_grad=True)\n",
    "        return bias\n",
    "    \n",
    "    def _three():\n",
    "        return (\n",
    "            _ones((input_num, hidden_num)),\n",
    "            _ones((hidden_num, hidden_num)),\n",
    "            _zeros(hidden_num),\n",
    "        )\n",
    "    \n",
    "    w_xr, w_hr, b_r = _three()\n",
    "    w_xz, w_hz, b_z = _three()\n",
    "    w_xh, w_hh, b_h = _three()\n",
    "    w_hy = _ones((hidden_num, output_num))\n",
    "    b_y = _zeros((output_num))\n",
    "    return nn.ParameterList([w_xr, w_hr, b_r, w_xz, w_hz, b_z, w_xh, w_hh, b_h, w_hy, b_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义gru层结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T15:44:03.119498Z",
     "start_time": "2020-11-06T15:44:03.114177Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def init_hidden_state(batch_size, hidden_num, device):\n",
    "    return torch.zeros(batch_size, hidden_num, device=device)\n",
    "\n",
    "\n",
    "def gru(inputs, h_state, params):\n",
    "    w_xr, w_hr, b_r, w_xz, w_hz, b_z, w_xh, w_hh, b_h, w_hy, b_y = params\n",
    "    outputs = []\n",
    "    \n",
    "    # inputs.shape is (num_step, batch_size, vocab_size)\n",
    "    for x in inputs:\n",
    "        rt = torch.sigmoid(torch.mm(x, w_xr) + torch.mm(h_state, w_hr) + b_r)\n",
    "        zt = torch.sigmoid(torch.mm(x, w_xz) + torch.mm(h_state, w_hz) + b_z)\n",
    "        h_candidate = torch.tanh(torch.mm(x, w_xh) + rt*torch.mm(h_state, w_hh) + b_h)\n",
    "        h_state = zt*h_state + (1-zt)*h_candidate\n",
    "        y = torch.mm(h_state, w_hy) + b_y\n",
    "        outputs.append(y.unsqueeze(0))\n",
    "        \n",
    "    return reduce(lambda x, y: torch.cat((x, y)), outputs), h_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T15:43:19.940007Z",
     "start_time": "2020-11-06T15:43:17.958695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2, 15]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证\n",
    "vocab_size, hidden_num = 15, 10\n",
    "x = torch.arange(10).view(2, 5)\n",
    "inputs = to_onehot(x, vocab_size, \"cuda\")\n",
    "h_state = init_hidden_state(x.shape[0], hidden_num, \"cuda\")\n",
    "params = get_params(vocab_size, hidden_num, vocab_size, \"cuda\")\n",
    "outputs, h_state = gru(inputs, h_state, params)\n",
    "outputs.shape, h_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 字符级别预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T15:44:07.085579Z",
     "start_time": "2020-11-06T15:44:07.078199Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, pred_num, model, init_hidden_state, hidden_num, \n",
    "                params, char_to_idx, vocab_set, vocab_size, device):\n",
    "    outputs = [char_to_idx[prefix[0]]]\n",
    "    h_state = init_hidden_state(1, hidden_num, device)\n",
    "    \n",
    "    for i in range(len(prefix) + pred_num - 1):\n",
    "        # inputs.shape is (1, 1, vocab_size)\n",
    "        inputs = to_onehot(torch.tensor(outputs[-1]).view(-1, 1), vocab_size, device)\n",
    "        # y.shape is (1, 1, vocab_size), h_state.shape is (1, hidden_num)\n",
    "        y, h_state = model(inputs, h_state, params)\n",
    "        \n",
    "        if i + 1 < len(prefix):\n",
    "            outputs.append(char_to_idx[prefix[i+1]])\n",
    "        else:\n",
    "            outputs.append(y.argmax(dim=2).item())\n",
    "            \n",
    "    return \"\".join(vocab_set[i] for i in outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T15:43:53.489202Z",
     "start_time": "2020-11-06T15:43:51.620610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开鐘鐘真脚脚典盖起现现'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证\n",
    "# load data\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "# params\n",
    "hidden_num, device = 256, \"cuda\"\n",
    "params = get_params(vocab_size, hidden_num, vocab_size, device)\n",
    "predict_rnn(\"分开\", 10, gru, init_hidden_state, hidden_num, params, char_to_idx, vocab_set, vocab_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度剪裁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T15:44:09.658064Z",
     "start_time": "2020-11-06T15:44:09.654659Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad_clipping(params, clipping_theta, device):\n",
    "    # l2 norm\n",
    "    norm = torch.zeros(1, device=device)\n",
    "    # cumsum all grad data\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "        \n",
    "    norm = norm.sqrt()\n",
    "    \n",
    "    # grad explode\n",
    "    if norm > clipping_theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (clipping_theta / norm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T15:44:09.845912Z",
     "start_time": "2020-11-06T15:44:09.837582Z"
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "import numpy as np\n",
    "from sqdm import sqdm\n",
    "from optim import sgd\n",
    "\n",
    "def train_rnn(epoch_num, batch_num, model, loss, get_params, init_hidden_state, hidden_num, batch_size,\n",
    "              lr, data_iter, prefixs, pred_num, corpus_index, char_to_idx, vocab_set, vocab_size, \n",
    "              num_step, predict_rnn, clipping_theta=1e-2, random_sample=True, device=\"cuda\"):\n",
    "    \n",
    "    # init(use in calculate perplexity)\n",
    "    l_sum, n_class = 0., 0.\n",
    "    # training bar\n",
    "    process_bar = sqdm()\n",
    "    # init params\n",
    "    params = get_params(vocab_size, hidden_num, vocab_size, device)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        # sample in consecutive\n",
    "        if not random_sample:\n",
    "            h_state = init_hidden_state(batch_size, hidden_num, device)\n",
    "        for x, y in data_iter(corpus_index, batch_size, vocab_size, device):\n",
    "            # 原始x的shape为(batch_size, num_step)，onehot后的shape为(num_step, batch_size, vocab_size)\n",
    "            x = to_onehot(x, vocab_size, device)\n",
    "            if random_sample:\n",
    "                h_state = init_hidden_state(batch_size, hidden_num, device)\n",
    "            else:\n",
    "                # 脱离计算图，使得上一时刻的隐藏状态变成叶子节点，防止在销毁计算图后（隐藏节点还存在），因反向传播到更早的\n",
    "                # 隐藏层时刻（不在当前计算图内）而出错\n",
    "                h_state.detach_()\n",
    "                \n",
    "            # model\n",
    "            # outputs.shape is (num_step, batch_size, vocab_size), h_state.shape is (batch_size, hidden_num)\n",
    "            outputs, h_state = model(x, h_state, params)\n",
    "            # change output.shape --> (num_step, batch_size, vocab_size), 主要是为了方便计算loss\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            # 原始y的shape为(batch_size, num_step), ---> (num_step, batch_size) ---> 1维向量\n",
    "            # 转置后变成内存不连续，使用contiguous变成连续的向量\n",
    "            y = y.transpose(0, 1).contiguous().view(-1)\n",
    "            \n",
    "            # 计算loss, 标签需要是int\n",
    "            l = loss(outputs, y.long())\n",
    "            # grad clear\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            # grad backward\n",
    "            l.backward()\n",
    "            # grad clipping\n",
    "            grad_clipping(params, clipping_theta, device)\n",
    "            # update grad\n",
    "            sgd(params, lr)\n",
    "            \n",
    "            # calculate l_sum\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n_class += y.shape[0]\n",
    "            \n",
    "            # calculate perplexity\n",
    "            try:\n",
    "                perplexity = np.exp(l_sum / n_class)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "                \n",
    "            # training bar\n",
    "            process_bar.show_process(batch_num, 1, perplexity)\n",
    "        print(\"\\n\")\n",
    "        # predict\n",
    "        for prefix in prefixs:\n",
    "            print(f\"prefix-{prefix}: \", predict_rnn(prefix, pred_num, model, init_hidden_state, hidden_num, \n",
    "                                                    params, char_to_idx, vocab_set, vocab_size, device))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T15:49:11.938053Z",
     "start_time": "2020-11-06T15:44:09.997258Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "12/989 [------------------------------] - train_loss: 1244.3197, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开      的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的\n",
      "prefix-不分开:  不分开     的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 的的 \n",
      "\n",
      "\n",
      "Epoch [2/5]\n",
      "21/989 [------------------------------] - train_loss: 998.7580, train_score: -, test_loss: -, test_score: --\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-63b4ff31dcc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                       super_params[\"num_step\"], \"cpu\")))\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msuper_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b0c61d3b2dce>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(epoch_num, batch_num, model, loss, get_params, init_hidden_state, hidden_num, batch_size, lr, data_iter, prefixs, pred_num, corpus_index, char_to_idx, vocab_set, vocab_size, num_step, predict_rnn, clipping_theta, random_sample, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# grad backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;31m# grad clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mgrad_clipping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load data\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "\n",
    "super_params = {\n",
    "        \"epoch_num\": 5,\n",
    "        \"model\": gru,\n",
    "        \"loss\": nn.CrossEntropyLoss(),\n",
    "        \"init_hidden_state\": init_hidden_state,\n",
    "        \"hidden_num\": 256,\n",
    "        \"get_params\": get_params,\n",
    "        \"batch_size\": 2,\n",
    "        \"num_step\": 32,\n",
    "        \"corpus_index\": corpus_index,\n",
    "        \"data_iter\": data_iter_random,\n",
    "        \"lr\": 100,\n",
    "        \"char_to_idx\": char_to_idx,\n",
    "        \"vocab_set\": vocab_set,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"predict_rnn\": predict_rnn,\n",
    "        \"pred_num\": 50,\n",
    "        \"prefixs\": [\"分开\", \"不分开\"],\n",
    "        #     \"random_sample\": False\n",
    "    }\n",
    "\n",
    "super_params[\"batch_num\"] = len(list(data_iter_random(corpus_index, super_params[\"batch_size\"],\n",
    "                                                      super_params[\"num_step\"], \"cpu\")))\n",
    "\n",
    "train_rnn(**super_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T00:52:07.892558Z",
     "start_time": "2020-11-07T00:52:07.885362Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.hidden_num = self.rnn.hidden_size * (2 if self.rnn.bidirectional else 1)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc = nn.Linear(hidden_num, vocab_size)\n",
    "        self.h_state = None\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        # x.shape is (num_step, batch_size, vocab_size)\n",
    "        y, self.h_state = self.rnn(x, h_state)\n",
    "        return self.fc(y), self.h_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T01:10:41.992945Z",
     "start_time": "2020-11-07T01:10:41.971574Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_rnn_pytorch(prefix, pred_num, model, char_to_idx, vocab_size, vocab_set, device):\n",
    "    outputs = [char_to_idx[prefix[0]]]\n",
    "    h_state = None\n",
    "    \n",
    "    for i in range(len(prefix) + pred_num - 1):\n",
    "        inputs = to_onehot(torch.tensor(outputs[-1]).view(-1, 1), vocab_size, device)\n",
    "        if h_state is not None:\n",
    "            if isinstance(h_state, tuple): # lstm , (h,c)\n",
    "                h_state = (h_state[0].to(device), h_state[1].to(device))\n",
    "            else:\n",
    "                h_state = h_state.to(device)\n",
    "                \n",
    "        y, h_state = model(inputs, h_state)\n",
    "        if i + 1 < len(prefix):\n",
    "            outputs.append(char_to_idx[prefix[i+1]])\n",
    "        else:\n",
    "            outputs.append(y.argmax(dim=2).item())\n",
    "            \n",
    "    return \"\".join(vocab_set[i] for i in outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T01:10:42.213383Z",
     "start_time": "2020-11-07T01:10:42.141580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开八斤斤野宝斤宝苛赞赞'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证\n",
    "# load data\n",
    "hidden_num = 256\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "rnn_layer = nn.GRU(vocab_size, hidden_num)\n",
    "model = RNNModel(rnn_layer, vocab_size)\n",
    "model = model.cuda()\n",
    "train_rnn_pytorch(\"分开\", 10, model, char_to_idx, vocab_size, vocab_set, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T01:13:39.450149Z",
     "start_time": "2020-11-07T01:13:39.440401Z"
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "import numpy as np\n",
    "from sqdm import sqdm\n",
    "from optim import sgd\n",
    "\n",
    "def train_rnn_pytorch(epoch_num, batch_num, model, loss, optimizer, batch_size, lr, data_iter, prefixs, \n",
    "                      pred_num, corpus_index, char_to_idx, vocab_set, vocab_size, num_step, predict_rnn_pytorch, \n",
    "                      clipping_theta=1e-2, random_sample=True, device=\"cuda\"):\n",
    "    \n",
    "    # init(use in calculate perplexity)\n",
    "    l_sum, n_class = 0., 0.\n",
    "    # training bar\n",
    "    process_bar = sqdm()\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        # sample in consecutive\n",
    "        if not random_sample:\n",
    "            h_state = None\n",
    "        for x, y in data_iter(corpus_index, batch_size, vocab_size, device):\n",
    "            # 原始x的shape为(batch_size, num_step)，onehot后的shape为(num_step, batch_size, vocab_size)\n",
    "            x = to_onehot(x, vocab_size, device)\n",
    "            if random_sample:\n",
    "                h_state = None\n",
    "            else:\n",
    "                # 脱离计算图，使得上一时刻的隐藏状态变成叶子节点，防止在销毁计算图后（隐藏节点还存在），因反向传播到更早的\n",
    "                # 隐藏层时刻（不在当前计算图内）而出错\n",
    "                if h_state is not None:\n",
    "                    if isinstance(h_state, tuple): # lstm, state: (h, c)\n",
    "                        h_state = (h_state[0].deatch(), h_state[1].deatch())\n",
    "                    else:\n",
    "                        h_state.detach_()\n",
    "                \n",
    "            # model\n",
    "            # outputs.shape is (num_step, batch_size, vocab_size), h_state.shape is (batch_size, hidden_num)\n",
    "            outputs, h_state = model(x, h_state)\n",
    "            # change output.shape --> (num_step, batch_size, vocab_size), 主要是为了方便计算loss\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            # 原始y的shape为(batch_size, num_step), ---> (num_step, batch_size) ---> 1维向量\n",
    "            # 转置后变成内存不连续，使用contiguous变成连续的向量\n",
    "            y = y.transpose(0, 1).contiguous().view(-1)\n",
    "            \n",
    "            # 计算loss, 标签需要是int\n",
    "            l = loss(outputs, y.long())\n",
    "            # grad clear\n",
    "            optimizer.zero_grad()\n",
    "            # grad backward\n",
    "            l.backward()\n",
    "            # grad clipping\n",
    "            grad_clipping(params, clipping_theta, device)\n",
    "            # update grad\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate l_sum\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n_class += y.shape[0]\n",
    "            \n",
    "            # calculate perplexity\n",
    "            try:\n",
    "                perplexity = np.exp(l_sum / n_class)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "                \n",
    "            # training bar\n",
    "            process_bar.show_process(batch_num, 1, perplexity)\n",
    "        print(\"\\n\")\n",
    "        # predict\n",
    "        for prefix in prefixs:\n",
    "            print(f\"prefix-{prefix}: \", predict_rnn(prefix, pred_num, model, init_hidden_state, hidden_num, \n",
    "                                                    params, char_to_idx, vocab_set, vocab_size, device))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_num = 256\n",
    "rnn_layer = nn.GRU(vocab_size, hidden_num)\n",
    "model = RNNModel(rnn_layer, vocab_size)\n",
    "model = model.cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "params = {\n",
    "    \"epoch_num\": 250,\n",
    "    \"model\": model,\n",
    "    \"loss\": loss,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_step\": 32,\n",
    "    \"corpus_index\": corpus_index,\n",
    "    \"data_iter\": data_iter_consecutive,\n",
    "    \"char_to_idx\": char_to_idx,\n",
    "    \"vocab_set\": vocab_set,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"predict_rnn_pytorch\": predict_rnn_pytorch,\n",
    "    \"pred_num\": 50,\n",
    "    \"prefixs\": [\"分开\", \"不分开\"],\n",
    "    \"random_sample\": False\n",
    "}\n",
    "\n",
    "params[\"batch_num\"] = len(list(data_iter_consecutive(corpus_index, params[\"batch_size\"],\n",
    "                                                     params[\"num_step\"], \"cpu\")))\n",
    "\n",
    "train_rnn_pytorch(**params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
