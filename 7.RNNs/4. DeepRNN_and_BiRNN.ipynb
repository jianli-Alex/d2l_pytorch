{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:30.086336Z",
     "start_time": "2020-11-14T15:50:30.066804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:30.154208Z",
     "start_time": "2020-11-14T15:50:30.151256Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:30.897694Z",
     "start_time": "2020-11-14T15:50:30.315281Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import reduce\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from data_prepare import load_data_jay_song, data_iter_random, data_iter_consecutive, to_onehot\n",
    "from sqdm import sqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:30.901722Z",
     "start_time": "2020-11-14T15:50:30.899019Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "普通的RNNs模型，如果从时间步来看，它是一个深度模型，而如果从纵向来看，它其实是一个浅层的模型。\n",
    "- 因此，可以在纵向上添加多个隐藏层，使得模型变成深度RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![深度RNNs](./img/deep-rnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层的运算和普通的RNN没有什么区别，而后面层的输入则是上一层隐层的输出\n",
    "$$H^{(1)} = \\delta(X_{t}W_{xh}^{(1)} + H_{t-1}^{(1)}W_{hh}^{(1)} + b_h^{(1)})$$\n",
    "$$H^{(l)} = \\delta(H_{t}^{(l-1)}W_{xh}^{(l)} + H_{t-1}^{(l)}W_{hh}^{(l)} + b_h^{(l)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep-RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里实现普通的深层RNNs模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:38.693402Z",
     "start_time": "2020-11-14T15:50:38.688141Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "def get_params(input_num, hidden_num, num_layers, device):\n",
    "    # weight\n",
    "    def _ones(shape):\n",
    "        weight = nn.Parameter(torch.normal(0, 0.01, size=shape, device=device), requires_grad=True)\n",
    "        return weight\n",
    "    \n",
    "    def _zeros(shape):\n",
    "        bias = nn.Parameter(torch.zeros(shape, device=device), requires_grad=True)\n",
    "        return bias\n",
    "    \n",
    "    def _three(input_num, hidden_num):\n",
    "        return [\n",
    "            _ones((input_num, hidden_num)),\n",
    "            _ones((hidden_num, hidden_num)),\n",
    "            _zeros(hidden_num)\n",
    "        ]\n",
    "    \n",
    "    # hidden layer\n",
    "    params = _three(input_num, hidden_num)\n",
    "    for num in range(1, num_layers):\n",
    "        params.extend(_three(hidden_num, hidden_num))\n",
    "        \n",
    "    # outputs layer\n",
    "    params.append(_ones((hidden_num, input_num)))\n",
    "    params.append(_zeros(input_num))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:39.778815Z",
     "start_time": "2020-11-14T15:50:39.774003Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义rnn隐藏层的状态\n",
    "def init_rnn_hidden_state(batch_size, hidden_num, num_layers, device):\n",
    "#     return reduce(lambda x, y: torch.cat((x, y)), \n",
    "#                   [torch.zeros(1, batch_size, hidden_num, device=device)]*num_layers)\n",
    "    return [torch.zeros(batch_size, hidden_num, device=device) for _ in range(num_layers)]\n",
    "\n",
    "# 定义rnn网络计算\n",
    "def deep_rnn(inputs, params, h_state, num_layers, device):\n",
    "    # 隐藏层的输出\n",
    "    outputs = []\n",
    "    \n",
    "    \n",
    "    # inputs.shape is (num_step, batch_size, vocab_size), h_state.shape is (num_layers, batch_size, hidden_num)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        x = inputs[i]\n",
    "        for num in range(num_layers):\n",
    "            h_state[num] = torch.tanh(torch.mm(x, params[3*num]) + \n",
    "                                      torch.mm(h_state[num], params[3*num+1]) + params[3*num+2])\n",
    "            x = h_state[num]\n",
    "        y = torch.mm(x, params[-2] + params[-1])\n",
    "        outputs.append(y.unsqueeze(0))\n",
    "        \n",
    "    return reduce(lambda x,y: torch.cat((x, y)), outputs), h_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:42.140529Z",
     "start_time": "2020-11-14T15:50:40.367678Z"
    }
   },
   "outputs": [],
   "source": [
    "# 验证\n",
    "set_seed(2020)\n",
    "vocab_size, hidden_num, num_layers, device = 15, 20, 2, \"cuda\"\n",
    "inputs = torch.arange(10).view(2, 5)\n",
    "inputs = to_onehot(inputs, vocab_size, device)\n",
    "params = get_params(vocab_size, hidden_num, num_layers, device)\n",
    "h_state = init_rnn_hidden_state(inputs.shape[1], hidden_num, num_layers, device)\n",
    "outputs, h_state = deep_rnn(inputs, params, h_state, num_layers, device)\n",
    "# print(outputs.shape, h_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T07:45:10.479142Z",
     "start_time": "2020-11-14T07:45:10.468644Z"
    }
   },
   "source": [
    "#### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:43.597696Z",
     "start_time": "2020-11-14T15:50:43.591186Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, pred_num, model, init_rnn_hidden_state, hidden_num, num_layers, \n",
    "                params, char_to_idx, vocab_set, vocab_size, device):\n",
    "    # 将词转为index\n",
    "    outputs = [char_to_idx[prefix[0]]]\n",
    "    # 初始化hidden state\n",
    "    h_state = init_rnn_hidden_state(1, hidden_num, num_layers, device)\n",
    "    \n",
    "    for i in range(len(prefix) + pred_num - 1):\n",
    "        # to_onehot接收的shape为(batch_size, time_step), 单独预测时time_step为1\n",
    "        inputs = to_onehot(torch.tensor(outputs[-1]).view(-1, 1), vocab_size, device)\n",
    "        # 预测, y.shape-->(1, 1, vocab_size), h_state.shape-->(num_layers, batch_size, hidden_num)\n",
    "        y, h_state = model(inputs, params, h_state, num_layers, device)\n",
    "        # 添加到outpus\n",
    "        if i + 1 < len(prefix):\n",
    "            outputs.append(char_to_idx[prefix[i+1]])\n",
    "        else:\n",
    "            outputs.append(y.argmax(dim=2).item())\n",
    "    return \"\".join(vocab_set[i] for i in outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:45.229356Z",
     "start_time": "2020-11-14T15:50:45.198257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开诀舍键亲找忽此泛马骑'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "set_seed(2020)\n",
    "hidden_num, pred_num, num_layers, device = 256, 10, 2, \"cuda\"\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "params = get_params(vocab_size, hidden_num, num_layers, device)\n",
    "# 预测\n",
    "predict_rnn(\"分开\", pred_num, deep_rnn, init_rnn_hidden_state, hidden_num, \n",
    "            num_layers, params, char_to_idx, vocab_set, vocab_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:47.011815Z",
     "start_time": "2020-11-14T15:50:47.001088Z"
    }
   },
   "outputs": [],
   "source": [
    "from optim import sgd, grad_clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:50:48.649013Z",
     "start_time": "2020-11-14T15:50:48.634722Z"
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def train_rnn(epoch_num, batch_num, model, loss, init_hidden_state, get_params, data_iter, corpus_index,\n",
    "              num_step, hidden_num, lr, batch_size, char_to_idx, vocab_set, vocab_size, prefixs, num_layers,\n",
    "              predict_rnn, pred_num, clipping_theta=1e-2, random_sample=True, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    function: training and predict in rnn\n",
    "    params epoch_num: the number of epoch\n",
    "    params batch_num: the number of batch in a epoch\n",
    "    params rnn: the rnn model\n",
    "    params loss: such as nn.CrossEntropyLoss()\n",
    "    params init_hidden_state: define the state of hidden layer\n",
    "    params get_params: get the weight and bias in rnn\n",
    "    params data_iter: data_iter_random/data_iter_consecutive\n",
    "    params corpus_index: the index of corpus\n",
    "    params num_step: the number of time step in rnn\n",
    "    params hidden_num: the number of unit in hidden layer in rnn\n",
    "    params lr: the learning rate\n",
    "    params batch_size: the size of a batch\n",
    "    params char_to_idx: char index which convert Chinese to idx\n",
    "    params vocab_set: the list of word in corpus\n",
    "    params vocab_size: the length of vocab_set\n",
    "    params prefixs: the list include input when you want to predict, such as [\"分开\", \"不分开\"]\n",
    "    params pred_num: the number you want to predict\n",
    "    params clipping_heta: the max value of the norm of grad\n",
    "    params random_sample: if sample in random, use data_iter_random. otherwise, use data_iter_consecutive\n",
    "    params device: \"cpu\"/\"cuda\"\n",
    "    \"\"\"\n",
    "    # training bar\n",
    "    process_bar = sqdm()\n",
    "    # init\n",
    "    l_sum, n_class = 0, 0\n",
    "    # get params in rnn\n",
    "    params = get_params(vocab_size, hidden_num, num_layers, device)\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        # sample in consecutive\n",
    "        if not random_sample:\n",
    "            h_state = init_rnn_hidden_state(batch_size, hidden_num, num_layers, device)\n",
    "        print(f\"Epoch [{epoch + 1}/{epoch_num}]\")\n",
    "        for x, y in data_iter(corpus_index, batch_size, num_step, device):\n",
    "            # x shape: (num_step, batch_size, vocab_size)\n",
    "            inputs = to_onehot(x, vocab_size, device)\n",
    "            # if sample with random, init h_state in each batch\n",
    "            if random_sample:\n",
    "                h_state = init_rnn_hidden_state(inputs.shape[1], hidden_num, num_layers, device)\n",
    "            else:\n",
    "                if h_state is not None:\n",
    "                    if isinstance(h_state, list):\n",
    "                        h_state = [h_state[0].detach_(), h_state[1].detach_()]\n",
    "                    else:\n",
    "                        # split h_state from cal graph, when sample_consecusive\n",
    "                        h_state.detach_()\n",
    "\n",
    "            # rnn, the shape of outputs is (num_step, batch_size, vocab_size)\n",
    "            outputs, h_state = model(inputs, params, h_state, num_layers, device)\n",
    "#             print(outputs.shape, h_state.shape)\n",
    "            # In order to calculate loss, change outputs shape and y shape\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            y = y.transpose(0, 1).contiguous().view(-1)\n",
    "            # calculate loss, y --> long type\n",
    "            l = loss(outputs, y.long())\n",
    "            \n",
    "            # update params\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "\n",
    "            # backward\n",
    "            l.backward()\n",
    "            # grad clip\n",
    "            grad_clipping(params, clipping_theta, device)\n",
    "            # sgd\n",
    "            sgd(params, lr)\n",
    "\n",
    "            # loss_sum\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n_class += y.shape[0]\n",
    "\n",
    "            # perplexity\n",
    "            try:\n",
    "                perplexity = np.exp(l_sum / n_class)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "\n",
    "            # training bar\n",
    "            process_bar.show_process(batch_num, 1, train_loss=perplexity)\n",
    "\n",
    "        # predict\n",
    "        print(\"\\n\")\n",
    "        for prefix in prefixs:\n",
    "            print(f\"prefix-{prefix}: \", predict_rnn(prefix, pred_num, model, init_rnn_hidden_state, hidden_num, \n",
    "                                                    num_layers, params, char_to_idx, vocab_set, vocab_size, device))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:17.256611Z",
     "start_time": "2020-11-14T15:50:51.871026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 923.5364, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [2/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 672.6997, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [3/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 587.1384, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [4/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 545.1814, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [5/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 522.2617, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开   的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "prefix-不分开:  不分开  的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "super_params = {\n",
    "    \"epoch_num\": 1000,\n",
    "    \"model\": deep_rnn,\n",
    "    \"loss\": nn.CrossEntropyLoss(),\n",
    "    \"init_hidden_state\": init_rnn_hidden_state,\n",
    "    \"hidden_num\": 256,\n",
    "    \"get_params\": get_params,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_step\": 32,\n",
    "    \"corpus_index\": corpus_index,\n",
    "    \"data_iter\": data_iter_consecutive,\n",
    "    \"lr\": 10,\n",
    "    \"char_to_idx\": char_to_idx,\n",
    "    \"vocab_set\": vocab_set,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"predict_rnn\": predict_rnn,\n",
    "    \"pred_num\": 50,\n",
    "    \"prefixs\": [\"分开\", \"不分开\"],\n",
    "    \"num_layers\": 2,\n",
    "    \"random_sample\": False\n",
    "}\n",
    "\n",
    "super_params[\"batch_num\"] = len(list(data_iter_consecutive(corpus_index, super_params[\"batch_size\"],\n",
    "                                                     super_params[\"num_step\"], \"cpu\")))\n",
    "\n",
    "train_rnn(**super_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T02:48:19.999938Z",
     "start_time": "2020-11-14T02:48:19.995173Z"
    }
   },
   "source": [
    "### 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:17.470622Z",
     "start_time": "2020-11-14T15:51:17.259218Z"
    }
   },
   "outputs": [],
   "source": [
    "from model_train import train_rnn_pytorch\n",
    "from predict import predict_rnn_pytorch\n",
    "from rnn_model import RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:31.945458Z",
     "start_time": "2020-11-14T15:51:17.472681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 585.5224, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [2/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 492.3270, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [3/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 463.4048, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [4/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 449.2634, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [5/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 441.5806, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "# model\n",
    "hidden_num, num_layers = 256, 2\n",
    "rnn_layer = nn.RNN(vocab_size, hidden_num, num_layers)\n",
    "model = RNNModel(rnn_layer, vocab_size)\n",
    "model = model.cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "params = {\n",
    "    \"epoch_num\": 1000,\n",
    "    \"model\": model,\n",
    "    \"loss\": loss,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_step\": 32,\n",
    "    \"corpus_index\": corpus_index,\n",
    "    \"data_iter\": data_iter_consecutive,\n",
    "    \"char_to_idx\": char_to_idx,\n",
    "    \"vocab_set\": vocab_set,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"predict_rnn_pytorch\": predict_rnn_pytorch,\n",
    "    \"pred_num\": 50,\n",
    "    \"prefixs\": [\"分开\", \"不分开\"],\n",
    "    \"random_sample\": False\n",
    "}\n",
    "\n",
    "params[\"batch_num\"] = len(list(data_iter_consecutive(corpus_index, params[\"batch_size\"],\n",
    "                                                     params[\"num_step\"], \"cpu\")))\n",
    "\n",
    "train_rnn_pytorch(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T13:42:00.593995Z",
     "start_time": "2020-11-14T13:42:00.588027Z"
    }
   },
   "source": [
    "## 双向RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结构定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:31.952046Z",
     "start_time": "2020-11-14T15:51:31.947136Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "def get_params(input_num, hidden_num, device):\n",
    "    # weight\n",
    "    def _ones(shape):\n",
    "        weight = nn.Parameter(torch.normal(0, 0.01, size=shape, device=device), requires_grad=True)\n",
    "        return weight\n",
    "    \n",
    "    def _zeros(shape):\n",
    "        bias = nn.Parameter(torch.zeros(shape, device=device), requires_grad=True)\n",
    "        return bias\n",
    "    \n",
    "    def _three(input_num, hidden_num):\n",
    "        return [\n",
    "            _ones((input_num, hidden_num)),\n",
    "            _ones((hidden_num, hidden_num)),\n",
    "            _zeros(hidden_num)\n",
    "        ]\n",
    "    \n",
    "    # hidden layer\n",
    "    params = _three(input_num, hidden_num)\n",
    "    params.extend(_three(input_num, hidden_num))\n",
    "        \n",
    "    # outputs layer\n",
    "    params.append(_ones((hidden_num*2, input_num)))\n",
    "    params.append(_zeros(input_num))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:31.973255Z",
     "start_time": "2020-11-14T15:51:31.954067Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义rnn隐藏层的状态\n",
    "def init_rnn_hidden_state(batch_size, hidden_num, device):\n",
    "#     return reduce(lambda x, y: torch.cat((x, y)), \n",
    "#                   [torch.zeros(1, batch_size, hidden_num, device=device)]*num_layers)\n",
    "    return [torch.zeros(batch_size, hidden_num, device=device), \n",
    "            torch.zeros(batch_size, hidden_num, device=device)]\n",
    "\n",
    "# 定义rnn网络计算\n",
    "def birnn(inputs, params, h_state, device):\n",
    "    # 隐藏层的输出\n",
    "    outputs = []\n",
    "    # 反转inputs(根据时间步)\n",
    "    inputs_convert = torch.index_select(inputs, 0, \n",
    "                                        torch.arange(inputs.shape[0]-1, -1, -1, device=inputs.device))\n",
    "    \n",
    "    # inputs.shape is (num_step, batch_size, vocab_size), h_state.shape is (num_layers, batch_size, hidden_num)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        h_state[0] = torch.tanh(torch.mm(inputs[i], params[0]) + \n",
    "                                  torch.mm(h_state[0], params[1]) + params[2])\n",
    "        h_state[1] = torch.tanh(torch.mm(inputs_convert[i], params[3]) + \n",
    "                                  torch.mm(h_state[1], params[4]) + params[5])\n",
    "        y = torch.mm(torch.cat((h_state[0], h_state[1]), dim=1), params[-2]) + params[-1]\n",
    "        outputs.append(y.unsqueeze(0))\n",
    "        \n",
    "    return reduce(lambda x,y: torch.cat((x, y)), outputs), h_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:31.987364Z",
     "start_time": "2020-11-14T15:51:31.975259Z"
    }
   },
   "outputs": [],
   "source": [
    "# 验证\n",
    "set_seed(2020)\n",
    "vocab_size, hidden_num, device = 15, 20, \"cuda\"\n",
    "inputs = torch.arange(10).view(2, 5)\n",
    "inputs = to_onehot(inputs, vocab_size, device)\n",
    "params = get_params(vocab_size, hidden_num, device)\n",
    "h_state = init_rnn_hidden_state(inputs.shape[1], hidden_num, device)\n",
    "outputs, h_state = birnn(inputs, params, h_state, device)\n",
    "# print(outputs.shape, h_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:31.994757Z",
     "start_time": "2020-11-14T15:51:31.989718Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, pred_num, model, init_rnn_hidden_state, hidden_num, \n",
    "                params, char_to_idx, vocab_set, vocab_size, device):\n",
    "    # 将词转为index\n",
    "    outputs = [char_to_idx[prefix[0]]]\n",
    "    # 初始化hidden state\n",
    "    h_state = init_rnn_hidden_state(1, hidden_num, device)\n",
    "    \n",
    "    for i in range(len(prefix) + pred_num - 1):\n",
    "        # to_onehot接收的shape为(batch_size, time_step), 单独预测时time_step为1\n",
    "        inputs = to_onehot(torch.tensor(outputs[-1]).view(-1, 1), vocab_size, device)\n",
    "        # 预测, y.shape-->(1, 1, vocab_size), h_state.shape-->(num_layers, batch_size, hidden_num)\n",
    "        y, h_state = model(inputs, params, h_state, device)\n",
    "        # 添加到outpus\n",
    "        if i + 1 < len(prefix):\n",
    "            outputs.append(char_to_idx[prefix[i+1]])\n",
    "        else:\n",
    "            outputs.append(y.argmax(dim=2).item())\n",
    "    return \"\".join(vocab_set[i] for i in outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:32.042785Z",
     "start_time": "2020-11-14T15:51:31.998049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开麦書浅位轨氧莉褪详殿'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "set_seed(2020)\n",
    "hidden_num, pred_num, num_layers, device = 256, 10, 2, \"cuda\"\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "params = get_params(vocab_size, hidden_num, device)\n",
    "# 预测\n",
    "predict_rnn(\"分开\", pred_num, birnn, init_rnn_hidden_state, hidden_num, \n",
    "            params, char_to_idx, vocab_set, vocab_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:32.047190Z",
     "start_time": "2020-11-14T15:51:32.044603Z"
    }
   },
   "outputs": [],
   "source": [
    "from optim import sgd, grad_clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:51:32.061706Z",
     "start_time": "2020-11-14T15:51:32.049065Z"
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def train_rnn(epoch_num, batch_num, model, loss, init_hidden_state, get_params, data_iter, corpus_index,\n",
    "              num_step, hidden_num, lr, batch_size, char_to_idx, vocab_set, vocab_size, prefixs, \n",
    "              predict_rnn, pred_num, clipping_theta=1e-2, random_sample=True, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    function: training and predict in rnn\n",
    "    params epoch_num: the number of epoch\n",
    "    params batch_num: the number of batch in a epoch\n",
    "    params rnn: the rnn model\n",
    "    params loss: such as nn.CrossEntropyLoss()\n",
    "    params init_hidden_state: define the state of hidden layer\n",
    "    params get_params: get the weight and bias in rnn\n",
    "    params data_iter: data_iter_random/data_iter_consecutive\n",
    "    params corpus_index: the index of corpus\n",
    "    params num_step: the number of time step in rnn\n",
    "    params hidden_num: the number of unit in hidden layer in rnn\n",
    "    params lr: the learning rate\n",
    "    params batch_size: the size of a batch\n",
    "    params char_to_idx: char index which convert Chinese to idx\n",
    "    params vocab_set: the list of word in corpus\n",
    "    params vocab_size: the length of vocab_set\n",
    "    params prefixs: the list include input when you want to predict, such as [\"分开\", \"不分开\"]\n",
    "    params pred_num: the number you want to predict\n",
    "    params clipping_heta: the max value of the norm of grad\n",
    "    params random_sample: if sample in random, use data_iter_random. otherwise, use data_iter_consecutive\n",
    "    params device: \"cpu\"/\"cuda\"\n",
    "    \"\"\"\n",
    "    # training bar\n",
    "    process_bar = sqdm()\n",
    "    # init\n",
    "    l_sum, n_class = 0, 0\n",
    "    # get params in rnn\n",
    "    params = get_params(vocab_size, hidden_num, device)\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        # sample in consecutive\n",
    "        if not random_sample:\n",
    "            h_state = init_rnn_hidden_state(batch_size, hidden_num, device)\n",
    "        print(f\"Epoch [{epoch + 1}/{epoch_num}]\")\n",
    "        for x, y in data_iter(corpus_index, batch_size, num_step, device):\n",
    "            # x shape: (num_step, batch_size, vocab_size)\n",
    "            inputs = to_onehot(x, vocab_size, device)\n",
    "            # if sample with random, init h_state in each batch\n",
    "            if random_sample:\n",
    "                h_state = init_rnn_hidden_state(inputs.shape[1], hidden_num, device)\n",
    "            else:\n",
    "                if h_state is not None:\n",
    "                    if isinstance(h_state, list):\n",
    "                        h_state = [h_state[0].detach_(), h_state[1].detach_()]\n",
    "                    else:\n",
    "                        # split h_state from cal graph, when sample_consecusive\n",
    "                        h_state.detach_()\n",
    "\n",
    "            # rnn, the shape of outputs is (num_step, batch_size, vocab_size)\n",
    "            outputs, h_state = model(inputs, params, h_state, device)\n",
    "#             print(outputs.shape, h_state.shape)\n",
    "            # In order to calculate loss, change outputs shape and y shape\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            y = y.transpose(0, 1).contiguous().view(-1)\n",
    "            # calculate loss, y --> long type\n",
    "            l = loss(outputs, y.long())\n",
    "            \n",
    "            # update params\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "\n",
    "            # backward\n",
    "            l.backward()\n",
    "            # grad clip\n",
    "            grad_clipping(params, clipping_theta, device)\n",
    "            # sgd\n",
    "            sgd(params, lr)\n",
    "\n",
    "            # loss_sum\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n_class += y.shape[0]\n",
    "\n",
    "            # perplexity\n",
    "            try:\n",
    "                perplexity = np.exp(l_sum / n_class)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "\n",
    "            # training bar\n",
    "            process_bar.show_process(batch_num, 1, train_loss=perplexity)\n",
    "\n",
    "        # predict\n",
    "        print(\"\\n\")\n",
    "        for prefix in prefixs:\n",
    "            print(f\"prefix-{prefix}: \", predict_rnn(prefix, pred_num, model, init_rnn_hidden_state, hidden_num, \n",
    "                                                    params, char_to_idx, vocab_set, vocab_size, device))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:52:05.792803Z",
     "start_time": "2020-11-14T15:51:32.064175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "31/31 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 936.3162, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [2/5]\n",
      "31/31 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 734.0991, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [3/5]\n",
      "31/31 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 648.9943, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [4/5]\n",
      "31/31 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 610.7383, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [5/5]\n",
      "31/31 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 583.9415, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "super_params = {\n",
    "    \"epoch_num\": 1000,\n",
    "    \"model\": birnn,\n",
    "    \"loss\": nn.CrossEntropyLoss(),\n",
    "    \"init_hidden_state\": init_rnn_hidden_state,\n",
    "    \"hidden_num\": 256,\n",
    "    \"get_params\": get_params,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_step\": 32,\n",
    "    \"corpus_index\": corpus_index,\n",
    "    \"data_iter\": data_iter_random,\n",
    "    \"lr\": 15,\n",
    "    \"char_to_idx\": char_to_idx,\n",
    "    \"vocab_set\": vocab_set,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"predict_rnn\": predict_rnn,\n",
    "    \"pred_num\": 50,\n",
    "    \"prefixs\": [\"分开\", \"不分开\"],\n",
    "#     \"random_sample\": False\n",
    "}\n",
    "\n",
    "super_params[\"batch_num\"] = len(list(data_iter_random(corpus_index, super_params[\"batch_size\"],\n",
    "                                                     super_params[\"num_step\"], \"cpu\")))\n",
    "\n",
    "train_rnn(**super_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:52:05.798545Z",
     "start_time": "2020-11-14T15:52:05.795363Z"
    }
   },
   "outputs": [],
   "source": [
    "from model_train import train_rnn_pytorch\n",
    "from predict import predict_rnn_pytorch\n",
    "from rnn_model import RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T15:52:23.658550Z",
     "start_time": "2020-11-14T15:52:05.815760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 615.0332, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [2/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 492.8114, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开                                                  \n",
      "prefix-不分开:  不分开                                                  \n",
      "\n",
      "\n",
      "Epoch [3/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 418.5828, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "prefix-不分开:  不分开 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "\n",
      "Epoch [4/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 339.9148, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "prefix-不分开:  不分开 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "\n",
      "Epoch [5/5]\n",
      "30/30 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 266.2329, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "prefix-分开:  分开始开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开\n",
      "prefix-不分开:  不分开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开 开\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "corpus_index, char_to_idx, vocab_set, vocab_size = load_data_jay_song()\n",
    "# model\n",
    "hidden_num, num_layers = 256, 2\n",
    "rnn_layer = nn.RNN(vocab_size, hidden_num, bidirectional=True)\n",
    "model = RNNModel(rnn_layer, vocab_size)\n",
    "model = model.cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "params = {\n",
    "    \"epoch_num\": 1000,\n",
    "    \"model\": model,\n",
    "    \"loss\": loss,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_step\": 32,\n",
    "    \"corpus_index\": corpus_index,\n",
    "    \"data_iter\": data_iter_consecutive,\n",
    "    \"char_to_idx\": char_to_idx,\n",
    "    \"vocab_set\": vocab_set,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"predict_rnn_pytorch\": predict_rnn_pytorch,\n",
    "    \"pred_num\": 50,\n",
    "    \"prefixs\": [\"分开\", \"不分开\"],\n",
    "    \"random_sample\": False\n",
    "}\n",
    "\n",
    "params[\"batch_num\"] = len(list(data_iter_consecutive(corpus_index, params[\"batch_size\"],\n",
    "                                                     params[\"num_step\"], \"cpu\")))\n",
    "\n",
    "train_rnn_pytorch(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.038px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
