{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现bp网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*-coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "function: realize multiple perceptron and back propagation by numpy\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_prepare import data_iter\n",
    "from sqdm import sqdm\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    function: define MLP network\n",
    "    params input_num: the neuron number in input layer\n",
    "    params hidden_num: the neuron number in hidden layer\n",
    "    params out_num: the neuron number in output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_num, hidden_num, output_num, alpha=0.01):\n",
    "        self.input_num = input_num\n",
    "        self.hidden_num = hidden_num\n",
    "        self.output_num = output_num\n",
    "        self.sample_num = None\n",
    "        self.w1 = np.random.normal(0, 0.01, size=(input_num, hidden_num))\n",
    "        self.b1 = np.zeros(hidden_num)\n",
    "        self.w2 = np.random.normal(0, 0.01, size=(hidden_num, output_num))\n",
    "        self.b2 = np.zeros(output_num)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(y_pred):\n",
    "        return np.maximum(0, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(y_pred):\n",
    "        return np.exp(y_pred)/(np.expand_dims(np.exp(y_pred).sum(axis=1), 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def linreg(X, w, b):\n",
    "        return X@w + b\n",
    "\n",
    "    def predict_prod(self, X):\n",
    "        a1 = self.relu(self.linreg(X, self.w1, self.b1))\n",
    "        a2 = self.softmax(self.linreg(a1, self.w2, self.b2))\n",
    "        return a2\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy_loss(y_pred, y):\n",
    "        return -(y*np.log(y_pred)).sum()/len(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_error_grad(y_pred, y):\n",
    "        \"\"\"\n",
    "        the derivative of Loss about z is \"[diag(a)- aa^T] @ diag(1/a) @ y^T\",\n",
    "        in the formula above, Loss is \"y^T@log(a)\", y is the true label\n",
    "        in one-hot, a is the softmax output, z is the output before softmax,\n",
    "        the shape of y_pred and y is c x 1\n",
    "        \"\"\"\n",
    "        diag_a = np.diag(1 / y_pred)\n",
    "        diag_f = np.diag(y_pred) - y_pred @ y_pred.T\n",
    "        error_grad = diag_f @ diag_a @ y\n",
    "        return error_grad.T\n",
    "\n",
    "    def cal_batch_error_grad(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        the function \"cal_error_grad\" calculate the error item grad of a sample,\n",
    "        but we often enter a batch sample, so this function is used to\n",
    "        calculate in batch sample\n",
    "        \"\"\"\n",
    "        error_grad = np.zeros((1, self.output_num))\n",
    "        for index in range(self.sample_num):\n",
    "            temp_y = y[index]\n",
    "            temp_pred = y_pred[index]\n",
    "            error_grad = np.vstack((error_grad, self.cal_error_grad(temp_pred, temp_y)))\n",
    "\n",
    "        error_grad = -error_grad[1:] / self.sample_num\n",
    "\n",
    "        return error_grad\n",
    "\n",
    "    def back_propagation(self, X, y_pred, y):\n",
    "        # output in each layer\n",
    "        a0 = X\n",
    "        a1 = self.relu(self.linreg(X, self.w1, self.b1))\n",
    "\n",
    "        # error item\n",
    "        error_grad2 = self.cal_batch_error_grad(y_pred, y)\n",
    "        error_grad1 = error_grad2 @ self.w2.T @ self.relu(np.diag(a1))\n",
    "\n",
    "        print(error_grad2.shape)\n",
    "        print(error_grad1.shape)\n",
    "\n",
    "        # grad of wight and bias\n",
    "        dw2 = a1.T @ error_grad2\n",
    "        db2 = error_grad2\n",
    "        dw1 = a0.T @ error_grad1\n",
    "        db1 = error_grad1\n",
    "\n",
    "        # print(db1)\n",
    "        # print(db1.shape)\n",
    "        # print(dw1)\n",
    "        # print(dw1.shape)\n",
    "        # print(db2)\n",
    "        # print(db2.shape)\n",
    "        # print(dw2)\n",
    "        # print(dw2.shape)\n",
    "\n",
    "        return db1, dw1, db2, dw2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # sample num in each iteration\n",
    "        self.sample_num = int(X.size / self.input_num)\n",
    "        # predict\n",
    "        y_pred = self.predict_prod(X)\n",
    "\n",
    "        # bp\n",
    "        db1, dw1, db2, dw2 = self.back_propagation(X, y_pred, y)\n",
    "\n",
    "        # update grad\n",
    "        self.w1 -= self.alpha * dw1\n",
    "        self.b1 -= self.alpha * db1\n",
    "        self.w2 -= self.alpha * dw2\n",
    "        self.b2 -= self.alpha * db2\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_prod(X)\n",
    "        y_pred = np.argmax(y_pred, axis=1).reshape(-1)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        acc = (y_pred == y).sum() / len(y)\n",
    "        return acc\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"epoch_num\": 10,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "# deal with label in iris data, such as (0 --> [1, 0, 0])\n",
    "label_dict = {\n",
    "    0: [1, 0, 0],\n",
    "    1: [0, 1, 0],\n",
    "    2: [0, 0, 1]\n",
    "}\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = np.array(list(map(lambda x: label_dict[x], list(iris.target))))\n",
    "\n",
    "model = MLP(4, 6, 3)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)\n",
    "\n",
    "for epoch in range(params[\"epoch_num\"]):\n",
    "    for xdata, ydata in data_iter(params[\"batch_size\"], xtrain, ytrain):\n",
    "        model.fit(xdata, ydata)\n",
    "        print(model.predict(xdata))\n",
    "        print(model.score(xdata, ydata))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
