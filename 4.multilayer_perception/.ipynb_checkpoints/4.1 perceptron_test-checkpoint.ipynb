{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T11:47:43.703053Z",
     "start_time": "2020-10-17T11:47:43.683821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现bp网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T11:47:45.438555Z",
     "start_time": "2020-10-17T11:47:44.538676Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from data_prepare import data_iter\n",
    "from sqdm import sqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T11:47:45.460637Z",
     "start_time": "2020-10-17T11:47:45.440104Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    params alpha: learning rate\n",
    "    params input_num: the number of input layer\n",
    "    params output_num: the number of output layer\n",
    "    params hidden_num: the number of hidden_num, it can accept a/more integer\n",
    "    \n",
    "    for example:\n",
    "    (0.01, 4, 3, 5)---> the network has one hidden_num which has 5 hidden_unit. Beside, the input layers has\n",
    "    four unit, the output layers has 3 unit.\n",
    "    (0.01, 4, 3, 5, 5)---> the network has two hidden_num which has 5 hidden_unit. Beside, the input layers has\n",
    "    four unit, the output layers has 3 unit.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, input_num, output_num, *hidden_num):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_num = (input_num, *hidden_num, output_num)\n",
    "        self.weight = {}\n",
    "        self.bias = {}\n",
    "        self.dw = {}\n",
    "        self.db = {}\n",
    "        self.alpha = alpha\n",
    "        self.activate = {}\n",
    "        self.count = 1\n",
    "        # init weight and bias, the grad of weight and bias\n",
    "        for idx, num in enumerate(zip(self.layer_num[1:], self.layer_num[:-1])):\n",
    "            self.weight[\"w\"+str(idx+1)] = np.random.normal(0, 0.1, size=num)\n",
    "            self.bias[\"b\"+str(idx+1)] = np.zeros(num[0])\n",
    "            self.dw.setdefault(\"dw\"+str(idx+1), [])\n",
    "            self.db.setdefault(\"db\"+str(idx+1), [])\n",
    "          \n",
    "    @staticmethod\n",
    "    def relu(y_pred):\n",
    "        return np.maximum(0, y_pred)\n",
    "    \n",
    "    @staticmethod\n",
    "    def linreg(x, w, b):\n",
    "        return np.dot(x, w.T) + b\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(y_pred):\n",
    "        return np.exp(y_pred) / np.expand_dims(np.exp(y_pred).sum(axis=1), 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy_loss(y_pred, y):\n",
    "        return -(y*np.log(y_pred+1e-8)).sum() / len(y)\n",
    "    \n",
    "    def cal_error_grad(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        calculate the grad of error item in a sample. In particular, the shape of y_pred and y is (1, c),\n",
    "        the y_pred stands for the output of network, the y stands for the true labels, c stands for \n",
    "        the number of category.\n",
    "        \"\"\"\n",
    "        theta = -y@np.diag(1/(y_pred+1e-8).reshape(-1))@(np.diag(y_pred.reshape(-1))-y_pred.T@y_pred)\n",
    "        return theta.T\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        calculate the activate output in the forward propagation. Beside, if network has one hidden layer,\n",
    "        it has three activate output. 'a0' is input, 'a1' is the first hidden layer output, 'a2' is the\n",
    "        network output. Except for the output(like 'a2') using the softmax function, other(not input, \n",
    "        like 'a1') using the relu function.\n",
    "        \"\"\"\n",
    "        if self.count < len(self.layer_num)-1:\n",
    "            output = self.relu(self.linreg(x, self.weight[\"w\"+str(self.count)], \n",
    "                                           self.bias[\"b\"+str(self.count)]))\n",
    "            self.activate[\"a\"+str(self.count)] = output\n",
    "            self.count += 1\n",
    "            self.forward(output)\n",
    "        if self.count == len(self.layer_num)-1:\n",
    "            a = self.activate[\"a\"+str(self.count-1)]\n",
    "            output = self.softmax(self.linreg(a, self.weight[\"w\"+str(self.count)], \n",
    "                                           self.bias[\"b\"+str(self.count)]))\n",
    "            self.activate[\"a\"+str(self.count)] = output\n",
    "            \n",
    "        return self.activate[\"a\"+str(self.count)]\n",
    "        \n",
    "    def cal_grad(self, theta, sample_num):\n",
    "        \"\"\"\n",
    "        calculate the grad of wight and bias in each layer.\n",
    "        dw(l) = theta(l)@[a(l-1)]^T, the a stands for the l-1 layer activate output\n",
    "        db(l) = theta(l), the theta is the grad of error item in l layer\n",
    "        theta(l-1) = diag(f'(z(l-1)))@[w(l)]^T@theta(l)\n",
    "        z(l) = w(l)@a(l-1) + b(l)\n",
    "        a(l) = f(z(l))\n",
    "        \"\"\"\n",
    "        for i in range(1, len(self.layer_num)):\n",
    "            num = len(self.layer_num)-i\n",
    "            act = np.expand_dims(self.activate[\"a\"+str(num-1)][sample_num], 0)\n",
    "            self.dw[\"dw\"+str(num)].append(theta@act)\n",
    "            self.db[\"db\"+str(num)].append(theta)\n",
    "            theta = np.diag(np.where(act>0, 1, 0).reshape(-1))@self.weight[\"w\"+str(num)].T@theta\n",
    "            \n",
    "    def fit(self, x, y):\n",
    "        # forward propagation\n",
    "        y_pred = self.predict(x)\n",
    "        # calculate the grad with a sample in each batch\n",
    "        for i in range(len(y)):\n",
    "            # the error item in the last layer\n",
    "            theta = self.cal_error_grad(np.expand_dims(y_pred[i], 0), np.expand_dims(y[i], 0))\n",
    "            # cal grad, i stans for the sample which is calculating\n",
    "            self.cal_grad(theta, i)\n",
    "            \n",
    "        # combine grad with batch\n",
    "        for i in range(1, len(self.layer_num)):\n",
    "            num = len(self.layer_num)-i\n",
    "            self.dw[\"dw\"+str(num)] = sum(self.dw[\"dw\"+str(num)]) / len(y)\n",
    "            self.weight[\"w\"+str(num)] -= self.alpha*self.dw[\"dw\"+str(num)]\n",
    "            self.db[\"db\"+str(num)] = sum(self.db[\"db\"+str(num)])\n",
    "            self.bias[\"b\"+str(num)] -= self.alpha*self.db[\"db\"+str(num)].reshape(-1)\n",
    "    \n",
    "        # clear grad\n",
    "        for i in range(1, len(self.layer_num)):\n",
    "            num = len(self.layer_num)-i\n",
    "            self.dw[\"dw\"+str(num)] = []\n",
    "            self.db[\"db\"+str(num)] = []\n",
    "       \n",
    "    def predict(self, x):\n",
    "        self.count = 1\n",
    "        self.activate[\"a0\"] = x\n",
    "        y_pred = self.forward(x)\n",
    "        return y_pred\n",
    "            \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        acc = (y_pred.argmax(axis=1) == y).sum() / len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T11:47:55.398824Z",
     "start_time": "2020-10-17T11:47:46.487284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0917, train_score: 0.500, test_loss: 1.0977, test_score: 0.395\n",
      "\n",
      "Epoch [2/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0895, train_score: 0.500, test_loss: 1.0976, test_score: 0.395\n",
      "\n",
      "Epoch [3/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0887, train_score: 0.500, test_loss: 1.0975, test_score: 0.395\n",
      "\n",
      "Epoch [4/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0883, train_score: 0.500, test_loss: 1.0974, test_score: 0.395\n",
      "\n",
      "Epoch [5/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0881, train_score: 0.500, test_loss: 1.0972, test_score: 0.395\n",
      "\n",
      "Epoch [6/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0878, train_score: 0.500, test_loss: 1.0970, test_score: 0.395\n",
      "\n",
      "Epoch [7/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0875, train_score: 0.500, test_loss: 1.0967, test_score: 0.395\n",
      "\n",
      "Epoch [8/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0870, train_score: 0.500, test_loss: 1.0962, test_score: 0.395\n",
      "\n",
      "Epoch [9/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0863, train_score: 0.500, test_loss: 1.0956, test_score: 0.395\n",
      "\n",
      "Epoch [10/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0852, train_score: 0.500, test_loss: 1.0945, test_score: 0.395\n",
      "\n",
      "Epoch [11/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0833, train_score: 0.500, test_loss: 1.0929, test_score: 0.395\n",
      "\n",
      "Epoch [12/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0802, train_score: 0.500, test_loss: 1.0900, test_score: 0.395\n",
      "\n",
      "Epoch [13/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0744, train_score: 0.500, test_loss: 1.0849, test_score: 0.395\n",
      "\n",
      "Epoch [14/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0631, train_score: 0.500, test_loss: 1.0750, test_score: 0.395\n",
      "\n",
      "Epoch [15/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0399, train_score: 0.500, test_loss: 1.0549, test_score: 0.395\n",
      "\n",
      "Epoch [16/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9943, train_score: 0.500, test_loss: 1.0154, test_score: 0.395\n",
      "\n",
      "Epoch [17/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9187, train_score: 0.500, test_loss: 0.9475, test_score: 0.474\n",
      "\n",
      "Epoch [18/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8084, train_score: 0.750, test_loss: 0.8398, test_score: 0.658\n",
      "\n",
      "Epoch [19/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6577, train_score: 0.750, test_loss: 0.6895, test_score: 0.658\n",
      "\n",
      "Epoch [20/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.5540, train_score: 0.750, test_loss: 0.5997, test_score: 0.658\n",
      "\n",
      "Epoch [21/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.4923, train_score: 0.750, test_loss: 0.5463, test_score: 0.658\n",
      "\n",
      "Epoch [22/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.4590, train_score: 0.750, test_loss: 0.5076, test_score: 0.658\n",
      "\n",
      "Epoch [23/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.4358, train_score: 0.750, test_loss: 0.4798, test_score: 0.684\n",
      "\n",
      "Epoch [24/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.4181, train_score: 0.750, test_loss: 0.4580, test_score: 0.684\n",
      "\n",
      "Epoch [25/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.4031, train_score: 0.750, test_loss: 0.4395, test_score: 0.684\n",
      "\n",
      "Epoch [26/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3908, train_score: 0.750, test_loss: 0.4255, test_score: 0.711\n",
      "\n",
      "Epoch [27/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3779, train_score: 0.750, test_loss: 0.4088, test_score: 0.711\n",
      "\n",
      "Epoch [28/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3662, train_score: 0.750, test_loss: 0.3941, test_score: 0.789\n",
      "\n",
      "Epoch [29/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3537, train_score: 0.750, test_loss: 0.3781, test_score: 0.789\n",
      "\n",
      "Epoch [30/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3412, train_score: 0.750, test_loss: 0.3624, test_score: 0.868\n",
      "\n",
      "Epoch [31/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3288, train_score: 0.750, test_loss: 0.3472, test_score: 0.895\n",
      "\n",
      "Epoch [32/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3165, train_score: 0.750, test_loss: 0.3326, test_score: 0.895\n",
      "\n",
      "Epoch [33/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3043, train_score: 0.750, test_loss: 0.3185, test_score: 0.895\n",
      "\n",
      "Epoch [34/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2925, train_score: 1.000, test_loss: 0.3052, test_score: 0.895\n",
      "\n",
      "Epoch [35/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2809, train_score: 1.000, test_loss: 0.2927, test_score: 0.895\n",
      "\n",
      "Epoch [36/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2697, train_score: 1.000, test_loss: 0.2809, test_score: 0.921\n",
      "\n",
      "Epoch [37/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2589, train_score: 1.000, test_loss: 0.2700, test_score: 0.947\n",
      "\n",
      "Epoch [38/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2485, train_score: 1.000, test_loss: 0.2599, test_score: 0.974\n",
      "\n",
      "Epoch [39/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2387, train_score: 1.000, test_loss: 0.2505, test_score: 0.974\n",
      "\n",
      "Epoch [40/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2293, train_score: 1.000, test_loss: 0.2419, test_score: 0.974\n",
      "\n",
      "Epoch [41/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2204, train_score: 1.000, test_loss: 0.2339, test_score: 0.974\n",
      "\n",
      "Epoch [42/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2120, train_score: 1.000, test_loss: 0.2265, test_score: 0.974\n",
      "\n",
      "Epoch [43/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2041, train_score: 1.000, test_loss: 0.2197, test_score: 0.974\n",
      "\n",
      "Epoch [44/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1965, train_score: 1.000, test_loss: 0.2134, test_score: 0.974\n",
      "\n",
      "Epoch [45/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1894, train_score: 1.000, test_loss: 0.2075, test_score: 0.974\n",
      "\n",
      "Epoch [46/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1826, train_score: 1.000, test_loss: 0.2021, test_score: 0.974\n",
      "\n",
      "Epoch [47/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1761, train_score: 1.000, test_loss: 0.1970, test_score: 0.974\n",
      "\n",
      "Epoch [48/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1699, train_score: 1.000, test_loss: 0.1923, test_score: 0.974\n",
      "\n",
      "Epoch [49/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1641, train_score: 1.000, test_loss: 0.1878, test_score: 0.974\n",
      "\n",
      "Epoch [50/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1584, train_score: 1.000, test_loss: 0.1837, test_score: 0.974\n",
      "\n",
      "Epoch [51/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1517, train_score: 1.000, test_loss: 0.1797, test_score: 0.974\n",
      "\n",
      "Epoch [52/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1461, train_score: 1.000, test_loss: 0.1760, test_score: 0.974\n",
      "\n",
      "Epoch [53/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1410, train_score: 1.000, test_loss: 0.1725, test_score: 0.974\n",
      "\n",
      "Epoch [54/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1361, train_score: 1.000, test_loss: 0.1693, test_score: 0.974\n",
      "\n",
      "Epoch [55/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1314, train_score: 1.000, test_loss: 0.1662, test_score: 0.974\n",
      "\n",
      "Epoch [56/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1265, train_score: 1.000, test_loss: 0.1631, test_score: 0.974\n",
      "\n",
      "Epoch [57/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1220, train_score: 1.000, test_loss: 0.1602, test_score: 0.974\n",
      "\n",
      "Epoch [58/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1177, train_score: 1.000, test_loss: 0.1575, test_score: 0.974\n",
      "\n",
      "Epoch [59/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1135, train_score: 1.000, test_loss: 0.1549, test_score: 0.974\n",
      "\n",
      "Epoch [60/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1094, train_score: 1.000, test_loss: 0.1525, test_score: 0.974\n",
      "\n",
      "Epoch [61/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1055, train_score: 1.000, test_loss: 0.1503, test_score: 0.974\n",
      "\n",
      "Epoch [62/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1018, train_score: 1.000, test_loss: 0.1482, test_score: 1.000\n",
      "\n",
      "Epoch [63/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0982, train_score: 1.000, test_loss: 0.1462, test_score: 1.000\n",
      "\n",
      "Epoch [64/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0947, train_score: 1.000, test_loss: 0.1443, test_score: 0.974\n",
      "\n",
      "Epoch [65/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0913, train_score: 1.000, test_loss: 0.1426, test_score: 0.974\n",
      "\n",
      "Epoch [66/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0881, train_score: 1.000, test_loss: 0.1411, test_score: 0.974\n",
      "\n",
      "Epoch [67/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0850, train_score: 1.000, test_loss: 0.1396, test_score: 0.974\n",
      "\n",
      "Epoch [68/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0821, train_score: 1.000, test_loss: 0.1383, test_score: 0.974\n",
      "\n",
      "Epoch [69/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0792, train_score: 1.000, test_loss: 0.1371, test_score: 0.974\n",
      "\n",
      "Epoch [70/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0765, train_score: 1.000, test_loss: 0.1361, test_score: 0.974\n",
      "\n",
      "Epoch [71/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0739, train_score: 1.000, test_loss: 0.1351, test_score: 0.974\n",
      "\n",
      "Epoch [72/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0714, train_score: 1.000, test_loss: 0.1343, test_score: 0.974\n",
      "\n",
      "Epoch [73/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0690, train_score: 1.000, test_loss: 0.1335, test_score: 0.974\n",
      "\n",
      "Epoch [74/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0667, train_score: 1.000, test_loss: 0.1328, test_score: 0.974\n",
      "\n",
      "Epoch [75/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0646, train_score: 1.000, test_loss: 0.1322, test_score: 0.974\n",
      "\n",
      "Epoch [76/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0625, train_score: 1.000, test_loss: 0.1317, test_score: 0.974\n",
      "\n",
      "Epoch [77/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0606, train_score: 1.000, test_loss: 0.1312, test_score: 0.974\n",
      "\n",
      "Epoch [78/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0588, train_score: 1.000, test_loss: 0.1308, test_score: 0.974\n",
      "\n",
      "Epoch [79/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0570, train_score: 1.000, test_loss: 0.1305, test_score: 0.974\n",
      "\n",
      "Epoch [80/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0553, train_score: 1.000, test_loss: 0.1302, test_score: 0.974\n",
      "\n",
      "Epoch [81/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0537, train_score: 1.000, test_loss: 0.1300, test_score: 0.974\n",
      "\n",
      "Epoch [82/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0521, train_score: 1.000, test_loss: 0.1299, test_score: 0.974\n",
      "\n",
      "Epoch [83/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0505, train_score: 1.000, test_loss: 0.1298, test_score: 0.974\n",
      "\n",
      "Epoch [84/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0491, train_score: 1.000, test_loss: 0.1298, test_score: 0.974\n",
      "\n",
      "Epoch [85/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0477, train_score: 1.000, test_loss: 0.1298, test_score: 0.974\n",
      "\n",
      "Epoch [86/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0463, train_score: 1.000, test_loss: 0.1298, test_score: 0.974\n",
      "\n",
      "Epoch [87/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0450, train_score: 1.000, test_loss: 0.1299, test_score: 0.974\n",
      "\n",
      "Epoch [88/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0438, train_score: 1.000, test_loss: 0.1300, test_score: 0.974\n",
      "\n",
      "Epoch [89/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0426, train_score: 1.000, test_loss: 0.1301, test_score: 0.974\n",
      "\n",
      "Epoch [90/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0414, train_score: 1.000, test_loss: 0.1303, test_score: 0.974\n",
      "\n",
      "Epoch [91/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0403, train_score: 1.000, test_loss: 0.1304, test_score: 0.974\n",
      "\n",
      "Epoch [92/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0393, train_score: 1.000, test_loss: 0.1306, test_score: 0.974\n",
      "\n",
      "Epoch [93/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0383, train_score: 1.000, test_loss: 0.1308, test_score: 0.974\n",
      "\n",
      "Epoch [94/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0373, train_score: 1.000, test_loss: 0.1310, test_score: 0.974\n",
      "\n",
      "Epoch [95/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0363, train_score: 1.000, test_loss: 0.1312, test_score: 0.974\n",
      "\n",
      "Epoch [96/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0354, train_score: 1.000, test_loss: 0.1314, test_score: 0.974\n",
      "\n",
      "Epoch [97/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0346, train_score: 1.000, test_loss: 0.1317, test_score: 0.974\n",
      "\n",
      "Epoch [98/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0337, train_score: 1.000, test_loss: 0.1320, test_score: 0.974\n",
      "\n",
      "Epoch [99/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0330, train_score: 1.000, test_loss: 0.1322, test_score: 0.974\n",
      "\n",
      "Epoch [100/100]\n",
      "112/112 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0322, train_score: 1.000, test_loss: 0.1324, test_score: 0.974\n",
      "\n",
      "the accuracy of all data: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"epoch_num\": 100,\n",
    "    \"batch_size\": 4,\n",
    "}\n",
    "\n",
    "# deal with label in iris data, such as (0 --> [1, 0, 0])\n",
    "label_dict = {\n",
    "    0: [1, 0, 0],\n",
    "    1: [0, 1, 0],\n",
    "    2: [0, 0, 1]\n",
    "}\n",
    "\n",
    "# load iris data\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = np.array(list(map(lambda x: label_dict[x], list(iris.target))))\n",
    "\n",
    "# define model\n",
    "model = MLP(0.03, 4, 3, 5, 5)\n",
    "# split dataset\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)\n",
    "\n",
    "# training bar\n",
    "process_bar = sqdm()\n",
    "for epoch in range(params[\"epoch_num\"]):\n",
    "    print(f\"Epoch [{epoch+1}/{params['epoch_num']}]\")\n",
    "    for xdata, ydata in data_iter(params[\"batch_size\"], xtrain, ytrain):\n",
    "        # data fit\n",
    "        model.fit(xdata, ydata)\n",
    "        # training\n",
    "        train_pred = model.predict(xdata)\n",
    "        train_loss = model.entropy_loss(train_pred, ydata)\n",
    "        train_acc = model.score(xdata, ydata.argmax(axis=1))\n",
    "        \n",
    "        # test\n",
    "        test_pred = model.predict(xtest)\n",
    "        test_loss = model.entropy_loss(test_pred, ytest)\n",
    "        test_acc = model.score(xtest, ytest.argmax(axis=1))\n",
    "        process_bar.show_process(len(xtrain), params[\"batch_size\"], train_loss=train_loss, \n",
    "                                train_score=train_acc, test_loss=test_loss, test_score=test_acc)\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(f\"the accuracy of all data: {model.score(x, iris.target)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
