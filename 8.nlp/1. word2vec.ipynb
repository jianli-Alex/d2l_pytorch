{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T08:46:03.325448Z",
     "start_time": "2020-11-13T08:46:03.305285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T08:46:51.932980Z",
     "start_time": "2020-11-13T08:46:51.926331Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T10:13:54.766661Z",
     "start_time": "2020-11-13T10:13:54.761807Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from sqdm import sqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T10:13:54.901203Z",
     "start_time": "2020-11-13T10:13:54.893084Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PATHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据/建立索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T08:53:10.505461Z",
     "start_time": "2020-11-13T08:53:10.359968Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "with open(\"../data/ptb/ptb.train.txt\", \"r+\") as f:\n",
    "    lines = f.readlines()\n",
    "    # split传入的值为空时，分割空格，包括\"\\n\"\n",
    "    corpus = [sentence.split() for sentence in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:15:08.212754Z",
     "start_time": "2020-11-13T09:15:07.957371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887100\n"
     ]
    }
   ],
   "source": [
    "# 统计各个token的出现次数\n",
    "token_counter = Counter([token for sentence in corpus for token in sentence])\n",
    "# 去除出现5次以下的token\n",
    "token_counter = dict(filter(lambda x: x[1] >= 5, token_counter.items()))\n",
    "# 词表\n",
    "vocab_set = list(token_counter.keys())\n",
    "# 词表索引化\n",
    "token_to_idx = {token:idx for idx, token in enumerate(vocab_set)}\n",
    "# corpus索引化\n",
    "corpus_index = [[token_to_idx[token] for token in sentence if token in token_to_idx] for sentence in corpus]\n",
    "# 统计数量\n",
    "token_num = sum([len(sentence) for sentence in corpus_index])\n",
    "print(token_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二次采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:04:44.494886Z",
     "start_time": "2020-11-13T09:04:44.480319Z"
    }
   },
   "source": [
    "文本数据中一般会出现一些高频词。通常来说，在一个背景窗口中，一个词和较低词频的词同时出现比和较高词频同时出现对训练词嵌入模型更加有益处\n",
    "- 因此，在训练词嵌入模型的时候，可以对词进行二次采样\n",
    "- 实际上，就是对每一个索引词$w_i$以一定的概率丢弃，其中$f_w$是中心词$w_i$在数据集中的个数与总词数之比，t通常取1e-4\n",
    "$$P(w_i) = max(1 - \\sqrt{\\frac{t}{f_w}}, 0)$$\n",
    "- 当$f(w_i) > t$的时候，才有可能在二次采样中丢弃词$w_i$，越是高频的词被丢弃的概率越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:17:45.190258Z",
     "start_time": "2020-11-13T09:17:42.172534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375603\n"
     ]
    }
   ],
   "source": [
    "def discard(index, theta=1e-4):\n",
    "    \"\"\"\n",
    "    function: 二次采样中是否丢弃词，True时丢弃\n",
    "    params index: 传入的是词索引\n",
    "    \"\"\"\n",
    "    return np.random.uniform(0, 1) < (1 - np.sqrt(theta / token_counter[vocab_set[index]] * token_num))\n",
    "\n",
    "# 二次采样\n",
    "subsampling_corpus_index = [[token for token in sentence if not discard(token)] for sentence in corpus_index]\n",
    "subsampling_token_num = sum([len(sentence) for sentence in subsampling_corpus_index])\n",
    "print(subsampling_token_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较二次采样前后，高频词和低频词的数量变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:23:32.735086Z",
     "start_time": "2020-11-13T09:23:32.729305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3].count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:24:09.678691Z",
     "start_time": "2020-11-13T09:24:09.672238Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_count(token):\n",
    "    \"\"\"\n",
    "    function: 比较二次采样前后的词数量变化\n",
    "    params token: token是词，像\"the\"/\"join\"等\n",
    "    \"\"\"\n",
    "    token_index = token_to_idx[token]\n",
    "    before = sum([sentence.count(token_index) for sentence in corpus_index])\n",
    "    after = sum([sentence.count(token_index) for sentence in subsampling_corpus_index])\n",
    "    print(f\"{token}, before:{before}, after:{after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:24:09.911574Z",
     "start_time": "2020-11-13T09:24:09.828207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the, before:50770, after:2124\n",
      "join, before:45, after:45\n"
     ]
    }
   ],
   "source": [
    "compare_count(\"the\")\n",
    "compare_count(\"join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取中心词和上下文词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取与中心词距离不超过背景窗口大小的词作为上下文词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:40:53.611065Z",
     "start_time": "2020-11-13T09:40:53.601874Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_center_and_context(corpus_index, max_window_size):\n",
    "    \"\"\"\n",
    "    function: 提取中心词和上下文词\n",
    "    params corpus_index: 索引化后的corpus\n",
    "    params max_window_size: 最大背景窗口\n",
    "    \"\"\"\n",
    "    # 用于存放提取的中心词和上下文词\n",
    "    all_centers, all_contexts = [], []\n",
    "    \n",
    "    for sentence in corpus_index:\n",
    "        # 由于最少需要(中心词，背景词),即sentence的最小长度为2\n",
    "        if len(sentence) < 2:\n",
    "            continue\n",
    "        # 添加中心词，实际上sentence中的每一个词都可以作为中心词\n",
    "        all_centers += sentence\n",
    "        # 添加背景词\n",
    "        for index in range(len(sentence)):\n",
    "            # 在1-最大背景窗口中采样一个背景窗口\n",
    "            window_size = np.random.randint(1, max_window_size+1)\n",
    "            # 获取背景词的索引\n",
    "            index_list = list(np.arange(max(0, index-window_size), min(len(sentence), index+window_size+1)))\n",
    "            # 去掉当前中心词的索引\n",
    "            index_list.remove(index)\n",
    "            all_contexts.append(list(np.array(sentence)[index_list]))\n",
    "    return all_centers, all_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:42:02.970584Z",
     "start_time": "2020-11-13T09:42:02.950263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center: 0, has contexts: [1]\n",
      "center: 1, has contexts: [0, 2]\n",
      "center: 2, has contexts: [0, 1, 3, 4]\n",
      "center: 3, has contexts: [2, 4]\n",
      "center: 4, has contexts: [2, 3, 5, 6]\n",
      "center: 5, has contexts: [3, 4, 6]\n",
      "center: 6, has contexts: [4, 5]\n",
      "center: 7, has contexts: [8, 9]\n",
      "center: 8, has contexts: [7, 9]\n",
      "center: 9, has contexts: [8]\n"
     ]
    }
   ],
   "source": [
    "# 验证\n",
    "set_seed(2020)\n",
    "# 创建人工数据集\n",
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print(\"dataset\", tiny_dataset)\n",
    "for center, context in zip(*get_center_and_context(tiny_dataset, 2)):\n",
    "    print(f\"center: {center}, has contexts: {context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取最大窗口为5的中心词和背景词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:43:17.482003Z",
     "start_time": "2020-11-13T09:43:12.945394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374633"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "374633"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_centers, all_contexts = get_center_and_context(subsampling_corpus_index, 5)\n",
    "len(all_centers)\n",
    "len(all_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:40:22.654051Z",
     "start_time": "2020-11-13T09:40:22.640732Z"
    }
   },
   "source": [
    "### 负采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用负采样来近似训练，对于一对中心词和背景词，我们随机采样k个噪声词\n",
    "- 噪声词的采样频率$P(w)$设为w词频和总词频之比的0.75次方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T10:09:50.846968Z",
     "start_time": "2020-11-13T10:09:50.830943Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_negative(all_contexts, sample_weight, k):\n",
    "    \"\"\"\n",
    "    function: 实现负采样\n",
    "    params all_contexts: 所有的背景词\n",
    "    params sample_weight: 词表中的采样权重\n",
    "    params k: 采样倍数\n",
    "    \"\"\"\n",
    "    all_negatives, negatives_candidate, i = [], [], 0\n",
    "    # 总词表的长度\n",
    "    population = list(range(len(sample_weight)))\n",
    "    \n",
    "    for context in all_contexts:\n",
    "        negatives = []\n",
    "            \n",
    "        while len(negatives) < len(context) * k:\n",
    "            if i == len(negatives_candidate):\n",
    "                negatives_candidate, i = random.choices(population, sample_weight, k=int(1e5)), 0\n",
    "            # 更新\n",
    "            neg, i = negatives_candidate[i], i+1\n",
    "            # 噪声词不能是背景词(其实，在这里可以看出来word2vec并没有考虑语序的)\n",
    "            if neg not in context:\n",
    "                negatives.append(neg)\n",
    "                \n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T10:10:05.422939Z",
     "start_time": "2020-11-13T10:09:51.611451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374633"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight = [token_counter[w] ** 0.75 for w in vocab_set]\n",
    "all_negatives = get_negative(all_contexts, sample_weight, k=5)\n",
    "len(all_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T10:32:44.605602Z",
     "start_time": "2020-11-13T10:32:44.599918Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(Data.Dataset):\n",
    "    def __init__(self, all_centers, all_contexts, all_negatives):\n",
    "        assert len(all_centers) == len(all_contexts) == len(all_negatives)\n",
    "        self.centers = all_centers\n",
    "        self.contexts = all_contexts\n",
    "        self.negatives = all_negatives\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为语料是不定长的，所以在训练之前，需要填充0。此外，为了防止在训练的时候，填充的部分不会对损失函数造成影响，需要进行mask\n",
    "- 另外，通常把背景词和噪声词拼接起来，因此需要区分哪些是背景词，哪些是噪声词ℹi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T10:32:44.887869Z",
     "start_time": "2020-11-13T10:32:44.879456Z"
    }
   },
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    \"\"\"\n",
    "    function: 定义每个批数据是怎么运算的，传入的数据shape是(batch_size, list)，这个list由Dataset来定义\n",
    "    \"\"\"\n",
    "    # 背景词和噪声词的最大长度\n",
    "    max_len = max([len(c) + len(n) for _, c, n in data])\n",
    "    # init\n",
    "    centers, context_negatives, masks, labels = [], [], [], []\n",
    "    \n",
    "    for center, context, negative in data:\n",
    "        # center: int, context.shape: window_size的倍数(list), negative.shape: k倍的context.shape(list)\n",
    "        # 当前批中，一条数据背景词和噪声词的长度\n",
    "        cur_len = len(context) + len(negative)\n",
    "        # 添加中心词\n",
    "        centers += [center]\n",
    "        # 添加背景词和噪声词（一个列表对应着一个中心词(int)）\n",
    "        context_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        # 添加mask\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        # 添加label，背景词是正类，噪声词和填充词是负类\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(context_negatives), \n",
    "            torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T10:32:45.329486Z",
     "start_time": "2020-11-13T10:32:45.057346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers torch.Size([512, 1])\n",
      "context_negatives torch.Size([512, 60])\n",
      "masks torch.Size([512, 60])\n",
      "labels torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(all_centers, all_contexts, all_negatives)\n",
    "# 生成迭代器\n",
    "train_iter = Data.DataLoader(dataset, batch_size=512, collate_fn=batchify, num_workers=4, shuffle=True)\n",
    "\n",
    "# 验证\n",
    "for batch in train_iter:\n",
    "    for name, data in zip([\"centers\", \"context_negatives\", \"masks\", \"labels\"], batch):\n",
    "        print(name, data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skip_gram模型实际上就是用中心词来预测周围词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T10:42:46.971660Z",
     "start_time": "2020-11-13T10:42:46.967279Z"
    }
   },
   "outputs": [],
   "source": [
    "def skip_gram(centers, context_negatives, embed_u, embed_v):\n",
    "    \"\"\"\n",
    "    function: 实现skip_gram model\n",
    "    params centers: 中心词, shape-->(b, 1)\n",
    "    params context_negatives: 背景词+噪声词, shape-->(b, max_len)\n",
    "    params embed_u: 中心词的embedding, shape-->(len(vocab_set), d_model), d_model为词向量的维度\n",
    "    params embed_v: 背景词+噪声词的embedding, shape-->(len(vocab_set)\n",
    "    \"\"\"\n",
    "    # 通过词索引进行向量化\n",
    "    centers = embed_u(centers) # shape--> (b, 1, d_model)\n",
    "    context_negatives = embed_v(context_negatives) # shape--> (b, max_len, d_model)\n",
    "    # 矩阵相乘\n",
    "    pred = torch.bmm(centers, context_negatives.transpose(1, 2))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T12:47:31.036831Z",
     "start_time": "2020-11-13T12:47:31.031092Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义loss(二元交叉熵损失)\n",
    "class BinarySigmoidEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinarySigmoidEntropyLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, label, mask=None):\n",
    "        # mask是掩码，使得填充项不参与计算\n",
    "        if mask is not None:\n",
    "            mask = mask.float()\n",
    "        y_pred, label = y_pred.float(), label.float()\n",
    "        # 和BCEWithLogitsLoss的作用一样\n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, label, weight=mask, reduction=\"none\")\n",
    "        if mask is not None:\n",
    "            # 对mask部分进行修正\n",
    "            loss = loss.mean(dim=1) * mask.shape[1] / mask.sum(dim=1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T12:47:31.530543Z",
     "start_time": "2020-11-13T12:47:31.516825Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "def trainer(epoch_num, batch_num, data_iter, model, embed, loss, lr, device):\n",
    "    # training bar\n",
    "    process_bar = sqdm()\n",
    "    # define embedding params\n",
    "    embed = embed.to(device)\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(embed.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        l_sum, count = 0., 0.\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        for batch in data_iter:\n",
    "            # to device\n",
    "            centers, context_negatives, mask, label = [d.to(device) for d in batch]\n",
    "            # model\n",
    "            y_pred = model(centers, context_negatives, embed[0], embed[1])\n",
    "            # loss, y_pred.shape is (batch, 1, max_len), label.shape is (batch, max_len)\n",
    "            l = loss(y_pred.view(label.shape), label, mask)\n",
    "            # grad\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            l_sum += l.item()\n",
    "            count += 1\n",
    "            # training bar\n",
    "            process_bar.show_process(batch_num, 1, l_sum/count)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T12:57:43.008706Z",
     "start_time": "2020-11-13T12:57:42.959114Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化embedding\n",
    "d_model = 100\n",
    "\n",
    "embed = nn.Sequential(\n",
    "    nn.Embedding(len(vocab_set), d_model),\n",
    "    nn.Embedding(len(vocab_set), d_model),\n",
    ")\n",
    "\n",
    "# init train_iter\n",
    "dataset = Dataset(all_centers, all_contexts, all_negatives)\n",
    "train_iter = Data.DataLoader(dataset, batch_size=512, collate_fn=batchify, num_workers=4, shuffle=True)\n",
    "\n",
    "# loss\n",
    "loss = BinarySigmoidEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T12:57:51.548122Z",
     "start_time": "2020-11-13T12:57:47.765926Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"epoch_num\": 10,\n",
    "    \"batch_num\": len(list(train_iter)),\n",
    "    \"data_iter\": train_iter,\n",
    "    \"model\": skip_gram,\n",
    "    \"embed\": embed,\n",
    "    \"loss\": loss,\n",
    "    \"lr\": 0.01,\n",
    "    \"device\": \"cuda\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:00:23.000824Z",
     "start_time": "2020-11-13T12:57:51.551422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.9712, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [2/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6237, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [3/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.4501, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [4/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3951, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [5/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3692, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [6/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3535, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [7/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3419, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [8/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3325, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [9/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3243, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [10/10]\n",
      "732/732 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3173, train_score: -, test_loss: -, test_score: -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "trainer(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用词嵌入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以根据两个词向量的余弦相似度表示词与词在语义上的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:00:23.011480Z",
     "start_time": "2020-11-13T13:00:23.003453Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar_token(query_token, k, embed):\n",
    "    # 中心词的词向量, shape-->(len(vocab_set), d_model)\n",
    "    w = embed.weight.data\n",
    "    # shape-->(d_model, )\n",
    "    x = w[token_to_idx[query_token]]\n",
    "    # 添加1e-9增加稳定性, 求余弦相似度\n",
    "    cos = torch.matmul(w, x) / (torch.sum(w * w, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "#     cos = torch.mv(w, x) / (torch.norm(w, dim=1) * torch.norm(x))\n",
    "    # 返回的是相似度，以及对应的索引,其中第一个是x本身（为1），所以如果返回3个最相近的词，实际上是传入4\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print(\"cosine sim=%.3f: %s\" %(cos[i], vocab_set[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:03:17.931496Z",
     "start_time": "2020-11-13T13:03:17.926662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.490: machines\n",
      "cosine sim=0.456: caution\n",
      "cosine sim=0.442: speculation\n",
      "cosine sim=0.403: mainframes\n",
      "cosine sim=0.396: cray\n"
     ]
    }
   ],
   "source": [
    "get_similar_token(\"chip\", 5, embed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结：\n",
    "1. 二次采样试图尽可能减轻高频词对训练词嵌入模型的影响\n",
    "2. 可以将长度不同的样本填充到长度相同的小批量，并通过掩码变量区分非填充和填充项，只让填充项参加损失函数的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用gensim来训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:11:50.154164Z",
     "start_time": "2020-11-13T13:11:50.151738Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:14:55.617917Z",
     "start_time": "2020-11-13T13:14:46.213634Z"
    }
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(corpus, size=100, alpha=0.01, window=5, min_count=5, sg=1, hs=0, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:17:55.677645Z",
     "start_time": "2020-11-13T13:17:55.672816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine, sim:0.9848711490631104\n",
      "charging, sim:0.9845905303955078\n",
      "mainframe, sim:0.9842156171798706\n",
      "suspension, sim:0.9840888381004333\n",
      "dentsu, sim:0.98377925157547\n"
     ]
    }
   ],
   "source": [
    "for word in model.wv.similar_by_word(\"chip\", topn=5):\n",
    "    # word, sim\n",
    "    print(f\"{word[0]}, sim:{word[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
