{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T00:57:51.864770Z",
     "start_time": "2020-10-30T00:57:51.844367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:32:33.017263Z",
     "start_time": "2020-10-30T02:32:33.003843Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from data_prepare import download_data_fashion_mnist, load_data_fashion_mnist\n",
    "from model_train import train_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的目标是输入x，学到映射f(x)，ResNet指出先学到残差映射`f(x) - x`(更容易学到)，再通过残差连接`(f(x) - x) + x`得到f(x)\n",
    "- 残差模块一般包括两个3x3的卷积核，和可选的1x1卷积核（用于改变通道，获取空间信息），和GoogleNet有点不同的是，在3x3卷积核后面加上BN层，此时可以保证输入和输出的维度一样，以便于进行残差连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 残差模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T01:41:05.684902Z",
     "start_time": "2020-10-30T01:41:05.667749Z"
    }
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    \"\"\"\n",
    "    function: realize the Residual Module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_1x1=False, stride=1):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        # if use_1x1 conv, we define a 1x1 conv\n",
    "        if use_1x1:\n",
    "            self.conv3 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride)\n",
    "            )\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        # if not use 1x1 conv, add relu after conv2. otherwise, add relu after conv3.\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        # use 1x1 conv\n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "        return self.relu(output + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T01:41:06.782919Z",
     "start_time": "2020-10-30T01:41:06.763294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 3)\n",
    "x = torch.rand(4, 3, 6, 6)\n",
    "blk(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T01:41:07.693353Z",
     "start_time": "2020-10-30T01:41:07.675172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 6, True, 2)\n",
    "blk(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet和GoogleNet一样，先使用64个输出通道，stride=2, padding=3的卷积核，然后再接上3x3, stride=2的overlapping 最大池化，有点不一样的是，ResNet在每个卷积层后面加上BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和GoogleNet（接了4个Inception组成的模块）一样，ResNet后面也使用由4个由残差块组成的模块，每个模块使用若干个同样输出通道的残差块，第一个模块的通道数和输入通道数一样，因为之前使用过stride=2的最大池化，所以第一个模块不用减高宽，之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高宽减半"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个模块使用两个残差块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:20:34.945956Z",
     "start_time": "2020-10-30T02:20:34.934265Z"
    }
   },
   "outputs": [],
   "source": [
    "def residual_block(in_channels, out_channels, res_num, first_block=False):\n",
    "    \"\"\"\n",
    "    function: realize residual_block which has two Residual Module\n",
    "    params in_channels: the channels of input\n",
    "    params out_channels: the channels of output\n",
    "    params res_num: the number of residual module\n",
    "    params first_block: if the first block, the in_channels is equal to out_channels\n",
    "    \"\"\"\n",
    "    blk = []\n",
    "    if first_block:\n",
    "        assert in_channels == out_channels\n",
    "        \n",
    "    for num in range(res_num):\n",
    "        blk.append(Residual(in_channels, out_channels))\n",
    "    \n",
    "    if not first_block:\n",
    "        blk.append(nn.MaxPool2d(2))\n",
    "        \n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:53:05.047638Z",
     "start_time": "2020-10-30T02:53:05.038942Z"
    }
   },
   "outputs": [],
   "source": [
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "    \n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer = nn.Sequential(OrderedDict({\n",
    "            \"block1\": nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, padding=3, stride=2),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            ),\n",
    "            \n",
    "            \"block2\": residual_block(64, 64, 2, first_block=True),\n",
    "            \"block3\": residual_block(64, 128, 2),\n",
    "            \"block4\": residual_block(128, 256, 2),\n",
    "            \"block5\": residual_block(256, 512, 2),\n",
    "            # output shape (Batch, 512, 1, 1)\n",
    "            \"global_avg_pool\": GlobalAvgPool2d(),\n",
    "            \"fc\": nn.Sequential(\n",
    "                FlattenLayer(),\n",
    "                nn.Linear(512, 10)\n",
    "            )\n",
    "        }))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.forward(x)\n",
    "        acc = (y_pred.argmax(dim=1) == y).sum().item() / len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印网络结构\n",
    "model = ResNet()\n",
    "model = model.cuda()\n",
    "summary(model, input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = ResNet()\n",
    "model = model.cuda()\n",
    "# loss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load data\n",
    "mnist_train, mnist_test = download_data_fashion_mnist()\n",
    "\n",
    "# params\n",
    "params = {\n",
    "    \"epoch_num\": 15,\n",
    "    \"data_num\": len(mnist_train),\n",
    "    \"model\": model,\n",
    "    \"loss\": loss,\n",
    "    \"batch_size\": 128,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"evaulate\": model.score,\n",
    "    \"gpu\": True,\n",
    "    \"draw\": True,\n",
    "    \"save_fig\": True,\n",
    "}\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=params[\"batch_size\"], num_workers=8)\n",
    "params[\"train_iter\"] = train_iter\n",
    "params[\"test_iter\"] = test_iter\n",
    "\n",
    "# training\n",
    "train_epoch(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet又称稠密神经网络，与ResNet神经网络不同的是，残差连接改成在通道上的连接，而且这种连接不限于在稠密块的输入和输出之间，还应用在稠密块内部卷积块，即输入和各卷积块也进行通道的连接\n",
    "- ResNet的残差连接是应用在残差块的输入和输出之间\n",
    "- 这里像ResNet一样，DenseNet先经过一个64x1x7x7的卷积层（加BN），然后经过4个稠密块(每个稠密块有4个卷积层)，后面接着全局最大池化和fc\n",
    "- ResNet是通过stride来减少高宽，而DenseNet为了降低模型的复杂度，增加过渡层，使用1x1的卷积来减少通道数，使用stride=2的平均池化来减少高宽"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稠密块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T07:23:33.537600Z",
     "start_time": "2020-10-30T07:23:33.526954Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T07:23:33.687799Z",
     "start_time": "2020-10-30T07:23:33.681680Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    function: realize DenseBlock in DenseNet\n",
    "    params in_channels: the number of channels in input \n",
    "    params out_channels: Actually, the out_channels stands for the growth rate of concat, \n",
    "                         when concatenate the conv_block in DenseBlock\n",
    "    params num_conv: the number of conv layer in a DenseBlock\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_conv):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        block = []\n",
    "        for num in range(num_conv):\n",
    "            in_c = in_channels + num * out_channels\n",
    "            block.append(conv_block(in_c, out_channels))\n",
    "        self.block = nn.ModuleList(block)\n",
    "        # calculate the number of channels\n",
    "        self.out_channels = in_channels + num_conv * out_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for blk in self.block:\n",
    "            y = blk(x)\n",
    "            # concat in channels\n",
    "            x = torch.cat((x, y), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T06:15:27.700330Z",
     "start_time": "2020-10-30T06:15:27.687111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseBlock(\n",
       "  (block): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(13, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(3, 10, 2)\n",
    "blk\n",
    "x = torch.rand(4, 3, 8, 8)\n",
    "y = blk(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过渡块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T07:23:37.914006Z",
     "start_time": "2020-10-30T07:23:37.901965Z"
    }
   },
   "outputs": [],
   "source": [
    "def transition_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(2)\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T06:21:47.382421Z",
     "start_time": "2020-10-30T06:21:47.360230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过渡块验证，使用1x1 conv来改变通道数，平均池化来减少高宽\n",
    "blk = transition_block(23, 10)\n",
    "# 从通道23到10, 高宽从8到4\n",
    "blk(y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T07:23:40.912264Z",
     "start_time": "2020-10-30T07:23:40.863152Z"
    }
   },
   "outputs": [],
   "source": [
    "DenseNet = nn.Sequential()\n",
    "# block1\n",
    "DenseNet.add_module(\"block1\", nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, padding=3, stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, padding=1, stride=2),\n",
    "))\n",
    "# dense_block and transition block\n",
    "in_channels, growth_channels = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    # DenseBlock\n",
    "    dense_block = DenseBlock(in_channels, growth_channels, num_convs)\n",
    "    DenseNet.add_module(\"den_block%d\" %(i+2), dense_block)\n",
    "    in_channels = dense_block.out_channels\n",
    "    \n",
    "    # transition block\n",
    "    # In the last dense_block, we not add transition block\n",
    "    if i != (len(num_convs_in_dense_blocks) - 1):\n",
    "        DenseNet.add_module(\"trans_block%d\" %(i+2), transition_block(in_channels, in_channels//2))\n",
    "        in_channels = in_channels // 2\n",
    "        \n",
    "# bn\n",
    "DenseNet.add_module(\"bn\", nn.BatchNorm2d(in_channels))\n",
    "DenseNet.add_module(\"relu\", nn.ReLU())\n",
    "DenseNet.add_module(\"gapool\", GlobalAvgPool2d())\n",
    "DenseNet.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(in_channels, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet()\n",
    "model = model.cuda()\n",
    "summary(model, input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T07:23:41.024642Z",
     "start_time": "2020-10-30T07:23:41.019181Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (den_block2): DenseBlock(\n",
       "    (block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans_block2): Sequential(\n",
       "    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (den_block3): DenseBlock(\n",
       "    (block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans_block3): Sequential(\n",
       "    (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(224, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (den_block4): DenseBlock(\n",
       "    (block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(112, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(144, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(176, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(208, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans_block4): Sequential(\n",
       "    (0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(240, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (den_block5): DenseBlock(\n",
       "    (block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(120, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(152, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(184, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(216, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (gapool): GlobalAvgPool2d()\n",
       "  (fc): Sequential(\n",
       "    (0): FlattenLayer()\n",
       "    (1): Linear(in_features=248, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = ResNet()\n",
    "model = model.cuda()\n",
    "# loss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load data\n",
    "mnist_train, mnist_test = download_data_fashion_mnist()\n",
    "\n",
    "# params\n",
    "params = {\n",
    "    \"epoch_num\": 15,\n",
    "    \"data_num\": len(mnist_train),\n",
    "    \"model\": model,\n",
    "    \"loss\": loss,\n",
    "    \"batch_size\": 128,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"evaulate\": model.score,\n",
    "    \"gpu\": True,\n",
    "    \"draw\": True,\n",
    "    \"save_fig\": True,\n",
    "}\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=params[\"batch_size\"], num_workers=8)\n",
    "params[\"train_iter\"] = train_iter\n",
    "params[\"test_iter\"] = test_iter\n",
    "\n",
    "# training\n",
    "train_epoch(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
