{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T04:10:56.143467Z",
     "start_time": "2020-10-31T04:10:56.125228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T04:10:56.947054Z",
     "start_time": "2020-10-31T04:10:56.264464Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from sqdm import sqdm\n",
    "from draw import set_fig_display\n",
    "from decorate import cal_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T04:10:56.952176Z",
     "start_time": "2020-10-31T04:10:56.948578Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_error_sample(list_length, sample_rate, equal_sample=True):\n",
    "    \"\"\"\n",
    "    function: sample with list, return sample index(two choice: equal distance in sample and unequal distance)\n",
    "    \"\"\"\n",
    "    if equal_sample:\n",
    "        index = map(lambda x: int(x), list(np.linspace(0, list_length, int(list_length * sample_rate))))\n",
    "    else:\n",
    "        index_array = np.arange(list_length)\n",
    "        index = np.random.choice(index_array, replace=False,\n",
    "                                 size=int(list_length * sample_rate))\n",
    "        index.sort()\n",
    "    return list(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T00:29:38.608200Z",
     "start_time": "2020-10-24T00:29:38.597525Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-3-b00a7b4e2558>, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b00a7b4e2558>\"\u001b[0;36m, line \u001b[0;32m66\u001b[0m\n\u001b[0;31m    if train_epoch:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class TrainNet(object):\n",
    "    def __init__(self, data_num, epoch_num, model, loss, train_iter, batch_size,\n",
    "                optimizer=None, test_iter=None, evaluate=None, draw=False,\n",
    "                draw_epoch=True, draw_mean=False, save_fig=False, save_path=\"./img/\",\n",
    "                accum_step=1, gpu=False, sample_rate=0, equal_sample=True):\n",
    "        # params\n",
    "        self.params = {\n",
    "            \"data_num\": data_num,\n",
    "            \"epoch_num\": epoch_num,\n",
    "            'model': model,\n",
    "            \"loss\": loss,\n",
    "            \"train_iter\": train_iter,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"test_iter\": test_iter,\n",
    "            \"evaulate\": evaluate,\n",
    "            \"draw\": draw,\n",
    "            \"draw_epoch\": draw_epoch,\n",
    "            \"draw_mean\": draw_mean,\n",
    "            \"save_fig\": save_fig,\n",
    "            \"save_path\": save_path,\n",
    "            \"accum_step\": accum_step,\n",
    "            \"gpu\": gpu,\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"equal_sample\": equal_sample,\n",
    "        }\n",
    "        # storage loss and score in train and test\n",
    "        self.train_loss_list, self.test_loss_list = [], []\n",
    "        self.train_score_list, self.test_score_list = [], []\n",
    "        # iteration num in each epoch\n",
    "        self.iter_num = np.ceil(data_num / batch_size)\n",
    "        # sample\n",
    "        self.sample_index = loss_error_sample(data_num, sample_rate, equal_sample)\n",
    "        \n",
    "    @cal_time\n",
    "    def train_epoch():\n",
    "        for epoch in range(self.params[\"epoch_num\"]):\n",
    "            print(f\"Epoch [{epoch + 1}/{self.params['epoch_num']}]\")\n",
    "            count, mean_train_loss, mean_train_score = 1., 0., 0.\n",
    "            test_num, mean_test_loss, mean_test_score = 0., 0., 0.\n",
    "            \n",
    "            # load data\n",
    "            for x, y in self.params[\"train_iter\"]:\n",
    "                # cuda available and want to use gpu\n",
    "                if torch.cuda.is_available() and self.params[\"gpu\"]:\n",
    "                    # send tensor from cpu to gpu\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                # train\n",
    "                model.train()\n",
    "                train_pred = self.params[\"model\"](x)\n",
    "                train_loss = self.params[\"loss\"](train_pred, y) / accum_\n",
    "                # calculate mean train loss\n",
    "                mean_train_loss = (((count - 1) * mean_train_loss +\n",
    "                                    accum_step * train_loss) / count).item()\n",
    "                if draw_epoch:\n",
    "                    if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cal_time\n",
    "def train_epoch(data_num, epoch_num, model, loss, train_iter, batch_size,\n",
    "                optimizer=None, test_iter=None, evaluate=None, draw=False,\n",
    "                draw_mean=False, save_fig=False, save_path=\"./img/\",\n",
    "                accum_step=1, gpu=False):\n",
    "    \"\"\"\n",
    "    function: training in pytorch (only with epoch), it will speed up in training in epoch\n",
    "    params data_num: the number of sample in train set\n",
    "    params epoch_num: the number of epoch\n",
    "    params model: model which fit data\n",
    "    params loss: calculate loss function\n",
    "    params train_iter: train data loader\n",
    "    params optimizer: torch optimizer which is used to update grad\n",
    "    params test_iter: test data loader, use in testing\n",
    "    params evaluate: criterion such as acc/f1 score\n",
    "    params draw: draw figure with loss and score in train/test whether or not\n",
    "    params draw_epoch: draw with data in iteration or epoch\n",
    "    params save_fig: save figure whether or not\n",
    "    params save_path: the path of saving figure\n",
    "    params accum_step: use in gradient accumulation, after the number of accum_step,\n",
    "                        it will be update the grad of parameters\n",
    "    params gpu: if want to use gpu and cuda is available, it will send tensor to gpu\n",
    "    \"\"\"\n",
    "    # training bar\n",
    "    process_bar = sqdm()\n",
    "\n",
    "    # init\n",
    "    test_loss = test_score = \"-\"\n",
    "    # storage loss and score in train and test\n",
    "    train_loss_list, test_loss_list = [], []\n",
    "    train_score_list, test_score_list = [], []\n",
    "    # iteration num in each epoch\n",
    "    iter_num = np.ceil(data_num / batch_size)\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch + 1}/{epoch_num}]\")\n",
    "        count, mean_train_loss, mean_train_score = 1., 0., 0.\n",
    "        test_num, mean_test_loss, mean_test_score = 0., 0., 0.\n",
    "        for x, y in train_iter:\n",
    "            # cuda available and want to use gpu\n",
    "            if torch.cuda.is_available() and gpu:\n",
    "                # send tensor from cpu to gpu\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            # train\n",
    "            # model.train()\n",
    "            train_pred = model(x)\n",
    "            train_loss = loss(train_pred, y) / accum_step\n",
    "            # calculate mean train loss\n",
    "            mean_train_loss = (((count - 1) * mean_train_loss +\n",
    "                                accum_step * train_loss) / count).item()\n",
    "            # when draw_mean is True, we save the mean_train_loss in each iteration,\n",
    "            # otherwise, we save the train_loss in the last iteration in each epoch\n",
    "            if count == iter_num:\n",
    "                if draw_mean:\n",
    "                    train_loss_list.append(mean_train_loss)\n",
    "                else:\n",
    "                    train_loss_list.append(accum_step * train_loss)\n",
    "            # if parameter have criterion(evaluate), like accuracy/f1_score\n",
    "            # use this criterion to calculate train_score\n",
    "            if evaluate is not None:\n",
    "                train_score = evaluate(x, y)\n",
    "                mean_train_score = ((count - 1) * mean_train_score +\n",
    "                                    train_score) / count\n",
    "                # function like the draw_epoch in train loss\n",
    "                if count == iter_num:\n",
    "                    if draw_mean:\n",
    "                        train_score_list.append(mean_train_score)\n",
    "                    else:\n",
    "                        train_score_list.append(train_score)\n",
    "\n",
    "            # bp\n",
    "            train_loss.backward()\n",
    "            if (count % accum_step) == 0:\n",
    "                # grad update\n",
    "                optimizer.step()\n",
    "                # clear grad\n",
    "                optimizer.zero_grad()\n",
    "            if count == iter_num:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # test loss\n",
    "            with torch.no_grad():\n",
    "                if (test_iter is not None) and (count == iter_num):\n",
    "                    # eval model, it will stop dropout and batch normalization\n",
    "                    model.eval()\n",
    "                    for test_data, test_label in test_iter:\n",
    "                        # calculate the num of test set\n",
    "                        test_num += len(test_label)\n",
    "                        # if cuda available and want to use gpu\n",
    "                        if torch.cuda.is_available() and gpu:\n",
    "                            test_data = test_data.cuda()\n",
    "                            test_label = test_label.cuda()\n",
    "\n",
    "                        test_pred = model(test_data)\n",
    "                        test_loss = loss(test_pred, test_label).item()\n",
    "                        mean_test_loss += len(test_label) * test_loss\n",
    "                        # print(mean_test_loss)\n",
    "                        # use this criterion to calculate test_score\n",
    "                        if evaluate is not None:\n",
    "                            test_score = evaluate(test_data, test_label)\n",
    "                            mean_test_score += len(test_label) * test_score\n",
    "                    # result\n",
    "                    mean_test_loss /= test_num\n",
    "                    test_loss_list.append(mean_test_loss)\n",
    "                    if evaluate is not None:\n",
    "                        mean_test_score /= test_num\n",
    "                        test_score_list.append(mean_test_score)\n",
    "                    # change to train model\n",
    "                    model.train()\n",
    "\n",
    "            # update counter\n",
    "            count += 1\n",
    "            # training bar\n",
    "            if evaluate is None:\n",
    "                mean_train_score = \"-\"\n",
    "                mean_test_score = \"-\"\n",
    "            if test_iter is None:\n",
    "                mean_test_loss = \"-\"\n",
    "                mean_test_score = \"-\"\n",
    "            # use mean loss and score in each epoch\n",
    "            process_bar.show_process(data_num, batch_size=batch_size,\n",
    "                                     train_loss=mean_train_loss,\n",
    "                                     train_score=mean_train_score,\n",
    "                                     test_loss=mean_test_loss,\n",
    "                                     test_score=mean_test_score)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # draw loss figure and score figure\n",
    "    # especially, loss in train dataset use train_loss not mean_loss in drawing\n",
    "    if draw:\n",
    "        # set figure format\n",
    "        set_fig_display(axes_spines_state=[True] * 4)\n",
    "        # add new figure and subplot\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(train_loss_list)), train_loss_list,\n",
    "                 label=\"train_loss\")\n",
    "        if len(test_loss_list) > 0:\n",
    "            ax1.plot(range(len(train_loss_list)), test_loss_list,\n",
    "                     label=\"test_loss\")\n",
    "        ax1.set_xlabel(\"epoch num\")\n",
    "        ax1.set_ylabel(\"loss\")\n",
    "\n",
    "        # ax2\n",
    "        if evaluate is not None:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(range(len(train_score_list)), train_score_list, \"c-\",\n",
    "                     label=\"train_score\", alpha=0.8)\n",
    "            if len(test_score_list) > 0:\n",
    "                ax2.plot(range(len(train_score_list)), test_score_list,\n",
    "                         \"r-\", label=\"test_score\", alpha=0.8)\n",
    "            ax2.set_ylabel(\"score\")\n",
    "            ax2.set_ylim([0, 1.1])\n",
    "\n",
    "        if test_iter is None and evaluate is None:\n",
    "            legend_labels = [\"train_loss\"]\n",
    "            legend_loc = [0.75, 0.82]\n",
    "        elif test_iter is None:\n",
    "            legend_labels = [\"train_loss\", \"train_score\"]\n",
    "            legend_loc = [0.58, 0.82]\n",
    "        elif evaluate is None:\n",
    "            legend_labels = [\"train_loss\", \"test_loss\"]\n",
    "            legend_loc = [0.595, 0.82]\n",
    "        else:\n",
    "            legend_labels = [\"train_loss\", \"test_loss\",\n",
    "                             \"train_score\", \"test_score\"]\n",
    "            legend_loc = [0.58, 0.78]\n",
    "        fig.legend(labels=legend_labels, ncol=2, loc=legend_loc)\n",
    "\n",
    "        if save_fig:\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            fig_name = \"fig\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".jpg\"\n",
    "            plt.savefig(save_path + fig_name, dpi=200)\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
