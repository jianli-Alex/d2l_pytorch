{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:27.535661Z",
     "start_time": "2020-10-12T03:11:27.522947Z"
    }
   },
   "outputs": [],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二维卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络是含有卷积层的神经网络\n",
    "- 我们用得最多就是二维的卷积层(有高宽两个维度)\n",
    "- 多输入通道数和多输出通道数的卷积层都是在这上面进行扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和互相关运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常来说,我们在卷积神经网络上使用的卷积运算并不是真正的卷积运算,其通常是使用互相关运算\n",
    "- 卷积运算是先将卷积核上下左右翻转后,再对输入数据进行互相关运算\n",
    "- 但由于深度学习中,卷积核的参数都是学习出来的,无论使用互相关运算或者是真正的卷积运算都不影响模型预测时的输出\n",
    "- 卷积核其实就是一个特征提取器,运算后的输出可以看成是输入在空间维度(高和宽)上某一级的表征\n",
    "- 考虑单通道输入和输出,将输入的高宽分别记为$I_h$和$I_w$,卷积核的高宽分别记为$K_h$和$K_w$,输出的的高宽分别记为$O_h$和$O_w$,对高宽进行的padding分别记为$p_h$和$p_w$,对高宽进行的stride分别记为$s_h$和$s_w$,那么对于以下几种情况的卷积输出分别为:\n",
    "    - 无padding和stride: $O_h = I_h - k_h + 1$, $O_w = I_w - k_w + 1$\n",
    "    - 有padding和无stride: $O_h = I_h - k_h + p_h + 1$, $O_h = I_w- k_w + p_w + 1$\n",
    "    - 无padding和有stride: $(O_h = I_h - k_h)/s_h + 1$, $(O_w = I_w - k_w)/s_w + 1$\n",
    "    - 有padding和有stride: $(O_h = I_h - k_h + p_h)/s_h + 1$, $(O_w = I_w - k_w + p_w)/s_w + 1$\n",
    "        - 其实第四条就能包括前三条,这里只是列得仔细点,另外p指的是两边一共padding的数量,有的书是用2p(这实际上是指单边的padding数量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过我们会将padding设为$k-1$,这样就能获得和输入同shape的tensor(这也叫等宽卷积)\n",
    "- 卷积核通常也是奇数,这样两端的padding一样,否则为偶数时,一边的padding需要向上取整,另一边padding要向下取整\n",
    "- 目前多用小的卷积核(像1x1, 3x3等)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T13:14:55.160755Z",
     "start_time": "2020-10-09T13:14:55.136989Z"
    }
   },
   "source": [
    "我们可以通过更深的网络结构来让感受野变得更加广阔,从而捕捉输入上更大尺寸特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积个人实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在二维互相关运算中(如无特殊说明,深度学习中的卷积就是指互相关运算)\n",
    "- 就是卷积窗口从输入数组的最上方开始,从左到右,从上到下的顺序,依次做滑窗运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.003865Z",
     "start_time": "2020-10-12T03:11:27.537380Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.009371Z",
     "start_time": "2020-10-12T03:11:28.005306Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d(x, k):\n",
    "    \"\"\"\n",
    "    功能: 实现卷积操作(无padding/无stride)\n",
    "    参数 x: 输入数据\n",
    "    参数 k; 传入一个卷积核\n",
    "    \"\"\"\n",
    "    # 获取卷积核的大小\n",
    "    h, w = k.shape\n",
    "    # 定义输出的shpe\n",
    "    y = torch.rand((x.shape[0] - h + 1, x.shape[1] - w + 1))\n",
    "    # 卷积运算\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            y[i, j] = (x[i:i+h, j:j+w] * k).sum()\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.024844Z",
     "start_time": "2020-10-12T03:11:28.010915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9).view(3, 3)\n",
    "k = torch.arange(4).view(2, 2)\n",
    "\n",
    "# 卷积运算\n",
    "conv2d(x, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T11:07:09.893140Z",
     "start_time": "2020-10-09T11:07:09.881886Z"
    }
   },
   "source": [
    "### 自定义卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.029288Z",
     "start_time": "2020-10-12T03:11:28.026478Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.053940Z",
     "start_time": "2020-10-12T03:11:28.031046Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    \"\"\"自定义实现卷积层\"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return conv2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积窗口形状为pxq的卷积层称为pxq卷积层\n",
    "- 说明卷积核的高和宽分别为p和q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像的物体边缘检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用卷积层来检测图像中的物体边缘(找到像素变化的位置)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.065008Z",
     "start_time": "2020-10-12T03:11:28.057273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个6*8的图像,中间4列为黑,其余为白\n",
    "x = torch.ones(6, 8)\n",
    "x[:, 2:6] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为实际上是测相邻的元素是否有变化,因此可以定义一个1*2的卷积核[[-1, 1]],只要相邻两行出现变化,卷积计算出来的就不为0,如果没有变化就为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义卷积核的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.081172Z",
     "start_time": "2020-10-12T03:11:28.068471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.tensor([[-1, 1]])\n",
    "y = conv2d(x, k)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层可以通过重复使用卷积核有效地表征局部空间\n",
    "\n",
    "像\n",
    "```\n",
    "[[1, 0, -1\n",
    "1, 0, -1\n",
    "1, 0, -1]] 叫做垂直边缘过滤器，其不但能检测边缘，也能区别明暗\n",
    "\n",
    "[[-1, 0, 1\n",
    "-1, 0, 1\n",
    "-1, 0, 1]] 叫做水平边缘过滤器\n",
    "\n",
    "[[1, 0, -1\n",
    "2, 0, -2\n",
    "1, 0, -1]] 叫做sobel过滤器，有更强的鲁棒性\n",
    "\n",
    "[[3, 0, -3\n",
    "10, 0, -10\n",
    "3， 0， -3]] 叫做scharr过滤器\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.087973Z",
     "start_time": "2020-10-12T03:11:28.083660Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from optim import sgd\n",
    "from sqdm import sqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.093102Z",
     "start_time": "2020-10-12T03:11:28.089845Z"
    }
   },
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    return ((y_pred - y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.733101Z",
     "start_time": "2020-10-12T03:11:28.094575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 41.5285, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [2/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 16.2697, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [3/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 11.2531, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [4/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 8.3722, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [5/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 6.2939, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [6/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 4.7535, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [7/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 3.6036, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [8/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 2.7406, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [9/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 2.0900, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [10/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.5977, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [11/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.2239, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [12/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9391, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [13/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7217, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [14/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.5553, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [15/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.4277, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [16/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3297, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [17/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2543, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [18/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1963, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [19/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1516, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [20/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1172, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [21/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0906, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [22/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0700, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [23/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0542, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [24/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0419, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [25/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0324, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [26/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0251, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [27/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0194, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [28/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0150, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [29/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0116, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [30/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0090, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [31/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0070, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [32/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0054, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [33/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0042, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [34/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0032, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [35/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0025, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [36/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0019, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [37/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0015, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [38/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0012, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [39/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0009, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [40/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0007, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [41/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0005, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [42/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0004, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [43/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0003, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [44/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0003, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [45/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0002, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [46/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0002, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [47/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [48/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [49/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [50/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [51/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [52/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [53/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [54/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [55/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [56/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [57/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [58/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [59/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [60/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [61/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [62/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [63/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [64/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [65/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [66/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [67/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [68/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [69/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [70/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [71/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [72/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [73/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [74/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [75/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [76/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [77/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [78/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [79/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [80/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [81/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [82/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [83/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [84/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [85/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [86/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [87/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [88/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [89/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [90/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [91/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [92/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [93/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [94/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [95/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [96/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [97/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [98/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [99/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [100/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model  = Conv2D(k.shape)\n",
    "loss = squared_loss\n",
    "epoch_num = 100\n",
    "lr = 0.01\n",
    "weight_decay = 0\n",
    "\n",
    "process_bar = sqdm()\n",
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "    y_pred = model(x)\n",
    "    l = loss(y_pred, y)\n",
    "    l.backward()\n",
    "    \n",
    "    sgd([model.weight, model.bias], lr=lr, weight_decay=weight_decay)\n",
    "#     _ = model.weight.grad.data.zero_()\n",
    "#     _ = model.bias.grad.data.zero_()\n",
    "    _ = model.weight.grad.fill_(0)\n",
    "    _ = model.bias.grad.fill_(0)\n",
    "    \n",
    "    process_bar.show_process(data_num=1, batch_size=1, train_loss=l.item())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.755350Z",
     "start_time": "2020-10-12T03:11:28.744762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0000,  1.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-5.9854e-14], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "model.weight\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果和自定义的[[-1, 1]]一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带padding卷积的个人实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.761059Z",
     "start_time": "2020-10-12T03:11:28.756812Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.767463Z",
     "start_time": "2020-10-12T03:11:28.762539Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_padding(x, k, padding=0):\n",
    "    \"\"\"\n",
    "    function: 实现带padding的卷积\n",
    "    params x: 输入张量\n",
    "    params k: 巻积核\n",
    "    params padding: 传入padding的元组(h, w),高padding多少,宽padding多少,\n",
    "                    如果为整数就高宽一样, padding是指两端都填充相同数量的零\n",
    "    \"\"\"\n",
    "    assert padding >= 0\n",
    "    if padding == 0:\n",
    "        return conv2d(x, k)\n",
    "    else:\n",
    "        if isinstance(padding, int):\n",
    "            h = w = padding\n",
    "        else:\n",
    "            h, w = padding\n",
    "            \n",
    "        x = x.numpy()\n",
    "        # 前一个元组是增加上下（列），后一个元组是增加左右（宽）\n",
    "        x = torch.from_numpy(np.pad(x, ((h, h), (w, w))))\n",
    "        return conv2d(x, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.782024Z",
     "start_time": "2020-10-12T03:11:28.769528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等宽巻积\n",
    "x = torch.rand(8, 8)\n",
    "k = torch.rand(3, 3)\n",
    "\n",
    "conv2d_padding(x, k, padding=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.795823Z",
     "start_time": "2020-10-12T03:11:28.789522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  8.,  4.],\n",
       "        [ 9., 19., 25., 10.],\n",
       "        [21., 37., 43., 16.],\n",
       "        [ 6.,  7.,  8.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和pytorch自带Conv2d的结果对比\n",
    "x = torch.arange(9.).view(3, 3)\n",
    "k = torch.arange(4.).view(2, 2)\n",
    "conv2d_padding(x, k, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.800420Z",
     "start_time": "2020-10-12T03:11:28.797400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用pytorch实现\n",
    "def compute_conv2d(x, k):\n",
    "    # pytorch实现的是四维的卷积核，先为x添加维度\n",
    "    x = x.view((1, 1) + x.shape)\n",
    "    result = k(x)\n",
    "    return result.view(result.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.815598Z",
     "start_time": "2020-10-12T03:11:28.801870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  8.,  4.],\n",
       "        [ 9., 19., 25., 10.],\n",
       "        [21., 37., 43., 16.],\n",
       "        [ 6.,  7.,  8.,  0.]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=2, padding=1)\n",
    "conv.weight.data = k.view((1,1) + k.shape)\n",
    "conv.bias.data = torch.zeros(1)\n",
    "\n",
    "compute_conv2d(x, conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带stride的巻积个人实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.833325Z",
     "start_time": "2020-10-12T03:11:28.817260Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_padding_stride(x, k, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    function: 实现带padding和stride的巻积\n",
    "    params x: 输入张量\n",
    "    params k: 巻积核\n",
    "    params padding: 传入padding的元组(h, w),指两端填充一样的值\n",
    "    params stride: 默认为1, 传入(h, w)或者整数\n",
    "    \"\"\"\n",
    "    if stride == 1:\n",
    "        return conv2d_padding(x, k, padding)\n",
    "    else:\n",
    "        kh, kw = k.shape\n",
    "        if isinstance(padding, int):\n",
    "            ph = pw = padding\n",
    "        else:\n",
    "            ph, pw = padding\n",
    "        \n",
    "        if isinstance(stride, int):\n",
    "            sh = sw = stride\n",
    "        else:\n",
    "            sh, sw = stride\n",
    "        \n",
    "        y = torch.zeros((x.shape[0]-kh+2*ph+sh)//sh, \n",
    "                        (x.shape[1]-kw+2*pw+sw)//sw)\n",
    "        x = torch.from_numpy(np.pad(x.numpy(), ((ph, ph), (pw, pw))))\n",
    "        for i in range(y.shape[0]):\n",
    "            for j in range(y.shape[1]):\n",
    "                y[i, j] = (x[(i*sh):(i*sh+kh), (j*sw):(j*sw+kw)] * k).sum()\n",
    "                \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.854559Z",
     "start_time": "2020-10-12T03:11:28.839033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4132, 0.4866, 1.4444, 0.9264],\n",
       "        [0.6863, 1.1458, 2.2513, 1.7484],\n",
       "        [1.1097, 1.0538, 1.7666, 1.6823],\n",
       "        [1.2972, 1.3523, 1.7586, 1.7562]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4132, 0.4866, 1.4444, 0.9264],\n",
       "        [0.6863, 1.1458, 2.2513, 1.7484],\n",
       "        [1.1097, 1.0538, 1.7666, 1.6823],\n",
       "        [1.2972, 1.3523, 1.7586, 1.7562]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试自定义和使用nn实现的结果\n",
    "test_x = torch.rand(8, 8)\n",
    "test_k = torch.rand(3, 3)\n",
    "conv2d_padding_stride(test_x, test_k, 1, 2)\n",
    "conv2d_padding_stride(test_x, test_k, 1, 2).shape\n",
    "\n",
    "\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, stride=2)\n",
    "conv.weight.data = test_k.view((1, 1) + test_k.shape)\n",
    "conv.bias.data = torch.zeros(1)\n",
    "compute_conv2d(test_x, conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.890025Z",
     "start_time": "2020-10-12T03:11:28.856297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3842, 3.7897],\n",
       "        [2.6026, 4.7549]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.3842, 3.7897],\n",
       "        [2.6026, 4.7549]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试自定义和使用nn实现的结果(复杂一点)\n",
    "test_x = torch.rand(8, 8)\n",
    "test_k = torch.rand(3, 5)\n",
    "padding = (0, 1)\n",
    "stride = (3, 4)\n",
    "conv2d_padding_stride(test_x, test_k, padding, stride)\n",
    "conv2d_padding_stride(test_x, test_k, padding, stride).shape\n",
    "\n",
    "\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=padding, stride=stride)\n",
    "conv.weight.data = test_k.view((1, 1) + test_k.shape)\n",
    "conv.bias.data = torch.zeros(1)\n",
    "compute_conv2d(test_x, conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多通道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多通道输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑多通道输入和多通道输出，正如pytorch一样，卷积核默认是四维的\n",
    "- 当输入数据是多通道的时候，我们要构造一个输入通道数与输入数据通道数相同的卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.899549Z",
     "start_time": "2020-10-12T03:11:28.894395Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_multi_in(x, k, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    function: 实现多通道的卷积运算，单通道输入\n",
    "    params x: 多通道的数据\n",
    "    params k: 多通道的卷积核\n",
    "    \"\"\"\n",
    "    result = conv2d_padding_stride(x[0], k[0], padding, stride)\n",
    "    for i in range(1, x.shape[0]):\n",
    "        result += conv2d_padding_stride(x[i], k[i], padding, stride)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.925936Z",
     "start_time": "2020-10-12T03:11:28.901598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [3, 4]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = torch.arange(9).view(3, 3), torch.arange(1, 10).view(3, 3)\n",
    "k1, k2 = torch.arange(4).view(2, 2), torch.arange(1, 5).view(2, 2)\n",
    "X, K = torch.stack((x1, x2)), torch.stack((k1, k2))\n",
    "X\n",
    "K\n",
    "\n",
    "conv2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多通道输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当有多输入通道的时候，我们实际上做了累计，所以输出通道都是0，如果想拿到多输出通道就对每个输出通道分别创建满足3维度的卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.931549Z",
     "start_time": "2020-10-12T03:11:28.927708Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_multi_in_out(X, K):\n",
    "    \"\"\"\n",
    "    function: 实现多通道输出，k是四维的卷积核\n",
    "    \"\"\"\n",
    "    result = torch.stack([conv2d_multi_in(X, k) for k in K])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.942597Z",
     "start_time": "2020-10-12T03:11:28.933615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = torch.stack((K, K+1, K+2))\n",
    "test_k.shape\n",
    "\n",
    "conv2d_multi_in_out(X, test_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为使用了最小的窗口，所以1x1卷积失去了可以识别高和宽相邻元素构成模式的功能\n",
    "- 1x1卷积的主要计算发生在通道维上\n",
    "- 如果把高宽元素当成样本，通道维当作是特征，那么1x1卷积作用就等价于全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.948348Z",
     "start_time": "2020-10-12T03:11:28.944285Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_multi_in_out_1x1(x, k):\n",
    "    \"\"\"\n",
    "    function：实现1x1卷积\n",
    "    params x: 是一个多通道的输入(3维度)\n",
    "    params k：是一个多通道的1x1卷积核(4维度)\n",
    "    \"\"\"\n",
    "    # 将高宽的元素认为是输入数据，将通道认为是特征，进行拉长\n",
    "    x = x.view(3, -1)\n",
    "    # 将两个通道的数据拼接起来\n",
    "    x = torch.stack((x, x))\n",
    "    # 将卷积核的数据按照mm转化\n",
    "    k = k.view(2, 1, 3)\n",
    "    # 批量的矩阵相乘\n",
    "    result = torch.bmm(k, x)\n",
    "    return result.view(2, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.962778Z",
     "start_time": "2020-10-12T03:11:28.950070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 45,  48,  51],\n",
       "         [ 54,  57,  60],\n",
       "         [ 63,  66,  69]],\n",
       "\n",
       "        [[126, 138, 150],\n",
       "         [162, 174, 186],\n",
       "         [198, 210, 222]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 45.,  48.,  51.],\n",
       "         [ 54.,  57.,  60.],\n",
       "         [ 63.,  66.,  69.]],\n",
       "\n",
       "        [[126., 138., 150.],\n",
       "         [162., 174., 186.],\n",
       "         [198., 210., 222.]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证1x1卷积和多通道输入输出卷积运算是否一样\n",
    "x = torch.arange(27).view(3, 3, 3)\n",
    "k = torch.arange(6).view(2, 3, 1, 1)\n",
    "conv2d_multi_in_out_1x1(x, k)\n",
    "\n",
    "conv2d_multi_in_out(x, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结：\n",
    "- 使用多通道可以拓展卷积层的模型参数\n",
    "- 假设将通道当作特征维，将高和宽维度上的元素当成数据样本，那么1x1卷积作用和全连接一样\n",
    "- 1x1卷积通常用来调整网络层之间的通道数，并控制模型复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际图像中，我们感兴趣的物体不会总是出现在固定的位置\n",
    "- 池化： 缓解卷积层对位置的过度敏感性\n",
    "- 通常用最大池化和平均池化\n",
    "- 在pytorch中，池化的卷积核为(3, 3)，那么stride也为(3, 3)\n",
    "    - 只要卷积层识别模型在高和宽上移动不超过一个元素，依然可以将它检测出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化个人实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:28.971840Z",
     "start_time": "2020-10-12T03:11:28.964875Z"
    }
   },
   "outputs": [],
   "source": [
    "# 这里实现的是默认带stride, 即卷积核多大，stride就多大\n",
    "def pool2d(x, kernel_size, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    function: 实现池化\n",
    "    params x: 输入数据\n",
    "    params kernel_size: 卷积核的size\n",
    "    params mode: mode为max 或者 mean\n",
    "    \"\"\"\n",
    "    x = x.float()\n",
    "    if isinstance(kernel_size, int):\n",
    "        h = w = kernel_size\n",
    "    else:\n",
    "        h, w = kernel_size\n",
    "    y = torch.zeros(x.shape[0]//h, x.shape[1]//w)\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            if mode.lower() == \"max\":\n",
    "                y[i, j] = x[(i*h):(i*h+h), (j*w):(j*w+w)].max()\n",
    "            else:\n",
    "                y[i, j] = x[(i*h):(i*h+h), (j*w):(j*w+w)].mean()\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:43:30.993139Z",
     "start_time": "2020-10-12T03:43:30.973754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9.).view(3, 3)\n",
    "pool2d(x, (2, 2))\n",
    "pool2d(x, (2, 2), mode=\"mean\")\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=2)\n",
    "pool(x.view((1, 1)+x.shape))\n",
    "\n",
    "pool = nn.AvgPool2d(kernel_size=2)\n",
    "pool(x.view((1, 1)+x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自由改变padding和stride实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:29.001330Z",
     "start_time": "2020-10-12T03:11:28.984557Z"
    }
   },
   "outputs": [],
   "source": [
    "def pool2d_padding_stride(x, kernel_size, padding=0, stride=None, mode=\"max\"):\n",
    "    \n",
    "    if isinstance(kernel_size, int):\n",
    "        kh = kw = kernel_size\n",
    "    else:\n",
    "        kh, kw = kernel_size\n",
    "        \n",
    "    if stride == None:\n",
    "        sh, sw = kh, kw\n",
    "    else:\n",
    "        if isinstance(stride, int):\n",
    "            sh = sw = stride\n",
    "        else:\n",
    "            sh, sw = stride\n",
    "        \n",
    "    if padding == 0:\n",
    "        return pool2d(x, kernel_size)\n",
    "    else:\n",
    "        if isinstance(padding, int):\n",
    "            ph = pw = padding\n",
    "        else:\n",
    "            ph, pw = padding\n",
    "            \n",
    "    y = torch.zeros((x.shape[0]-kh+2*ph+sh)//sh, (x.shape[1]-kw+2*pw+sw)//sw)\n",
    "    x = torch.from_numpy(np.pad(x.numpy(), ((ph, ph), (pw, pw))))\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            if mode.lower() == \"max\":\n",
    "                y[i, j] = x[(i*sh):(i*sh+kh), (j*sw):(j*sw+kw)].max()\n",
    "            else:\n",
    "                y[i, j] = x[(i*sh):(i*sh+kh), (j*sw):(j*sw+kw)].mean()\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:29.016577Z",
     "start_time": "2020-10-12T03:11:29.003829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  7.],\n",
       "        [13., 15.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  3.],\n",
       "        [ 9., 11.],\n",
       "        [13., 15.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(16.).view(4, 4)\n",
    "pool2d_padding_stride(x, kernel_size=3)\n",
    "pool2d_padding_stride(x, kernel_size=(3, 3), padding=1, stride=2)\n",
    "pool2d_padding_stride(x, kernel_size=(2, 4), padding=(1, 2), stride=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn池化实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化层可以通过padding和stride来改变输出的shape\n",
    "- 在`nn.MaxPool2d`中，默认stride和池化窗口形状是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:29.026596Z",
     "start_time": "2020-10-12T03:11:29.018640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  7.,  32.],\n",
       "          [134., 172.],\n",
       "          [ 63.,  44.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(2, 4), \n",
    "                 padding=(1, 2), stride=(2, 3))\n",
    "conv.weight.data = torch.arange(8.).view(1, 1, 2, 4)\n",
    "conv.bias.data = torch.zeros(1)\n",
    "conv(x.view((1, 1)+x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T03:11:29.041007Z",
     "start_time": "2020-10-12T03:11:29.028555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  3.],\n",
       "          [ 9., 11.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=3)\n",
    "pool(x.view((1, 1)+x.shape))\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "pool(x.view((1, 1)+x.shape))\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=(2, 4), padding=(1, 2), stride=(2, 3))\n",
    "pool(x.view((1, 1)+x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多通道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化层通道的情况不像卷积层那样需要把各通道的结果相加\n",
    "- 输入通道为多少，输出通道就为多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:07:07.643481Z",
     "start_time": "2020-10-12T04:07:07.640518Z"
    }
   },
   "outputs": [],
   "source": [
    "def pool2d_multi_in_out(x, kernel_size, padding, stride=None, mode=\"max\"):\n",
    "    result = [pool2d_padding_stride(x[k], kernel_size[1:], padding, stride, \n",
    "                                   mode) for k in range(kernel_size[0])]\n",
    "    return torch.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:09:26.029166Z",
     "start_time": "2020-10-12T04:09:26.017262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  2.,  3.],\n",
       "         [ 8., 10., 11.],\n",
       "         [12., 14., 15.]],\n",
       "\n",
       "        [[ 1.,  3.,  4.],\n",
       "         [ 9., 11., 12.],\n",
       "         [13., 15., 16.]],\n",
       "\n",
       "        [[ 2.,  4.,  5.],\n",
       "         [10., 12., 13.],\n",
       "         [14., 16., 17.]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(16.).view(4, 4)\n",
    "X = torch.stack((x, x+1, x+2))\n",
    "\n",
    "pool2d_multi_in_out(X, (3, 2, 2), padding=1, stride=2)\n",
    "pool2d_multi_in_out(X, (3, 2, 2), padding=1, stride=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:09:29.512640Z",
     "start_time": "2020-10-12T04:09:29.494341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  2.,  3.],\n",
       "         [ 8., 10., 11.],\n",
       "         [12., 14., 15.]],\n",
       "\n",
       "        [[ 1.,  3.,  4.],\n",
       "         [ 9., 11., 12.],\n",
       "         [13., 15., 16.]],\n",
       "\n",
       "        [[ 2.,  4.,  5.],\n",
       "         [10., 12., 13.],\n",
       "         [14., 16., 17.]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, padding=1, stride=2)\n",
    "pool(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结：\n",
    "- 最大池化和平均池化分别对池化窗口中输入元素中的最大值和平均值作为输出\n",
    "- 池化层的一个主要作用是缓解卷积层对位置的过度敏感性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
