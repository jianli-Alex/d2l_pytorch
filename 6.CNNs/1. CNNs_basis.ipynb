{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:04:34.983593Z",
     "start_time": "2020-10-11T09:04:34.956330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二维卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络是含有卷积层的神经网络\n",
    "- 我们用得最多就是二维的卷积层(有高宽两个维度)\n",
    "- 多输入通道数和多输出通道数的卷积层都是在这上面进行扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和互相关运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常来说,我们在卷积神经网络上使用的卷积运算并不是真正的卷积运算,其通常是使用互相关运算\n",
    "- 卷积运算是先将卷积核上下左右翻转后,再对输入数据进行互相关运算\n",
    "- 但由于深度学习中,卷积核的参数都是学习出来的,无论使用互相关运算或者是真正的卷积运算都不影响模型预测时的输出\n",
    "- 卷积核其实就是一个特征提取器,运算后的输出可以看成是输入在空间维度(高和宽)上某一级的表征\n",
    "- 考虑单通道输入和输出,将输入的高宽分别记为$I_h$和$I_w$,卷积核的高宽分别记为$K_h$和$K_w$,输出的的高宽分别记为$O_h$和$O_w$,对高宽进行的padding分别记为$p_h$和$p_w$,对高宽进行的stride分别记为$s_h$和$s_w$,那么对于以下几种情况的卷积输出分别为:\n",
    "    - 无padding和stride: $O_h = I_h - k_h + 1$, $O_w = I_w - k_w + 1$\n",
    "    - 有padding和无stride: $O_h = I_h - k_h + p_h + 1$, $O_h = I_w- k_w + p_w + 1$\n",
    "    - 无padding和有stride: $(O_h = I_h - k_h)/s_h + 1$, $(O_w = I_w - k_w)/s_w + 1$\n",
    "    - 有padding和有stride: $(O_h = I_h - k_h + p_h)/s_h + 1$, $(O_w = I_w - k_w + p_w)/s_w + 1$\n",
    "        - 其实第四条就能包括前三条,这里只是列得仔细点,另外p指的是两边一共padding的数量,有的书是用2p(这实际上是指单边的padding数量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过我们会将padding设为$k-1$,这样就能获得和输入同shape的tensor(这也叫等宽卷积)\n",
    "- 卷积核通常也是奇数,这样两端的padding一样,否则为偶数时,一边的padding需要向上取整,另一边padding要向下取整\n",
    "- 目前多用小的卷积核(像1x1, 3x3等)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T13:14:55.160755Z",
     "start_time": "2020-10-09T13:14:55.136989Z"
    }
   },
   "source": [
    "我们可以通过更深的网络结构来让感受野变得更加广阔,从而捕捉输入上更大尺寸特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积个人实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在二维互相关运算中(如无特殊说明,深度学习中的卷积就是指互相关运算)\n",
    "- 就是卷积窗口从输入数组的最上方开始,从左到右,从上到下的顺序,依次做滑窗运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:06:57.904179Z",
     "start_time": "2020-10-11T09:06:57.901156Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:06:58.654218Z",
     "start_time": "2020-10-11T09:06:58.641555Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d(x, k):\n",
    "    \"\"\"\n",
    "    功能: 实现卷积操作(无padding/无stride)\n",
    "    参数 x: 输入数据\n",
    "    参数 k; 传入一个卷积核\n",
    "    \"\"\"\n",
    "    # 获取卷积核的大小\n",
    "    h, w = k.shape\n",
    "    # 定义输出的shpe\n",
    "    y = torch.rand((x.shape[0] - h + 1, x.shape[1] - w + 1))\n",
    "    # 卷积运算\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            y[i, j] = (x[i:i+h, j:j+w] * k).sum()\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:07:01.727633Z",
     "start_time": "2020-10-11T09:07:01.706528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9).view(3, 3)\n",
    "k = torch.arange(4).view(2, 2)\n",
    "\n",
    "# 卷积运算\n",
    "conv2d(x, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T11:07:09.893140Z",
     "start_time": "2020-10-09T11:07:09.881886Z"
    }
   },
   "source": [
    "### 自定义卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T00:36:00.192610Z",
     "start_time": "2020-10-10T00:36:00.189157Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:11:08.654268Z",
     "start_time": "2020-10-11T09:11:08.649386Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    \"\"\"自定义实现卷积层\"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return conv2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积窗口形状为pxq的卷积层称为pxq卷积层\n",
    "- 说明卷积核的高和宽分别为p和q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像的物体边缘检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用卷积层来检测图像中的物体边缘(找到像素变化的位置)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:11:15.333299Z",
     "start_time": "2020-10-11T09:11:15.319881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个6*8的图像,中间4列为黑,其余为白\n",
    "x = torch.ones(6, 8)\n",
    "x[:, 2:6] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为实际上是测相邻的元素是否有变化,因此可以定义一个1*2的卷积核[[-1, 1]],只要相邻两行出现变化,卷积计算出来的就不为0,如果没有变化就为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义卷积核的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:11:21.466039Z",
     "start_time": "2020-10-11T09:11:21.446870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.tensor([[-1, 1]])\n",
    "y = conv2d(x, k)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层可以通过重复使用卷积核有效地表征局部空间\n",
    "\n",
    "像\n",
    "```\n",
    "[[1, 0, -1\n",
    "1, 0, -1\n",
    "1, 0, -1]] 叫做垂直边缘过滤器，其不但能检测边缘，也能区别明暗\n",
    "\n",
    "[[-1, 0, 1\n",
    "-1, 0, 1\n",
    "-1, 0, 1]] 叫做水平边缘过滤器\n",
    "\n",
    "[[1, 0, -1\n",
    "2, 0, -2\n",
    "1, 0, -1]] 叫做sobel过滤器，有更强的鲁棒性\n",
    "\n",
    "[[3, 0, -3\n",
    "10, 0, -10\n",
    "3， 0， -3]] 叫做scharr过滤器\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:30:11.637597Z",
     "start_time": "2020-10-11T09:30:11.635078Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "from optim import sgd\n",
    "from sqdm import sqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:30:12.649293Z",
     "start_time": "2020-10-11T09:30:12.640159Z"
    }
   },
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    return ((y_pred - y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:30:31.965334Z",
     "start_time": "2020-10-11T09:30:31.291626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 24.5223, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [2/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 12.6694, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [3/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 9.2757, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [4/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 7.0470, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [5/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 5.3818, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [6/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 4.1197, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [7/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 3.1595, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [8/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 2.4268, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [9/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.8665, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [10/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.4371, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [11/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.1076, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [12/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8543, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [13/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6593, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [14/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.5091, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [15/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3934, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [16/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3040, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [17/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2351, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [18/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1818, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [19/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1406, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [20/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1088, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [21/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0842, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [22/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0651, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [23/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0504, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [24/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0390, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [25/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0302, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [26/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0234, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [27/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0181, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [28/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0140, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [29/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0109, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [30/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0084, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [31/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0065, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [32/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0050, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [33/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0039, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [34/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0030, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [35/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0023, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [36/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0018, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [37/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0014, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [38/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0011, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [39/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0008, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [40/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0007, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [41/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0005, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [42/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0004, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [43/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0003, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [44/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0002, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [45/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0002, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [46/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [47/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [48/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [49/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [50/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0001, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [51/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [52/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [53/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [54/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [55/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [56/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [57/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [58/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [59/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [60/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [61/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [62/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [63/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [64/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [65/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [66/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [67/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [68/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [69/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [70/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [71/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [72/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [73/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [74/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [75/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [76/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [77/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [78/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [79/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [80/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [81/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [82/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [83/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [84/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [85/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [86/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [87/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [88/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [89/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [90/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [91/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [92/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [93/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [94/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [95/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [96/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [97/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [98/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [99/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [100/100]\n",
      "1/1 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0000, train_score: -, test_loss: -, test_score: -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model  = Conv2D(k.shape)\n",
    "loss = squared_loss\n",
    "epoch_num = 100\n",
    "lr = 0.01\n",
    "weight_decay = 0\n",
    "\n",
    "process_bar = sqdm()\n",
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "    y_pred = model(x)\n",
    "    l = loss(y_pred, y)\n",
    "    l.backward()\n",
    "    \n",
    "    sgd([model.weight, model.bias], lr=lr, weight_decay=weight_decay)\n",
    "#     _ = model.weight.grad.data.zero_()\n",
    "#     _ = model.bias.grad.data.zero_()\n",
    "    _ = model.weight.grad.fill_(0)\n",
    "    _ = model.bias.grad.fill_(0)\n",
    "    \n",
    "    process_bar.show_process(data_num=1, batch_size=1, train_loss=l.item())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T09:30:37.712185Z",
     "start_time": "2020-10-11T09:30:37.702883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0000,  1.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-3.0925e-14], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "model.weight\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果和自定义的[[-1, 1]]一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带padding卷积的个人实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T11:23:35.141342Z",
     "start_time": "2020-10-11T11:23:35.128637Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_padding(x, k, padding=0):\n",
    "    \"\"\"\n",
    "    function: 实现带padding的卷积\n",
    "    params x: 输入张量\n",
    "    params k: 巻积核\n",
    "    params padding: 传入padding的元组(h, w),高padding多少,宽padding多少,\n",
    "                    如果为整数就高宽一样, padding是指两端都填充相同数量的零\n",
    "    \"\"\"\n",
    "    assert padding >= 0\n",
    "    if padding == 0:\n",
    "        return conv2d(x, k)\n",
    "    else:\n",
    "        if isinstance(padding, int):\n",
    "            h = w = padding\n",
    "        else:\n",
    "            h, w = padding\n",
    "            \n",
    "        x = x.numpy()\n",
    "        # 前一个元组是增加上下（列），后一个元组是增加左右（宽）\n",
    "        x = torch.from_numpy(np.pad(x, ((h, h), (w, w))))\n",
    "        return conv2d(x, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T11:23:42.025044Z",
     "start_time": "2020-10-11T11:23:42.004321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等宽巻积\n",
    "x = torch.rand(8, 8)\n",
    "k = torch.rand(3, 3)\n",
    "\n",
    "conv2d_padding(x, k, padding=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T11:23:42.830600Z",
     "start_time": "2020-10-11T11:23:42.811983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  8.,  4.],\n",
       "        [ 9., 19., 25., 10.],\n",
       "        [21., 37., 43., 16.],\n",
       "        [ 6.,  7.,  8.,  0.]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和pytorch自带Conv2d的结果对比\n",
    "x = torch.arange(9.).view(3, 3)\n",
    "k = torch.arange(4.).view(2, 2)\n",
    "conv2d_padding(x, k, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T11:23:44.371046Z",
     "start_time": "2020-10-11T11:23:44.362032Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用pytorch实现\n",
    "def compute_conv2d(x, k):\n",
    "    # pytorch实现的是四维的卷积核，先为x添加维度\n",
    "    x = x.view((1, 1) + x.shape)\n",
    "    result = k(x)\n",
    "    return result.view(result.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T11:23:45.134042Z",
     "start_time": "2020-10-11T11:23:45.113497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  8.,  4.],\n",
       "        [ 9., 19., 25., 10.],\n",
       "        [21., 37., 43., 16.],\n",
       "        [ 6.,  7.,  8.,  0.]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=2, padding=1)\n",
    "conv.weight.data = k.view((1,1) + k.shape)\n",
    "conv.bias.data = torch.zeros(1)\n",
    "\n",
    "compute_conv2d(x, conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带stride的巻积个人实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T11:59:05.244435Z",
     "start_time": "2020-10-11T11:59:05.233123Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_padding_stride(x, k, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    function: 实现带padding和stride的巻积\n",
    "    params x: 输入张量\n",
    "    params k: 巻积核\n",
    "    params padding: 传入padding的元组(h, w),指两端填充一样的值\n",
    "    params stride: 默认为1, 传入(h, w)或者整数\n",
    "    \"\"\"\n",
    "    if stride == 1:\n",
    "        return conv2d_padding(x, k, padding)\n",
    "    else:\n",
    "        kh, kw = k.shape\n",
    "        if isinstance(padding, int):\n",
    "            ph = pw = padding\n",
    "        else:\n",
    "            ph, pw = padding\n",
    "        \n",
    "        if isinstance(stride, int):\n",
    "            sh = sw = stride\n",
    "        else:\n",
    "            sh, sw = stride\n",
    "        \n",
    "        y = torch.zeros(int((x.shape[0]-kh+ph+sh)//sh), int((x.shape[1]-kw+pw+sw)//sw))\n",
    "        x = torch.from_numpy(np.pad(x.numpy(), ((ph, ph), (pw, pw))))\n",
    "        for i in range(y.shape[0]):\n",
    "            for j in range(y.shape[1]):\n",
    "                y[i, j] = (x[(i*sh):(i*sh+kh), (j*sw):(j*sw+kw)] * k).sum()\n",
    "                \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T11:59:06.347450Z",
     "start_time": "2020-10-11T11:59:06.317938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7724, 1.4222, 1.3222, 2.6037],\n",
       "        [1.4689, 3.4087, 2.8485, 3.9121],\n",
       "        [1.5229, 2.9481, 3.0770, 2.3263],\n",
       "        [1.2288, 2.5033, 2.9996, 3.0804]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7724, 1.4222, 1.3222, 2.6037],\n",
       "        [1.4689, 3.4087, 2.8485, 3.9121],\n",
       "        [1.5229, 2.9481, 3.0770, 2.3263],\n",
       "        [1.2288, 2.5033, 2.9996, 3.0804]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试自定义和使用nn实现的结果\n",
    "test_x = torch.rand(8, 8)\n",
    "test_k = torch.rand(3, 3)\n",
    "conv2d_padding_stride(test_x, test_k, 1, 2)\n",
    "conv2d_padding_stride(test_x, test_k, 1, 2).shape\n",
    "\n",
    "\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, stride=2)\n",
    "conv.weight.data = test_k.view((1, 1) + test_k.shape)\n",
    "conv.bias.data = torch.zeros(1)\n",
    "compute_conv2d(test_x, conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T11:59:08.306534Z",
     "start_time": "2020-10-11T11:59:08.272437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7317, 4.2351],\n",
       "        [1.9964, 5.1802]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.7317, 4.2351],\n",
       "        [1.9964, 5.1802]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试自定义和使用nn实现的结果(复杂一点)\n",
    "test_x = torch.rand(8, 8)\n",
    "test_k = torch.rand(3, 5)\n",
    "padding = (0, 1)\n",
    "stride = (3, 4)\n",
    "conv2d_padding_stride(test_x, test_k, padding, stride)\n",
    "conv2d_padding_stride(test_x, test_k, padding, stride).shape\n",
    "\n",
    "\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=padding, stride=stride)\n",
    "conv.weight.data = test_k.view((1, 1) + test_k.shape)\n",
    "conv.bias.data = torch.zeros(1)\n",
    "compute_conv2d(test_x, conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多通道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多通道输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑多通道输入和多通道输出，正如pytorch一样，卷积核默认是四维的\n",
    "- 当输入数据是多通道的时候，我们要构造一个输入通道数与输入数据通道数相同的卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T12:08:11.319945Z",
     "start_time": "2020-10-11T12:08:11.309521Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_multi_in(x, k, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    function: 实现多通道的卷积运算，单通道输入\n",
    "    params x: 多通道的数据\n",
    "    params k: 多通道的卷积核\n",
    "    \"\"\"\n",
    "    result = conv2d_padding_stride(x[0], k[0], padding, stride)\n",
    "    for i in range(1, x.shape[0]):\n",
    "        result += conv2d_padding_stride(x[i], k[i], padding, stride)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T12:11:20.216439Z",
     "start_time": "2020-10-11T12:11:20.188672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [3, 4]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = torch.arange(9).view(3, 3), torch.arange(1, 10).view(3, 3)\n",
    "k1, k2 = torch.arange(4).view(2, 2), torch.arange(1, 5).view(2, 2)\n",
    "X, K = torch.stack((x1, x2)), torch.stack((k1, k2))\n",
    "X\n",
    "K\n",
    "\n",
    "conv2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多通道输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当有多输入通道的时候，我们实际上做了累计，所以输出通道都是0，如果想拿到多输出通道就对每个输出通道分别创建满足3维度的卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T12:22:15.455747Z",
     "start_time": "2020-10-11T12:22:15.450256Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_multi_in_out(X, K):\n",
    "    \"\"\"\n",
    "    function: 实现多通道输出，k是四维的卷积核\n",
    "    \"\"\"\n",
    "    result = torch.stack([conv2d_multi_in(X, k) for k in K])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T12:22:16.211977Z",
     "start_time": "2020-10-11T12:22:16.188953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_k = torch.stack((K, K+1, K+2))\n",
    "test_k.shape\n",
    "\n",
    "conv2d_multi_in_out(X, test_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为使用了最小的窗口，所以1x1卷积失去了可以识别高和宽相邻元素构成模式的功能\n",
    "- 1x1卷积的主要计算发生在通道维上\n",
    "- 如果把高宽元素当成样本，通道维当作是特征，那么1x1卷积作用就等价于全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T14:10:42.124874Z",
     "start_time": "2020-10-11T14:10:42.117850Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_multi_in_out_1x1(x, k):\n",
    "    \"\"\"\n",
    "    function：实现1x1卷积\n",
    "    params x: 是一个多通道的输入(3维度)\n",
    "    params k：是一个多通道的1x1卷积核(4维度)\n",
    "    \"\"\"\n",
    "    x = x.view(3, -1)\n",
    "    x = torch.stack((x, x))\n",
    "    k = k.view(2, 1, 3)\n",
    "    result = torch.bmm(k, x)\n",
    "    return result.view(2, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T14:11:10.891940Z",
     "start_time": "2020-10-11T14:11:10.878152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 45,  48,  51],\n",
       "         [ 54,  57,  60],\n",
       "         [ 63,  66,  69]],\n",
       "\n",
       "        [[126, 138, 150],\n",
       "         [162, 174, 186],\n",
       "         [198, 210, 222]]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 45.,  48.,  51.],\n",
       "         [ 54.,  57.,  60.],\n",
       "         [ 63.,  66.,  69.]],\n",
       "\n",
       "        [[126., 138., 150.],\n",
       "         [162., 174., 186.],\n",
       "         [198., 210., 222.]]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(27).view(3, 3, 3)\n",
    "k = torch.arange(6).view(2, 3, 1, 1)\n",
    "conv2d_multi_in_out_1x1(x, k)\n",
    "\n",
    "conv2d_multi_in_out(x, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
