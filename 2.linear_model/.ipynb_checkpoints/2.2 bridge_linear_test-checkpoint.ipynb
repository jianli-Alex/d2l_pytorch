{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:46:10.524560Z",
     "start_time": "2020-10-04T13:46:10.502569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:47:20.638765Z",
     "start_time": "2020-10-04T13:47:20.634647Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "import numpy as np\n",
    "from linreg_numpy import LinearModel\n",
    "from data_prepare import data_iter\n",
    "from sqdm import sqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:47:28.021315Z",
     "start_time": "2020-10-04T13:47:27.993519Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "input_num = 10000\n",
    "true_w = np.array([2, -3.4])\n",
    "true_b = np.array([4.2])\n",
    "\n",
    "x = np.random.normal(0, 1, size=(input_num, len(true_w)))\n",
    "error = np.random.normal(0, 0.01, size=input_num)\n",
    "y = x@true_w + true_b + error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:47:29.785892Z",
     "start_time": "2020-10-04T13:47:29.774711Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearBridge(LinearModel):\n",
    "    def __init__(self, weight_decay, alpha=0.01):\n",
    "        super(LinearBridge, self).__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # initialize w depend on the X shape\n",
    "        fea_num = int(X.size / len(y))\n",
    "        if self.count == 0:\n",
    "            self.w = np.zeros(fea_num)\n",
    "\n",
    "        # change X and y shape\n",
    "        X = X.reshape(len(y), fea_num)\n",
    "        y = y.reshape(-1)\n",
    "\n",
    "        # calculate y_pred\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # update grad\n",
    "        self.w = self.w - self.alpha * (X.T @ (y_pred - y) - self.weight_decay*self.w) / len(y)\n",
    "        self.b = self.b - self.alpha * ((y_pred - y).sum() - self.weight_decay*self.b) / len(y)\n",
    "        self.count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:47:42.996312Z",
     "start_time": "2020-10-04T13:47:33.982395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 85.4585, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "Epoch [1/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 16.6373, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [2/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 3.2215, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "Epoch [3/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6159, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [4/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1146, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [5/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0205, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [6/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0039, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [7/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0014, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [8/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0013, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [9/10]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0014, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "w before update is [ 2.  -3.4], w after update is [ 2.00038512 -3.40064687]\n",
      "b before update is [4.2], b after update is 4.200495035303656\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "params = {\n",
    "    \"epoch_num\": 10,\n",
    "    \"batch_size\": 128,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"alpha\": 0.01,\n",
    "    \"model\": LinearBridge,\n",
    "}\n",
    "\n",
    "process_bar = sqdm()\n",
    "\n",
    "def train(epoch_num, model, batch_size, alpha, weight_decay):\n",
    "    model = LinearBridge(weight_decay=weight_decay, alpha=alpha)\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, x, y):\n",
    "            model.fit(xdata, ydata)\n",
    "            mse = model.score(xdata, ydata)\n",
    "            process_bar.show_process(len(y), batch_size, round(mse, 4))\n",
    "            time.sleep(0.01)\n",
    "        print(\"\\n\")\n",
    "    return model\n",
    "\n",
    "model = train(**params)\n",
    "print(f\"w before update is {true_w}, w after update is {model.w}\")\n",
    "print(f\"b before update is {true_b}, b after update is {model.b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch实现1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用pytorch框架来实现，并对比是使用哪一种方式来进行正则化（权重衰减）\n",
    "- 权重衰减：$\\theta_t = (1 - \\beta)\\theta_{t-1} - \\alpha g_t$\n",
    "- L2正则化：$\\theta_t = \\theta_{t-1} - \\alpha (g_t + \\lambda \\theta_{t-1}) = (1 - \\alpha \\lambda)\\theta_{t-1} - \\alpha g_t$\n",
    "    - 其中$g_t + \\lambda \\theta_{t-1}$实际上是除了batch_size\n",
    "- pytorch实现中，对于$g_t + \\lambda \\theta_{t-1}$，前一部分$g_t$除了batch_size，后一部分$\\lambda \\theta_{t-1}$没有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-02T13:44:41.619327Z",
     "start_time": "2020-10-02T13:44:41.615861Z"
    }
   },
   "source": [
    "学习率0.01，weight_decay为0.05，优化器为SGD\n",
    "- 使用pytorch的backward梯度回传+自动梯度更新的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:47:47.218833Z",
     "start_time": "2020-10-04T13:47:47.191785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator object at 0x7f7950bcb7f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as Data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "params = {\n",
    "    \"input_num\": 10000,\n",
    "    \"fea_num\": 2,\n",
    "    \"epoch_num\": 20,\n",
    "    \"batch_size\": 128,\n",
    "    \"alpha\": 0.01,\n",
    "    \"weight_decay\": 0.05,\n",
    "}\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor([4.2])\n",
    "\n",
    "torch.manual_seed(1000)\n",
    "x = torch.normal(0, 1, size=(params[\"input_num\"], params[\"fea_num\"]))\n",
    "error = torch.normal(0, 0.01, size=(params[\"input_num\"], ))\n",
    "y = torch.mv(x, true_w) + true_b + error\n",
    "\n",
    "# 生成迭代器\n",
    "dataset = Data.TensorDataset(x, y)\n",
    "data_iter = Data.DataLoader(dataset, params[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:47:52.622484Z",
     "start_time": "2020-10-04T13:47:47.788669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator object at 0x7f7950bcb7f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 2.7500, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [1/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1162, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [2/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0284, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [3/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0154, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [4/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0230, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [5/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0156, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [6/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0220, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [7/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0180, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [8/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0182, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [9/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0287, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [10/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0187, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [11/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0219, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [12/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0180, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [13/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0115, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [14/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0145, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [15/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0145, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [16/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0154, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [17/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0170, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [18/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0148, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [19/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0250, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "w before update is tensor([ 2.0000, -3.4000]), w after update is Parameter containing:\n",
      "tensor([[ 1.9537, -3.3165]], requires_grad=True)\n",
      "b before update is tensor([4.2000]), b after update is Parameter containing:\n",
      "tensor([4.0979], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class PLinearBridge(nn.Module):\n",
    "    def __init__(self, fea_num):\n",
    "        super(PLinearBridge, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(fea_num, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "net = PLinearBridge(params[\"fea_num\"])\n",
    "torch.manual_seed(100)\n",
    "_ = init.normal_(net.layer[0].weight, 0, 0.01)\n",
    "_ = init.constant_(net.layer[0].bias, 0)\n",
    "loss = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=params[\"alpha\"])\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=params[\"alpha\"], weight_decay=params[\"weight_decay\"])\n",
    "\n",
    "for epoch in range(params[\"epoch_num\"]):\n",
    "    print(f\"Epoch [{epoch}/{params['epoch_num']}]\")\n",
    "    for xdata, ydata in data_iter:\n",
    "        l = loss(net(xdata), ydata.reshape(net(xdata).shape))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        process_bar.show_process(params[\"input_num\"], params[\"batch_size\"], round(l.item(), 4))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(f\"w before update is {true_w}, w after update is {net.layer[0].weight}\")\n",
    "print(f\"b before update is {true_b}, b after update is {net.layer[0].bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch实现2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率0.01，weight_decay为0.05，优化器为SGD\n",
    "- 使用pytorch的backward梯度回传+手写梯度更新的方式\n",
    "- 结果和pytorch实现1类似，说明pytorch在实现L2时，并没有对$\\theta$除以batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:48:17.484358Z",
     "start_time": "2020-10-04T13:48:14.648880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generate data by pytorch'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator object at 0x7f7950bcb7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'training'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator object at 0x7f7950bcb7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 21.0124, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "Epoch [1/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.7451, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "Epoch [2/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.4631, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [3/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3079, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [4/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2811, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [5/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2760, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [6/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2750, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [7/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [8/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [9/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [10/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [11/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [12/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [13/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [14/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [15/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [16/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [17/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [18/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [19/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2748, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "w before update is tensor([ 2.0000, -3.4000]), w after update is tensor([ 1.9515, -3.3150], requires_grad=True)\n",
      "b before update is tensor([4.2000]), b after update is tensor([4.0974], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*-coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "import torch\n",
    "from sqdm import sqdm\n",
    "from data_prepare import data_iter\n",
    "\n",
    "\n",
    "def linreg(X, w, b):\n",
    "    \"\"\"realize linear model\"\"\"\n",
    "    return torch.mv(X, w) + b\n",
    "\n",
    "\n",
    "def square_loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    calculate mean square loss which divide batch_size,\n",
    "    and don't divide batch_size when update gradient by mini-batch GD.\n",
    "    \"\"\"\n",
    "    return ((y_pred - y)**2).sum()\n",
    "\n",
    "\n",
    "def sgd(params, lr, weight_decay, batch_size):\n",
    "    \"\"\"realize optimization algorithm \"\"\"\n",
    "    for param in params:\n",
    "#         param.data -= lr * param.grad/batch_size\n",
    "        param.data = param.data - lr * param.grad/batch_size - lr * weight_decay*param.data\n",
    "\n",
    "\n",
    "def train(epoch_num, net, loss, batch_size, lr, weight_decay):\n",
    "    \"\"\"train function\"\"\"\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, x, y):\n",
    "            l = loss(net(xdata, w, b), ydata)\n",
    "            l.backward()\n",
    "            sgd([w, b], lr, weight_decay, len(ydata))\n",
    "\n",
    "            # clear grad, aviod grad accumulate\n",
    "            w.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "\n",
    "            # training bar\n",
    "            mse = np.round(loss(net(xdata, w, b), ydata).item(), 5)\n",
    "            process_bar.show_process(len(y), batch_size, mse)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "\"\"\"generate data by pytorch\"\"\"\n",
    "torch.manual_seed(1000)\n",
    "input_num = 10000\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor([4.2])\n",
    "x = torch.normal(mean=0, std=1, size=(input_num, len(true_w)))\n",
    "error = torch.normal(mean=0, std=0.01, size=(input_num, ))\n",
    "y = torch.mv(x, true_w) + true_b + error\n",
    "\n",
    "\"\"\"training\"\"\"\n",
    "# set parameter\n",
    "params = {\n",
    "    \"net\": linreg,\n",
    "    \"loss\": square_loss,\n",
    "    \"epoch_num\": 20,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 0.01,\n",
    "    \"weight_decay\":0.05,\n",
    "}\n",
    "\n",
    "# weight and bias initialize\n",
    "torch.manual_seed(100)\n",
    "w = torch.normal(mean=0, std=0.01, size=(2, ), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "process_bar = sqdm()\n",
    "train(**params)\n",
    "print(f\"w before update is {true_w}, w after update is {w}\")\n",
    "print(f\"b before update is {true_b}, b after update is {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch实现3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率0.01，weight_decay为0.05，优化器为SGD\n",
    "- 手写计算梯度+手写梯度更新的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:48:25.756590Z",
     "start_time": "2020-10-04T13:48:25.740391Z"
    }
   },
   "outputs": [],
   "source": [
    "class PLinearBridge(LinearModel):\n",
    "    def __init__(self, alpha=0.01, weight_decay=0.05):\n",
    "        super(PLinearBridge, self).__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # change X and y shape\n",
    "        fea_num = int(X.numel() / len(y))\n",
    "        X = X.reshape(len(y), fea_num)\n",
    "        y = y.reshape(-1)\n",
    "\n",
    "        # calculate y_pred\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # update grad\n",
    "        self.w = self.w - self.alpha * (X.T @ (y_pred - y)/len(y) + self.weight_decay*self.w)\n",
    "        self.b = self.b - self.alpha * ((y_pred - y).sum()/len(y) + self.weight_decay*self.b)\n",
    "#         self.w = self.w - self.alpha * (X.T @ (y_pred - y)) / len(y) - self.alpha * self.weight_decay * self.w\n",
    "#         self.b = self.b - self.alpha * (y_pred - y).sum() / len(y) - self.alpha * self.weight_decay * self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T13:48:34.154308Z",
     "start_time": "2020-10-04T13:48:31.609864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generate data by pytorch'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator object at 0x7f7950bcb7f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0036, -0.0029], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n",
      "Epoch [0/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 96.5073, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "Epoch [1/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 23.6168, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [2/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 7.2601, train_score: -, test_loss: -, test_score: --\n",
      "\n",
      "Epoch [3/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 3.0619, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [4/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.7896, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [5/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.3410, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [6/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.1655, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [7/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0928, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [8/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0618, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [9/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0485, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [10/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0427, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [11/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0401, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [12/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0390, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [13/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0385, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [14/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0383, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [15/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0382, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [16/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0382, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [17/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0382, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [18/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0382, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "Epoch [19/20]\n",
      "10000/10000 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0381, train_score: -, test_loss: -, test_score: -\n",
      "\n",
      "w before update is tensor([ 2.0000, -3.4000]), w after update is tensor([ 1.9061, -3.2345], grad_fn=<SubBackward0>)\n",
      "b before update is tensor([4.2000]), b after update is tensor([3.9998], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"generate data by pytorch\"\"\"\n",
    "torch.manual_seed(1000)\n",
    "input_num = 10000\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor([4.2])\n",
    "x = torch.normal(mean=0, std=1, size=(input_num, len(true_w)))\n",
    "error = torch.normal(mean=0, std=0.01, size=(input_num, ))\n",
    "y = torch.mv(x, true_w) + true_b + error\n",
    "\n",
    "params = {\n",
    "    \"epoch_num\": 20,\n",
    "    \"batch_size\": 128,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"alpha\": 0.01,\n",
    "    \"model\": PLinearBridge,\n",
    "}\n",
    "\n",
    "process_bar = sqdm()\n",
    "\n",
    "def train(epoch_num, model, batch_size, alpha, weight_decay):\n",
    "    model = model(weight_decay=weight_decay, alpha=alpha)\n",
    "    torch.manual_seed(100)\n",
    "    model.w = torch.normal(mean=0, std=0.01, size=(2, ), requires_grad=True)\n",
    "    print(model.w)\n",
    "    model.b = torch.zeros(1, requires_grad=True)\n",
    "    print(model.b)\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, x, y):\n",
    "            model.fit(xdata, ydata)\n",
    "            mse = model.score(xdata, ydata)\n",
    "            process_bar.show_process(len(y), batch_size, round(mse.item(), 5))\n",
    "        print(\"\\n\")\n",
    "    return model\n",
    "\n",
    "model = train(**params)\n",
    "print(f\"w before update is {true_w}, w after update is {model.w}\")\n",
    "print(f\"b before update is {true_b}, b after update is {model.b}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
