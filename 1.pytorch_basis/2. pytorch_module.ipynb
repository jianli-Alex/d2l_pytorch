{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:45.066364Z",
     "start_time": "2020-10-09T04:36:45.045583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 继承MODULE类来构造模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Module`是nn模块中提供的一个模型构建类，也是所有神经网络的基类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模板：\n",
    "```\n",
    "class ModelName(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelName, self).__init__():\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "```\n",
    "定义模型\n",
    "```\n",
    "net = ModelName()\n",
    "# 传入参数\n",
    "net(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实例化MLP---> net后，参数会得到初始化（pytorch会根据不同的层来初始化），传入x进行向前传播，此时`net(x)`会调用`MLP`继承自`Module`类的`__call__`函数，这个函数将调用`MLP`类定义的`forward`函数来完成向前传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODULE的子类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Module`类是一个通用的部件，此外，pytorch还实现了继承自`Module`的可以方便构建模型的类：像`Sequential`/`ModuleList`/`ModuleDict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用来简单串联各层的计算\n",
    "- 可以接受一个子模块的有序字典(OrderedDict)\n",
    "- 也可以接受一系列子模块（像nn.Linear等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它也是一个字典，但是是有序的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建方式：\n",
    "- 使用OrderedDict([(key1, value1), (key2, value2), ...])\n",
    "- 使用OrderedDict({key1: value1, key2: value2})\n",
    "- 先定义一个OrderedDict，再进行赋值\n",
    "    - a = OrderedDict()\n",
    "    - a[\"b\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.040376Z",
     "start_time": "2020-10-09T04:36:49.038186Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.095600Z",
     "start_time": "2020-10-09T04:36:49.092090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', [1]), ('b', 2)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用[(key, value), ([key, value]), ...]\n",
    "a = OrderedDict([(\"a\", [1]), (\"b\", 2)])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.148134Z",
     "start_time": "2020-10-09T04:36:49.144749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', [1]), ('b', 2)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = OrderedDict({\n",
    "    \"a\": [1], \n",
    "    \"b\": 2},\n",
    ")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.200543Z",
     "start_time": "2020-10-09T04:36:49.196820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', [1]), ('b', 2)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = OrderedDict()\n",
    "a[\"a\"] = [1]\n",
    "a[\"b\"] = 2\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### self._modules属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.281343Z",
     "start_time": "2020-10-09T04:36:49.277824Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def print_module(self):\n",
    "        print(self._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.334862Z",
     "start_time": "2020-10-09T04:36:49.329792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "OrderedDict([('layer', Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "))])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)\n",
    "net.print_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看见`self._modules`返回的是一个有序字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T00:36:47.965214Z",
     "start_time": "2020-10-09T00:36:47.955301Z"
    }
   },
   "source": [
    "可以使用`add_module`添加Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.446018Z",
     "start_time": "2020-10-09T04:36:49.442453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net.add_module(\"layer2\", nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(10, 10),\n",
    "))\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.495394Z",
     "start_time": "2020-10-09T04:36:49.492561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net.add_module(\"relu\", nn.ReLU())\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以接受一个有序的字典\n",
    "- 也可以传入一些Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.724309Z",
     "start_time": "2020-10-09T04:36:49.719895Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自定义一个模拟Sequential行为的类，其也是基于Module的\n",
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    # OrderedDict创建一个有序的字典\n",
    "    # args传进后会以元组表示，*args表示将元组解压\n",
    "    def __init__(self, *args):\n",
    "        super(MySequential, self).__init__()\n",
    "        # 如果是一个OrderedDict\n",
    "        if len(args) == 1 and isinstance(*args, OrderedDict):\n",
    "            for key, value in args[0].items():\n",
    "                self.add_module(key, value)\n",
    "        else:\n",
    "            # 传入的一些Module\n",
    "            for idx, module in enumerate(args):\n",
    "                # idx要传入字符串\n",
    "                self.add_module(str(idx), module)\n",
    "                \n",
    "    def forward(self, input):\n",
    "        # 使用self._modules返回一个有序的字典，使得成员能够按照顺序遍历\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.791546Z",
     "start_time": "2020-10-09T04:36:49.786785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 传入的是moduel\n",
    "net = MySequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.848971Z",
     "start_time": "2020-10-09T04:36:49.838495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0567,  0.0820, -0.0574,  ...,  0.2821,  0.0181, -0.3027],\n",
       "        [ 0.1462,  0.1129, -0.0375,  ...,  0.3956,  0.0942, -0.0992],\n",
       "        [ 0.1322, -0.0088, -0.0306,  ...,  0.4194,  0.1091, -0.2038],\n",
       "        ...,\n",
       "        [ 0.1811,  0.0881, -0.0141,  ...,  0.3552,  0.1496, -0.2456],\n",
       "        [ 0.1376,  0.1857,  0.0965,  ...,  0.3348,  0.1213, -0.1311],\n",
       "        [ 0.2055,  0.0972, -0.0441,  ...,  0.3078,  0.1898, -0.2231]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前向传播\n",
    "x = torch.rand(256, 784)\n",
    "net(x)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:49.978750Z",
     "start_time": "2020-10-09T04:36:49.953846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1651,  0.0609,  0.1220,  ..., -0.0687,  0.0242,  0.0490],\n",
       "        [-0.1672, -0.0228,  0.0726,  ..., -0.1520, -0.0485,  0.1081],\n",
       "        [-0.1326,  0.0356,  0.2017,  ..., -0.2820,  0.0073,  0.1017],\n",
       "        ...,\n",
       "        [-0.1498,  0.1202,  0.0870,  ..., -0.2101,  0.0692,  0.1829],\n",
       "        [-0.2243, -0.0015,  0.1774,  ..., -0.1841, -0.0783,  0.0442],\n",
       "        [-0.2135,  0.1082, -0.0158,  ..., -0.1506,  0.0029,  0.1712]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 传入的是OrderedDict\n",
    "net = MySequential(\n",
    "    OrderedDict({\n",
    "        \"linear1\": nn.Linear(784, 256),\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"linear2\": nn.Linear(256, 10)\n",
    "    })\n",
    ")\n",
    "\n",
    "# 向前传播\n",
    "x = torch.rand(256, 784)\n",
    "net(x)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModuleList`接受一个子模块的列表作为输入\n",
    "- 可以像list一样进行append和extend操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:50.393477Z",
     "start_time": "2020-10-09T04:36:50.385220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=256, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n",
    "_ = net.append(nn.Linear(256, 10))\n",
    "print(net)\n",
    "print(net[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModuleDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModuleDict可以接受一个子模块字典作为输入，也可以像字典一样进行访问操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:50.834098Z",
     "start_time": "2020-10-09T04:36:50.822801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (act): ReLU()\n",
      "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.ModuleDict({\n",
    "    \"linear\": nn.Linear(784, 256),\n",
    "    \"act\": nn.ReLU(),\n",
    "})\n",
    "\n",
    "net[\"output\"] = nn.Linear(256, 10)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建复杂的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接继承`Module`可以极大扩展模型构建的灵活性，也可以构建其他复杂的网络\n",
    "- 可以创建训练中不被迭代的参数，即常数参数\n",
    "- 使用`Tensor`的函数和python的控制流，并多次调用相同的层\n",
    "- 复用层（参数共享）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:51.265691Z",
     "start_time": "2020-10-09T04:36:51.256139Z"
    }
   },
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "        # 不可训练的参数（即常数参数）\n",
    "        self.rand_weight = torch.rand(20, 20, requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # 使用创建的常数参数和nn.functional的relu函数和mm函数\n",
    "        x = nn.functional.relu(torch.mm(x, self.rand_weight.data) + 1)\n",
    "        \n",
    "        # 复用全连接层，等价于两个全连接层是参数共享的\n",
    "        x = self.linear(x)\n",
    "        # 控制流\n",
    "        while x.norm().item() > 1:\n",
    "            x /= 2\n",
    "        if x.norm().item() < 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:51.418935Z",
     "start_time": "2020-10-09T04:36:51.411328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FancyMLP(\n",
      "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-2.0512, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 20)\n",
    "net = FancyMLP()\n",
    "print(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:46:50.044505Z",
     "start_time": "2020-10-09T01:46:50.041141Z"
    }
   },
   "source": [
    "`FancyMLP`和`Sequential`类都是`Module`类的子类，所以我们可以嵌套使用它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:51.710609Z",
     "start_time": "2020-10-09T04:36:51.703784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): NestMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (2): FancyMLP(\n",
      "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4770, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NestMLP, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential(nn.Linear(40, 30), nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "net = nn.Sequential(NestMLP(), nn.Linear(30, 20), FancyMLP())\n",
    "# net = nn.ModuleList([NestMLP(), nn.Linear(30, 20), FancyMLP()]) \n",
    "# 使用ModuleList和ModuleDict似乎都不行\n",
    "\n",
    "# 初始化\n",
    "x = torch.rand(2, 40)\n",
    "print(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结:\n",
    "- 可以继承`Module`来构建模型\n",
    "- `Sequential`/`ModuleList`/`ModuleDict`类都继承`Module`类\n",
    "- 虽然`Sequential`等类可以使模型构建更加简单,但是继承Module类可以极大地扩展模型构建的灵活性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数访问/初始化和共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:52.170505Z",
     "start_time": "2020-10-09T04:36:52.164624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (linear1): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "tensor(0.5399, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 默认来说pytorch已经根据不同的层来合理的初始化\n",
    "net = nn.Sequential()\n",
    "net.add_module(\"linear1\", nn.Linear(4, 3))\n",
    "net.add_module(\"relu\", nn.ReLU())\n",
    "net.add_module(\"linear2\", nn.Linear(3, 1))\n",
    "\n",
    "print(net)\n",
    "x = torch.rand(2, 4)\n",
    "y = net(x).sum()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 访问模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用`Module`类中的`parameters()`和`named_parameters()`来访问模型的参数\n",
    "- 后者的返回参数`Tensor`的同时,也会返回它的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:52.641685Z",
     "start_time": "2020-10-09T04:36:52.633430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "<class 'generator'>\n",
      "linear1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4247, -0.3197,  0.3450, -0.1388],\n",
      "        [ 0.4142,  0.2788,  0.0364,  0.0051],\n",
      "        [ 0.3108,  0.0935,  0.2756,  0.2643]], requires_grad=True)\n",
      "linear1.bias\n",
      "Parameter containing:\n",
      "tensor([0.0068, 0.2701, 0.0269], requires_grad=True)\n",
      "linear2.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3602, -0.4727,  0.5209]], requires_grad=True)\n",
      "linear2.bias\n",
      "Parameter containing:\n",
      "tensor([0.4158], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 本质上来说parameters和named_parameters都是生成器\n",
    "print(type(net.named_parameters()))\n",
    "print(type(net.parameters()))\n",
    "\n",
    "for name, params in net.named_parameters():\n",
    "    print(name)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`named_parameters`的时候,有名字的用名字,没有名字的就是用层数+weight/bias(例如1.weight)来命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:52.942123Z",
     "start_time": "2020-10-09T04:36:52.937485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4247, -0.3197,  0.3450, -0.1388],\n",
      "        [ 0.4142,  0.2788,  0.0364,  0.0051],\n",
      "        [ 0.3108,  0.0935,  0.2756,  0.2643]], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([0.0068, 0.2701, 0.0269], requires_grad=True)\n",
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "# 使用下标来访问单层的参数\n",
    "for name, param in net[0].named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(type(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而单层的时候,名字就不加层数前缀了\n",
    "- 这里的`param`属于'torch.nn.parameter.Parameter',是tensor的一个子类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:53.245965Z",
     "start_time": "2020-10-09T04:36:53.238855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel()\n",
      "weight1\n",
      "Parameter containing:\n",
      "tensor([[0.5875, 0.9530, 0.5149, 0.1770, 0.9609, 0.0769, 0.9802, 0.8526, 0.9440,\n",
      "         0.5968],\n",
      "        [0.9072, 0.1953, 0.9882, 0.9008, 0.8533, 0.1344, 0.3937, 0.0173, 0.5558,\n",
      "         0.2318],\n",
      "        [0.3763, 0.3459, 0.4883, 0.5732, 0.6462, 0.6204, 0.7309, 0.1122, 0.8888,\n",
      "         0.6764],\n",
      "        [0.9210, 0.7918, 0.6728, 0.5531, 0.3551, 0.6973, 0.0889, 0.3689, 0.1221,\n",
      "         0.2333],\n",
      "        [0.5374, 0.0720, 0.1262, 0.4361, 0.2105, 0.7629, 0.8946, 0.5489, 0.1397,\n",
      "         0.6722],\n",
      "        [0.7122, 0.5171, 0.6399, 0.7598, 0.0164, 0.1319, 0.0181, 0.1996, 0.2620,\n",
      "         0.2098],\n",
      "        [0.0432, 0.9111, 0.0874, 0.8975, 0.5694, 0.1937, 0.3560, 0.0923, 0.4043,\n",
      "         0.2997],\n",
      "        [0.6806, 0.8412, 0.6316, 0.6189, 0.3345, 0.0320, 0.1041, 0.9006, 0.8425,\n",
      "         0.9322],\n",
      "        [0.4916, 0.9815, 0.1425, 0.5754, 0.5289, 0.4450, 0.2661, 0.6241, 0.2262,\n",
      "         0.0359],\n",
      "        [0.7182, 0.1769, 0.8166, 0.3817, 0.9852, 0.9972, 0.2121, 0.5857, 0.6210,\n",
      "         0.1821]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 可以通过nn.Parameters来自动添加到模型的列表\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.weight1 = nn.Parameter(torch.rand(10, 10))\n",
    "        self.weihgt2 = torch.rand(10, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "n = MyModel()\n",
    "print(n)\n",
    "for name, param in n.named_parameters():\n",
    "    print(name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上面的模型没有定义层数,但是参数还是自动添加到模型的参数列表中\n",
    "- 而weight2不是`Parameter`,所以没有被添加到模型参数列表中\n",
    "- 因为`Parameter`是`Tensor`,所以`Tensor`拥有的属性它都有,比如可以根据`data`访问参数数值,也可以用`grad`来访问梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:53.556269Z",
     "start_time": "2020-10-09T04:36:53.550620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (linear1): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.4247, -0.3197,  0.3450, -0.1388],\n",
      "        [ 0.4142,  0.2788,  0.0364,  0.0051],\n",
      "        [ 0.3108,  0.0935,  0.2756,  0.2643]], requires_grad=True)\n",
      "tensor([[ 0.4247, -0.3197,  0.3450, -0.1388],\n",
      "        [ 0.4142,  0.2788,  0.0364,  0.0051],\n",
      "        [ 0.3108,  0.0935,  0.2756,  0.2643]])\n",
      "None\n",
      "None\n",
      "tensor([[-0.2958, -0.4023, -0.4977, -0.1075],\n",
      "        [-0.3882, -0.5280, -0.6533, -0.1411],\n",
      "        [ 0.4277,  0.5818,  0.7198,  0.1554]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net)\n",
    "weight0 = list(net[0].parameters())[0]\n",
    "print(weight0)\n",
    "print(weight0.data)\n",
    "print(weight0.grad)\n",
    "print(weight0.grad_fn)\n",
    "\n",
    "y.backward()\n",
    "print(weight0.grad)\n",
    "# 因为是叶子节点,所以没有grad_fn\n",
    "print(weight0.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch中的`nn.Module`模块参数都采取了较为合理的初始化策略\n",
    "- 也可以使用torch.nn.init提供的初始化参数的方式来初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:54.026680Z",
     "start_time": "2020-10-09T04:36:54.022992Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:54.197970Z",
     "start_time": "2020-10-09T04:36:54.189198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0008, -0.0094,  0.0033,  0.0004],\n",
       "        [ 0.0050,  0.0203, -0.0207, -0.0129],\n",
       "        [-0.0077,  0.0118,  0.0036,  0.0021]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0008, -0.0094,  0.0033,  0.0004],\n",
      "        [ 0.0050,  0.0203, -0.0207, -0.0129],\n",
      "        [-0.0077,  0.0118,  0.0036,  0.0021]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in net[0].named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        # 权重初始化为正态分布\n",
    "        init.normal_(param, 0, 0.01)\n",
    "    else:\n",
    "        # 偏置项初始化为0\n",
    "        init.constant_(param, 0)\n",
    "    print(name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只想对某个特定的参数进行初始化,可以使用`Parameter`类中`initialize`函数,它和`Block`类提供的`initialize`函数的使用方法一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义初始化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时我们需要初始化的方法在`init`中没有提供,可以实现一个初始化的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch中实现初始化的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:54.954852Z",
     "start_time": "2020-10-09T04:36:54.951859Z"
    }
   },
   "outputs": [],
   "source": [
    "def normal_(tensor, mean=0, std=1):\n",
    "    # 拿掉梯度\n",
    "    with torch.no_grad:\n",
    "        # 使用inplace的方式来进行初始化,这个过程也不记录梯度\n",
    "        return tensor.normal_(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义一个初始化方法:\n",
    "- 有一半权重的概率初始化为0\n",
    "- 有另一半的概率初始化为[-10, -5]和[5, 10]两个区间里均匀分布的随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:55.297175Z",
     "start_time": "2020-10-09T04:36:55.289075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight tensor([6.7310, 0.0000, 6.2424])\n"
     ]
    }
   ],
   "source": [
    "def init_weight_(tensor):\n",
    "    # 不记录梯度\n",
    "    with torch.no_grad():\n",
    "        # 以in_place的方式初始化张量为uniform的形式\n",
    "        tensor.uniform_(-10, 10)\n",
    "        # 将(-5, 5)的向量变成0,等号右边的部分是0/1组成的矩阵\n",
    "        tensor *= (tensor.abs() >= 5).float()\n",
    "        \n",
    "        \n",
    "for name, para in net[0].named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        init_weight_(param)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:55.451251Z",
     "start_time": "2020-10-09T04:36:55.446228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.bias tensor([7.7310, 1.0000, 7.2424])\n",
      "linear2.bias tensor([1.4158])\n"
     ]
    }
   ],
   "source": [
    "# 通过`data`改变模型参数值同时不会影响梯度\n",
    "for name, param in net.named_parameters():\n",
    "    if \"bias\" in name:\n",
    "        param.data += 1\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共享模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T03:23:46.544810Z",
     "start_time": "2020-10-09T03:23:46.534976Z"
    }
   },
   "source": [
    "如果希望在多个层之间共享模型参苏\n",
    "- 可以在`Module`类中forward函数中多次调用同一个层\n",
    "- 如果我们传入的`Sequential`模块是同一个`Module`实例,那参数也是共享的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:55.914733Z",
     "start_time": "2020-10-09T04:36:55.909433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "# 使用bias为False来去掉偏置项,不过在我们定义模型的时候,偏置项还是要保留的,不然模型能力会大大降低\n",
    "linear = nn.Linear(1, 1, bias=False)\n",
    "# 传入的是同一个实例时,参数也是共享的\n",
    "net = nn.Sequential(linear, linear)\n",
    "# 模型中存储了两层,参数只打印了一层\n",
    "print(net)\n",
    "\n",
    "# 可以看到只是打印了一个tensor,这是因为第1层的权重也是共享的\n",
    "for name, param in net.named_parameters():\n",
    "    _ = init.constant_(param, val=3)\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T03:29:06.392212Z",
     "start_time": "2020-10-09T03:29:06.375062Z"
    }
   },
   "source": [
    "在内存中,上述模型中的两个线性层其实时一个对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:56.239633Z",
     "start_time": "2020-10-09T04:36:56.230450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为模型参数包含了梯度,所以在反向传播计算的时候,这些共享参数的梯度是累加的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:56.536124Z",
     "start_time": "2020-10-09T04:36:56.530456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 1)\n",
    "y = net(x).sum()\n",
    "\n",
    "print(y)\n",
    "# 反向传播\n",
    "y.backward()\n",
    "# 单次的梯度应该是3,但是因为梯度累加的作用,两个梯度变成6\n",
    "print(net[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结:\n",
    "- 有多种方法来访问/初始化和共享模型的参数,也可以自定义初始化的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不含参数模型的自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:57.170785Z",
     "start_time": "2020-10-09T04:36:57.165932Z"
    }
   },
   "outputs": [],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CenteredLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x - x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:57.345563Z",
     "start_time": "2020-10-09T04:36:57.338701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:57.517963Z",
     "start_time": "2020-10-09T04:36:57.512660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用上述的不含参数模型的自定义层来定义更加复杂的模型\n",
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\n",
    "\n",
    "# Linear层后变成4*128,然后自定义层中心化4*128\n",
    "y = net(torch.rand(4, 8))\n",
    "y.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 含有参数模型的自定义层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Parameter`类是`Tensor`的子类,如果一个`Tensor`是`Parameter`,它就会被自动添加模型的参数列表中\n",
    "- 所以在自定义含有参数模型的层时,我们应该将参数定义成`Parameter`\n",
    "- 其实也可以使用`ParameterList`和`ParameterDict`分别定义参数的列表和字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ParameterList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接受一个`Parameter`实例的列表作为输入得到参数列表,使用的时候可以使用索引来访问这个参数\n",
    "- 像列表一样,可以使用append/extend等方法来新增参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:58.252794Z",
     "start_time": "2020-10-09T04:36:58.241626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        # 自定义参数\n",
    "        # 如果直接用nn.Parameter来定义,打印的时候其实是不会有显示的\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) \n",
    "                                       for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 在前向传播中,使用输入和自定义参数来进行计算\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "    \n",
    "    \n",
    "net = MyListDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ParameterDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ParameterDict`接收一个`Parameter`实例的字典作为输入然后得到一个参数字典\n",
    "- 可以向字典一样使用,像使用`update()`新增参数, 使用`keys()`返回所有键值,使用`items()`来返回所有的键值对\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:58.697240Z",
     "start_time": "2020-10-09T04:36:58.689971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "            \"linear1\": nn.Parameter(torch.randn(4, 4)),\n",
    "            \"linear2\": nn.Parameter(torch.randn(4, 1)),\n",
    "        })\n",
    "        \n",
    "        self.params.update({\n",
    "            \"linear3\": nn.Parameter(torch.randn(4, 2))\n",
    "        })\n",
    "        \n",
    "    def forward(self, x, choice=\"linear1\"):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "    \n",
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:58.864913Z",
     "start_time": "2020-10-09T04:36:58.843151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0152,  0.5102, -0.2263,  0.3058]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7763]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1749, -0.4352]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 4)\n",
    "net(x, \"linear1\")\n",
    "net(x, \"linear2\")\n",
    "net(x, \"linear3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用自定义层来构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:59.155884Z",
     "start_time": "2020-10-09T04:36:59.150460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): MyDictDense(\n",
      "    (params): ParameterDict(\n",
      "        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "    )\n",
      "  )\n",
      "  (1): MyListDense(\n",
      "    (params): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[-1.4527]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    MyDictDense(),\n",
    "    MyListDense(),\n",
    ")\n",
    "\n",
    "print(net)\n",
    "print(net(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型读取和存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以直接使用`save`和`load`函数分别存储和读取`Tensor`\n",
    "- `save`是使用了python的pickle对实例对象进行序列化,然后将序列化对象保存到disk中\n",
    "- 使用`save`可以保存各种对象(模型/张量/字典)\n",
    "- `load`使用pickle的unpickle工具将pickle的对象文件反序列化为内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "命名时建议命名成`.pt`格式的文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor/tensor列表/tensor字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:36:59.932262Z",
     "start_time": "2020-10-09T04:36:59.924778Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./file\"):\n",
    "    os.mkdir(\"./file\")\n",
    "    \n",
    "x = torch.ones(3)\n",
    "# 保存文件\n",
    "torch.save(x, \"./file/x.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:00.091834Z",
     "start_time": "2020-10-09T04:37:00.086516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将数据从文件中读回到内存中\n",
    "x2 = torch.load(\"./file/x.pt\")\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:00.249913Z",
     "start_time": "2020-10-09T04:37:00.242558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1.]), tensor([0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将tensor列表存储和读取\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y], \"./file/xy.pt\")\n",
    "xy_list = torch.load(\"./file/xy.pt\")\n",
    "xy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:00.441371Z",
     "start_time": "2020-10-09T04:37:00.436687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([1., 1., 1.]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储tensor字典\n",
    "torch.save({\"x\": x, \"y\": y}, \"./file/xy_dict.pt\")\n",
    "xy = torch.load(\"./file/xy_dict.pt\")\n",
    "xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读写模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`state_dict`是一个参数名称映射到模型参数的字典对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:01.142700Z",
     "start_time": "2020-10-09T04:37:01.138744Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:01.322796Z",
     "start_time": "2020-10-09T04:37:01.315620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer.0.weight', tensor([[-0.4958,  0.1084, -0.4380, -0.4480],\n",
       "        [-0.2386,  0.0387,  0.3148, -0.3404],\n",
       "        [-0.2728,  0.2810,  0.0195, -0.1575]])), ('layer.0.bias', tensor([-0.2769, -0.3658, -0.4286])), ('layer.2.weight', tensor([[-0.3411, -0.4762,  0.0986]])), ('layer.2.bias', tensor([0.2685]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "# 使用state_dict会返回一个参数的有序字典\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:22:24.561480Z",
     "start_time": "2020-10-09T04:22:24.550481Z"
    }
   },
   "source": [
    "不过只有具有可学习参数的层(像卷积层/线性层等)才有`state_dict`\n",
    "- 优化器(optim)中也有一个`state_dict`,它包含了优化器的状态和它所使用的超参数信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:01.681466Z",
     "start_time": "2020-10-09T04:37:01.673516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3]}]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存和加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中保存和加载模型的常见方式:\n",
    "- 仅仅保存和加载模型参数(state_dict)\n",
    "- 保存和加载整个模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用state_dict保存参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模板:\n",
    "```\n",
    "# 保存\n",
    "# 使用.pt或者.pth文件格式命名\n",
    "torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "# 加载\n",
    "# 先定义一个模型\n",
    "model = ModelName(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(path))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存和加载整个模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模板:\n",
    "```\n",
    "# 保存\n",
    "torch.save(model, path)\n",
    "\n",
    "\n",
    "# 加载\n",
    "model = torch.load(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpu信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:03.057541Z",
     "start_time": "2020-10-09T04:37:02.937609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct  9 12:37:02 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce MX250       Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   57C    P0    N/A /  N/A |    159MiB /  2002MiB |      9%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1025      G   /usr/lib/xorg/Xorg                           148MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# 查看显卡信息\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:03.111743Z",
     "start_time": "2020-10-09T04:37:03.109582Z"
    }
   },
   "outputs": [],
   "source": [
    "# 动态显示显卡信息(terminal中运行比较好)\n",
    "# watch -n 10 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:03.428795Z",
     "start_time": "2020-10-09T04:37:03.250384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dev/nvidia0:         7343\r\n",
      "/dev/nvidiactl:       7343\r\n",
      "/dev/nvidia-uvm:      7343\r\n"
     ]
    }
   ],
   "source": [
    "# 查看有那些进程占用gpu\n",
    "!fuser /dev/nvidia*\n",
    "\n",
    "# 可以使用kill -9 pid强制杀死进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过由于Pytorch有缓存分配器,所以`nvidia-smi`的显存显示可能不太准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:17:12.443208Z",
     "start_time": "2020-10-09T05:17:12.437779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看tensor的占用情况\n",
    "torch.cuda.memory_allocated()\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:17:13.301737Z",
     "start_time": "2020-10-09T05:17:13.285077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097152"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2097152"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看缓存分配器的占用情况\n",
    "torch.cuda.memory_reserved()\n",
    "torch.cuda.max_memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T04:37:03.896394Z",
     "start_time": "2020-10-09T04:37:03.894043Z"
    }
   },
   "outputs": [],
   "source": [
    "# 清除不同的gpu缓存,但是正在占用gpu的tensor无法释放\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:12:42.697919Z",
     "start_time": "2020-10-09T05:12:42.687806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看gpu是否可用\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:13:01.755821Z",
     "start_time": "2020-10-09T05:13:01.745163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看gpu数量\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:13:27.092163Z",
     "start_time": "2020-10-09T05:13:27.082663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看gpu索引号\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:13:57.282032Z",
     "start_time": "2020-10-09T05:13:57.277463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce MX250'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据索引号看gpu的名字\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpu计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:17:07.199040Z",
     "start_time": "2020-10-09T05:17:05.209395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x\n",
    "\n",
    "# 转到cuda(有几种方式)\n",
    "device = torch.device(\"cuda\")\n",
    "x.to(device)\n",
    "x\n",
    "\n",
    "x.to(\"cpu\")\n",
    "x.cuda(0) # 和x.cuda()等价\n",
    "\n",
    "x = torch.tensor([1, 2, 3], device=\"cuda:0\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值的注意的是,cpu和gpu之间,各gpu之间不能直接进行运算,使用`.device`属性来获取tensor在哪里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外,传到gpu后,该tensor就不再是位于叶子节点了,这时梯度回传时,就没有梯度,要使用retain_grad才能拿得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:32:27.316535Z",
     "start_time": "2020-10-09T05:32:27.303583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-94-803eb5c4dcc9>:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(x.grad)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "# cuda之后就变成了非叶子节点,直接用grad是拿不到梯度的\n",
    "# 但是在模型训练的时候,通常来说我们想要的是weight的梯度,同时把输入x传到gpu,\n",
    "# 在梯度回传的时候weight仍然是叶子节点, 并且我们并不需要x的梯度\n",
    "x = x.cuda()\n",
    "# x = x.to(device)\n",
    "\n",
    "z = x.sum()\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "# print(x.retain_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:29:52.564875Z",
     "start_time": "2020-10-09T05:29:52.557863Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T05:25:25.508316Z",
     "start_time": "2020-10-09T05:25:25.493985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 直接在gpu上生成tensor,就能有梯度在回传时\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True, device=\"cuda\")\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
