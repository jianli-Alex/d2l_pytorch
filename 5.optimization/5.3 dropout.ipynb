{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T13:37:28.280392Z",
     "start_time": "2020-10-06T13:37:28.268916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不像机器学习一样，`weight_decay`对深度学习的效果收效甚微，深度学习常常用dropout(丢弃法)来应对过拟合\n",
    "- 但是在批量归一化(batch normalization)出现后，也可以用来替代dropout\n",
    "- dropout 有点像集成学习，通过随机丢弃神经元来拟合出不同的神经网络模型，从而降低最后的方差。\n",
    "- pytorch的是以概率p来丢弃神经元，而tensorflow是以概率p来保留神经元\n",
    "- 通常dropout是对全连接层（fc层）使用的，很少或者不对卷积层使用，因为卷积的参数本来就少，再dropout往往欠拟合\n",
    "- 在做dropout的时候，有p概率的神经元会被清零，有1-p概率的神经元会除以1-p来做拉伸（dropout保持输入输出的期望值不变）\n",
    "    - 设随机变量$\\xi$为0和1的概率为p和1-p，计算新的神经元$h^{'}_i$\n",
    "    - $h^{'}_i = \\frac{\\xi_i}{1-p}h_i$\n",
    "    - 因为$E(\\xi_i) = 1- p$， 所以丢弃后的期望值和丢弃前的期望值是一样的\n",
    "    - $E(h^{'}_i) = \\frac{E(\\xi_i)}{1-p} h_i = h_i$\n",
    "- 测试模型的时候，为了拿到更加确定性的结果，一般不使用dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T13:31:14.906370Z",
     "start_time": "2020-10-06T13:31:14.899661Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T13:57:16.959534Z",
     "start_time": "2020-10-06T13:57:16.952929Z"
    }
   },
   "outputs": [],
   "source": [
    "def dropout(x, drop_prob):\n",
    "    # drop_prob是丢弃的概率\n",
    "    x = x.float()\n",
    "    # 如果丢弃概率大于1时，判错\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    if drop_prob == 1:\n",
    "        return torch.zeros_like(x)\n",
    "    mask = (torch.rand(x.shape) > drop_prob).float()\n",
    "    # 以1-p扩大剩下的神经元，保持dropout前后的期望值一致\n",
    "    return mask / (1 - drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T13:57:17.867766Z",
     "start_time": "2020-10-06T13:57:17.852860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 2., 0., 0., 0., 0., 0.],\n",
       "        [0., 2., 0., 2., 0., 0., 0., 2.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(16).reshape(2, 8)\n",
    "dropout(x, 1)\n",
    "dropout(x, 0.5)\n",
    "dropout(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:04:25.869188Z",
     "start_time": "2020-10-06T14:04:25.858538Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用伯努利分布来实现\n",
    "def dropout2(x, drop_prob):\n",
    "    # 伯努利分布中的drop_prob是保留概率,比如0.3,就是有30%的概率的神经元为1\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    if drop_prob == 0:\n",
    "        return torch.zeros_like(x)\n",
    "    m = torch.distributions.Bernoulli(torch.tensor([float(drop_prob)]))\n",
    "    mask = m.sample(x.shape).view(x.shape)\n",
    "    return mask / drop_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:04:27.254519Z",
     "start_time": "2020-10-06T14:04:27.231011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 2., 2., 0.],\n",
       "        [2., 0., 0., 2., 0., 2., 2., 2.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(16).reshape(2, 8)\n",
    "dropout2(x, 1)\n",
    "dropout2(x, 0.5)\n",
    "dropout2(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T13:37:41.467526Z",
     "start_time": "2020-10-06T13:37:41.460485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.distributions.Bernoulli(torch.tensor([0.9]))\n",
    "m.sample((10, 10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T13:34:33.889182Z",
     "start_time": "2020-10-06T13:34:33.871567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Bernoulli in module torch.distributions.bernoulli:\n",
      "\n",
      "class Bernoulli(torch.distributions.exp_family.ExponentialFamily)\n",
      " |  Bernoulli(probs=None, logits=None, validate_args=None)\n",
      " |  \n",
      " |  Creates a Bernoulli distribution parameterized by :attr:`probs`\n",
      " |  or :attr:`logits` (but not both).\n",
      " |  \n",
      " |  Samples are binary (0 or 1). They take the value `1` with probability `p`\n",
      " |  and `0` with probability `1 - p`.\n",
      " |  \n",
      " |  Example::\n",
      " |  \n",
      " |      >>> m = Bernoulli(torch.tensor([0.3]))\n",
      " |      >>> m.sample()  # 30% chance 1; 70% chance 0\n",
      " |      tensor([ 0.])\n",
      " |  \n",
      " |  Args:\n",
      " |      probs (Number, Tensor): the probability of sampling `1`\n",
      " |      logits (Number, Tensor): the log-odds of sampling `1`\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Bernoulli\n",
      " |      torch.distributions.exp_family.ExponentialFamily\n",
      " |      torch.distributions.distribution.Distribution\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  entropy(self)\n",
      " |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      " |  \n",
      " |  enumerate_support(self, expand=True)\n",
      " |      Returns tensor containing all values supported by a discrete\n",
      " |      distribution. The result will enumerate over dimension 0, so the shape\n",
      " |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      " |      (where `event_shape = ()` for univariate distributions).\n",
      " |      \n",
      " |      Note that this enumerates over all batched tensors in lock-step\n",
      " |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      " |      along dim 0, but with the remaining batch dimensions being\n",
      " |      singleton dimensions, `[[0], [1], ..`.\n",
      " |      \n",
      " |      To iterate over the full Cartesian product use\n",
      " |      `itertools.product(m.enumerate_support())`.\n",
      " |      \n",
      " |      Args:\n",
      " |          expand (bool): whether to expand the support over the\n",
      " |              batch dims to match the distribution's `batch_shape`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Tensor iterating over dimension 0.\n",
      " |  \n",
      " |  expand(self, batch_shape, _instance=None)\n",
      " |      Returns a new distribution instance (or populates an existing instance\n",
      " |      provided by a derived class) with batch dimensions expanded to\n",
      " |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      " |      the distribution's parameters. As such, this does not allocate new\n",
      " |      memory for the expanded distribution instance. Additionally,\n",
      " |      this does not repeat any args checking or parameter broadcasting in\n",
      " |      `__init__.py`, when an instance is first created.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_shape (torch.Size): the desired expanded size.\n",
      " |          _instance: new instance provided by subclasses that\n",
      " |              need to override `.expand`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New distribution instance with batch dimensions expanded to\n",
      " |          `batch_size`.\n",
      " |  \n",
      " |  log_prob(self, value)\n",
      " |      Returns the log of the probability density/mass function evaluated at\n",
      " |      `value`.\n",
      " |      \n",
      " |      Args:\n",
      " |          value (Tensor):\n",
      " |  \n",
      " |  logits(...)\n",
      " |  \n",
      " |  probs(...)\n",
      " |  \n",
      " |  sample(self, sample_shape=torch.Size([]))\n",
      " |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      " |      samples if the distribution parameters are batched.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  mean\n",
      " |      Returns the mean of the distribution.\n",
      " |  \n",
      " |  param_shape\n",
      " |  \n",
      " |  variance\n",
      " |      Returns the variance of the distribution.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      " |  \n",
      " |  has_enumerate_support = True\n",
      " |  \n",
      " |  support = Boolean()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  cdf(self, value)\n",
      " |      Returns the cumulative density/mass function evaluated at\n",
      " |      `value`.\n",
      " |      \n",
      " |      Args:\n",
      " |          value (Tensor):\n",
      " |  \n",
      " |  icdf(self, value)\n",
      " |      Returns the inverse cumulative density/mass function evaluated at\n",
      " |      `value`.\n",
      " |      \n",
      " |      Args:\n",
      " |          value (Tensor):\n",
      " |  \n",
      " |  perplexity(self)\n",
      " |      Returns perplexity of distribution, batched over batch_shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Tensor of shape batch_shape.\n",
      " |  \n",
      " |  rsample(self, sample_shape=torch.Size([]))\n",
      " |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      " |      shaped batch of reparameterized samples if the distribution parameters\n",
      " |      are batched.\n",
      " |  \n",
      " |  sample_n(self, n)\n",
      " |      Generates n samples or n batches of samples if the distribution\n",
      " |      parameters are batched.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  set_default_validate_args(value)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  batch_shape\n",
      " |      Returns the shape over which parameters are batched.\n",
      " |  \n",
      " |  event_shape\n",
      " |      Returns the shape of a single sample (without batching).\n",
      " |  \n",
      " |  stddev\n",
      " |      Returns the standard deviation of the distribution.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  has_rsample = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.distributions.Bernoulli)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
