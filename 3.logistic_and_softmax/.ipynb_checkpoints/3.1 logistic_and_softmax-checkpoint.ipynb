{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T07:46:07.613964Z",
     "start_time": "2020-10-03T07:46:07.579957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic是一种处理2分类的线性模型，其中，$x \\in R^D, y \\in {0, 1}$，我们希望找到一个非线性函数$g()$，使得$R^D --> {0, 1}$，以此来预测后验概率$P(Y=1|X)$\n",
    "- 模型：$P(y=1|X) = \\frac{1}{1+e^{-w^Tx}}$，$P(y=0|X) = \\frac{e^{-w^Tx}}{1+e^{-w^Tx}}$\n",
    "- 损失函数：$L = -\\frac{1}{N} \\sum_{n=1}^N y^{(n)} log(\\hat y^{(n)}) + (1 - y^{(n)}) log(1 - \\hat y^{(n)})$\n",
    "- 梯度：\n",
    "    - $\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{n=1}^Nx^{n} (\\hat y^{(n)} - y^{(n)})$\n",
    "    - $\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^N (\\hat y^{(n)} - y^{(n)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下使用sklearn的鸢尾花数据集来实现logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:52:48.398136Z",
     "start_time": "2020-10-03T09:52:48.385622Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import data_iter\n",
    "from sqdm import sqdm\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:52:48.674061Z",
     "start_time": "2020-10-03T09:52:48.666846Z"
    }
   },
   "outputs": [],
   "source": [
    "def bootstrap(x, y):\n",
    "    \"\"\"自助法实现\"\"\"\n",
    "    data_num = len(y)\n",
    "    \n",
    "    # 训练集的index和测试集的index\n",
    "    batch_index = np.random.choice(data_num, size=data_num, replace=True)\n",
    "    out_index = np.array(list(set(range(data_num)).difference(set(batch_index))))\n",
    "    \n",
    "    # 训练集\n",
    "    xtrain, ytrain = x[batch_index], y[batch_index]\n",
    "    # 测试集\n",
    "    xtest, ytest = x[out_index], y[out_index]\n",
    "    \n",
    "    return xtrain, ytrain, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:52:49.191252Z",
     "start_time": "2020-10-03T09:52:49.185432Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理数据集\n",
    "iris = load_iris()\n",
    "iris_data = np.hstack((iris.data, np.expand_dims(iris.target, 1)))\n",
    "iris_data = iris_data[iris.target < 2]\n",
    "xtrain, ytrain, xtest, ytest = bootstrap(iris_data[:, :4], iris_data[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:52:49.407018Z",
     "start_time": "2020-10-03T09:52:49.396455Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticModel(object):\n",
    "    def __init__(self, alpha=0.01, weight_decay=0):\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "        self.alpha = alpha\n",
    "        self.weight_decay = weight_decay\n",
    "        self.count = 0\n",
    "        \n",
    "    def linreg(self, X):\n",
    "        return X@self.w + self.b\n",
    "    \n",
    "    def sigmoid(self, y):\n",
    "        return 1 / (1 + np.exp(-y))\n",
    "    \n",
    "    def entropy_loss(self, y_pred, y):\n",
    "        y_pred = np.where(y==0, 1-y_pred, y_pred)\n",
    "        loss = -(np.log(y_pred).sum())/len(y)\n",
    "        return loss\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        fea_num = int(X.size / len(y))\n",
    "        if self.count == 0:\n",
    "            self.w = np.zeros(fea_num)\n",
    "        \n",
    "        # reshape X and y\n",
    "        X = X.reshape(len(y), fea_num)\n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        # predict\n",
    "        y_pred = self.predict_prob(X)\n",
    "        \n",
    "        # update grad\n",
    "        dw = (X.T@(y_pred - y)).sum()/len(y) + self.weight_decay*self.w\n",
    "        db = (y_pred - y).sum()/len(y) + self.weight_decay*self.b\n",
    "        self.w -= self.alpha * dw\n",
    "        self.b -= self.alpha * db\n",
    "        self.count += 1\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        y_pred = self.sigmoid(self.linreg(X)).reshape(-1)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        y_pred = np.where(y_pred>0.5, 1, 0)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        acc = (y_pred == y).sum()/len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:53:01.140654Z",
     "start_time": "2020-10-03T09:52:49.681807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1455, train_score: 1.00, test_loss: 0.9576, test_score: 0.44\n",
      "\n",
      "Epoch [2/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1454, train_score: 1.00, test_loss: 0.9418, test_score: 0.44\n",
      "\n",
      "Epoch [3/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1452, train_score: 1.00, test_loss: 0.9263, test_score: 0.44\n",
      "\n",
      "Epoch [4/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1451, train_score: 1.00, test_loss: 0.9111, test_score: 0.44\n",
      "\n",
      "Epoch [5/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1449, train_score: 1.00, test_loss: 0.8963, test_score: 0.44\n",
      "\n",
      "Epoch [6/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1448, train_score: 1.00, test_loss: 0.8817, test_score: 0.44\n",
      "\n",
      "Epoch [7/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1446, train_score: 1.00, test_loss: 0.8675, test_score: 0.44\n",
      "\n",
      "Epoch [8/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1445, train_score: 1.00, test_loss: 0.8535, test_score: 0.44\n",
      "\n",
      "Epoch [9/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1443, train_score: 1.00, test_loss: 0.8399, test_score: 0.44\n",
      "\n",
      "Epoch [10/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1441, train_score: 1.00, test_loss: 0.8265, test_score: 0.44\n",
      "\n",
      "Epoch [11/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1440, train_score: 1.00, test_loss: 0.8135, test_score: 0.44\n",
      "\n",
      "Epoch [12/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1438, train_score: 1.00, test_loss: 0.8007, test_score: 0.44\n",
      "\n",
      "Epoch [13/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1436, train_score: 1.00, test_loss: 0.7882, test_score: 0.44\n",
      "\n",
      "Epoch [14/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1434, train_score: 1.00, test_loss: 0.7760, test_score: 0.44\n",
      "\n",
      "Epoch [15/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1432, train_score: 1.00, test_loss: 0.7641, test_score: 0.44\n",
      "\n",
      "Epoch [16/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1431, train_score: 1.00, test_loss: 0.7525, test_score: 0.44\n",
      "\n",
      "Epoch [17/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1429, train_score: 1.00, test_loss: 0.7411, test_score: 0.44\n",
      "\n",
      "Epoch [18/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1427, train_score: 1.00, test_loss: 0.7300, test_score: 0.44\n",
      "\n",
      "Epoch [19/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1424, train_score: 1.00, test_loss: 0.7191, test_score: 0.44\n",
      "\n",
      "Epoch [20/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1422, train_score: 1.00, test_loss: 0.7085, test_score: 0.44\n",
      "\n",
      "Epoch [21/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1420, train_score: 1.00, test_loss: 0.6982, test_score: 0.44\n",
      "\n",
      "Epoch [22/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1418, train_score: 1.00, test_loss: 0.6881, test_score: 0.44\n",
      "\n",
      "Epoch [23/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1416, train_score: 1.00, test_loss: 0.6782, test_score: 0.44\n",
      "\n",
      "Epoch [24/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1413, train_score: 1.00, test_loss: 0.6686, test_score: 0.44\n",
      "\n",
      "Epoch [25/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1411, train_score: 1.00, test_loss: 0.6592, test_score: 0.46\n",
      "\n",
      "Epoch [26/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1408, train_score: 1.00, test_loss: 0.6501, test_score: 0.46\n",
      "\n",
      "Epoch [27/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1406, train_score: 1.00, test_loss: 0.6411, test_score: 0.46\n",
      "\n",
      "Epoch [28/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1403, train_score: 1.00, test_loss: 0.6324, test_score: 0.46\n",
      "\n",
      "Epoch [29/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1400, train_score: 1.00, test_loss: 0.6239, test_score: 0.49\n",
      "\n",
      "Epoch [30/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1398, train_score: 1.00, test_loss: 0.6156, test_score: 0.49\n",
      "\n",
      "Epoch [31/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1395, train_score: 1.00, test_loss: 0.6075, test_score: 0.49\n",
      "\n",
      "Epoch [32/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1392, train_score: 1.00, test_loss: 0.5996, test_score: 0.49\n",
      "\n",
      "Epoch [33/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1389, train_score: 1.00, test_loss: 0.5919, test_score: 0.49\n",
      "\n",
      "Epoch [34/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1386, train_score: 1.00, test_loss: 0.5844, test_score: 0.51\n",
      "\n",
      "Epoch [35/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1383, train_score: 1.00, test_loss: 0.5771, test_score: 0.51\n",
      "\n",
      "Epoch [36/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1379, train_score: 1.00, test_loss: 0.5700, test_score: 0.54\n",
      "\n",
      "Epoch [37/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1376, train_score: 1.00, test_loss: 0.5630, test_score: 0.54\n",
      "\n",
      "Epoch [38/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1373, train_score: 1.00, test_loss: 0.5562, test_score: 0.54\n",
      "\n",
      "Epoch [39/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1369, train_score: 1.00, test_loss: 0.5496, test_score: 0.56\n",
      "\n",
      "Epoch [40/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1366, train_score: 1.00, test_loss: 0.5431, test_score: 0.56\n",
      "\n",
      "Epoch [41/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1362, train_score: 1.00, test_loss: 0.5368, test_score: 0.62\n",
      "\n",
      "Epoch [42/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1359, train_score: 1.00, test_loss: 0.5306, test_score: 0.62\n",
      "\n",
      "Epoch [43/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1355, train_score: 1.00, test_loss: 0.5246, test_score: 0.62\n",
      "\n",
      "Epoch [44/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1352, train_score: 1.00, test_loss: 0.5188, test_score: 0.62\n",
      "\n",
      "Epoch [45/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1348, train_score: 1.00, test_loss: 0.5130, test_score: 0.62\n",
      "\n",
      "Epoch [46/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1344, train_score: 1.00, test_loss: 0.5074, test_score: 0.64\n",
      "\n",
      "Epoch [47/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1341, train_score: 1.00, test_loss: 0.5020, test_score: 0.64\n",
      "\n",
      "Epoch [48/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1337, train_score: 1.00, test_loss: 0.4966, test_score: 0.64\n",
      "\n",
      "Epoch [49/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1333, train_score: 1.00, test_loss: 0.4914, test_score: 0.64\n",
      "\n",
      "Epoch [50/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1329, train_score: 1.00, test_loss: 0.4863, test_score: 0.64\n",
      "\n",
      "Epoch [51/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1325, train_score: 1.00, test_loss: 0.4813, test_score: 0.67\n",
      "\n",
      "Epoch [52/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1321, train_score: 1.00, test_loss: 0.4765, test_score: 0.67\n",
      "\n",
      "Epoch [53/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1317, train_score: 1.00, test_loss: 0.4717, test_score: 0.67\n",
      "\n",
      "Epoch [54/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1313, train_score: 1.00, test_loss: 0.4671, test_score: 0.72\n",
      "\n",
      "Epoch [55/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1309, train_score: 1.00, test_loss: 0.4625, test_score: 0.72\n",
      "\n",
      "Epoch [56/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1305, train_score: 1.00, test_loss: 0.4581, test_score: 0.72\n",
      "\n",
      "Epoch [57/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1301, train_score: 1.00, test_loss: 0.4537, test_score: 0.72\n",
      "\n",
      "Epoch [58/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1298, train_score: 1.00, test_loss: 0.4494, test_score: 0.72\n",
      "\n",
      "Epoch [59/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1293, train_score: 1.00, test_loss: 0.4453, test_score: 0.72\n",
      "\n",
      "Epoch [60/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1290, train_score: 1.00, test_loss: 0.4412, test_score: 0.72\n",
      "\n",
      "Epoch [61/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1285, train_score: 1.00, test_loss: 0.4372, test_score: 0.72\n",
      "\n",
      "Epoch [62/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1281, train_score: 1.00, test_loss: 0.4333, test_score: 0.74\n",
      "\n",
      "Epoch [63/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1278, train_score: 1.00, test_loss: 0.4294, test_score: 0.74\n",
      "\n",
      "Epoch [64/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1274, train_score: 1.00, test_loss: 0.4257, test_score: 0.74\n",
      "\n",
      "Epoch [65/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1270, train_score: 1.00, test_loss: 0.4220, test_score: 0.74\n",
      "\n",
      "Epoch [66/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1266, train_score: 1.00, test_loss: 0.4184, test_score: 0.77\n",
      "\n",
      "Epoch [67/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1262, train_score: 1.00, test_loss: 0.4149, test_score: 0.77\n",
      "\n",
      "Epoch [68/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1258, train_score: 1.00, test_loss: 0.4114, test_score: 0.77\n",
      "\n",
      "Epoch [69/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1254, train_score: 1.00, test_loss: 0.4080, test_score: 0.77\n",
      "\n",
      "Epoch [70/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1250, train_score: 1.00, test_loss: 0.4047, test_score: 0.77\n",
      "\n",
      "Epoch [71/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1246, train_score: 1.00, test_loss: 0.4014, test_score: 0.77\n",
      "\n",
      "Epoch [72/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1242, train_score: 1.00, test_loss: 0.3982, test_score: 0.77\n",
      "\n",
      "Epoch [73/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1239, train_score: 1.00, test_loss: 0.3951, test_score: 0.77\n",
      "\n",
      "Epoch [74/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1235, train_score: 1.00, test_loss: 0.3920, test_score: 0.77\n",
      "\n",
      "Epoch [75/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1231, train_score: 1.00, test_loss: 0.3890, test_score: 0.77\n",
      "\n",
      "Epoch [76/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1228, train_score: 1.00, test_loss: 0.3860, test_score: 0.77\n",
      "\n",
      "Epoch [77/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1224, train_score: 1.00, test_loss: 0.3831, test_score: 0.87\n",
      "\n",
      "Epoch [78/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1220, train_score: 1.00, test_loss: 0.3802, test_score: 0.87\n",
      "\n",
      "Epoch [79/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1217, train_score: 1.00, test_loss: 0.3774, test_score: 0.87\n",
      "\n",
      "Epoch [80/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1213, train_score: 1.00, test_loss: 0.3746, test_score: 0.87\n",
      "\n",
      "Epoch [81/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1210, train_score: 1.00, test_loss: 0.3719, test_score: 0.87\n",
      "\n",
      "Epoch [82/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1206, train_score: 1.00, test_loss: 0.3692, test_score: 0.87\n",
      "\n",
      "Epoch [83/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1202, train_score: 1.00, test_loss: 0.3665, test_score: 0.87\n",
      "\n",
      "Epoch [84/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1199, train_score: 1.00, test_loss: 0.3640, test_score: 0.90\n",
      "\n",
      "Epoch [85/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1196, train_score: 1.00, test_loss: 0.3614, test_score: 0.90\n",
      "\n",
      "Epoch [86/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1192, train_score: 1.00, test_loss: 0.3589, test_score: 0.90\n",
      "\n",
      "Epoch [87/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1189, train_score: 1.00, test_loss: 0.3565, test_score: 0.90\n",
      "\n",
      "Epoch [88/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1186, train_score: 1.00, test_loss: 0.3540, test_score: 0.90\n",
      "\n",
      "Epoch [89/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1182, train_score: 1.00, test_loss: 0.3517, test_score: 0.90\n",
      "\n",
      "Epoch [90/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1179, train_score: 1.00, test_loss: 0.3493, test_score: 0.90\n",
      "\n",
      "Epoch [91/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1176, train_score: 1.00, test_loss: 0.3470, test_score: 0.90\n",
      "\n",
      "Epoch [92/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1173, train_score: 1.00, test_loss: 0.3447, test_score: 0.92\n",
      "\n",
      "Epoch [93/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1170, train_score: 1.00, test_loss: 0.3425, test_score: 0.92\n",
      "\n",
      "Epoch [94/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1167, train_score: 1.00, test_loss: 0.3403, test_score: 0.92\n",
      "\n",
      "Epoch [95/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1164, train_score: 1.00, test_loss: 0.3381, test_score: 0.92\n",
      "\n",
      "Epoch [96/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1161, train_score: 1.00, test_loss: 0.3360, test_score: 0.92\n",
      "\n",
      "Epoch [97/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1158, train_score: 1.00, test_loss: 0.3339, test_score: 0.92\n",
      "\n",
      "Epoch [98/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1155, train_score: 1.00, test_loss: 0.3318, test_score: 0.92\n",
      "\n",
      "Epoch [99/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1152, train_score: 1.00, test_loss: 0.3298, test_score: 0.92\n",
      "\n",
      "Epoch [100/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1149, train_score: 1.00, test_loss: 0.3278, test_score: 0.92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    \"model\": LogisticModel(alpha=0.02, weight_decay=0),\n",
    "    \"epoch_num\": 100,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "process_bar = sqdm()\n",
    "def train(model, epoch_num, batch_size):\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, xtrain, ytrain):\n",
    "            model.fit(xdata, ydata)\n",
    "            \n",
    "            # train\n",
    "            train_pred = model.predict_prob(xdata)\n",
    "            train_loss = round(model.entropy_loss(train_pred, ydata.reshape(train_pred.shape)), 5)\n",
    "            train_acc = model.score(xdata, ydata)\n",
    "            \n",
    "            # test\n",
    "            test_pred = model.predict_prob(xtest)\n",
    "            test_loss = round(model.entropy_loss(test_pred, ytest.reshape(test_pred.shape)), 5)\n",
    "            test_acc = model.score(xtest, ytest)\n",
    "\n",
    "            process_bar.show_process(len(ytrain), batch_size, train_loss=train_loss, \n",
    "                                    test_loss=test_loss, train_score=train_acc, test_score=test_acc)\n",
    "            \n",
    "        print(\"\\n\")\n",
    "    return model\n",
    "    \n",
    "model = train(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:53:01.159438Z",
     "start_time": "2020-10-03T09:53:01.142886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集结果\n",
    "model.predict(iris_data[:, :4])\n",
    "model.score(iris_data[:, :4], iris_data[:, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax可以看成是一个多项的logistic，实际上softmax是一种条件最大熵模型\n",
    "- 对于某个样本属于第c类的概率为：\n",
    "    - $P(y=c|x) = \\frac{exp(w^T_cx)}{\\sum_{c'=1}^{C}exp(w^T_{c'}x)}$\n",
    "    - 决策函数为\n",
    "        - $\\hat y = \\underset{c}{arg min} \\  P(y=c|x) = \\underset{c}{arg min}\\  w^T_cx$\n",
    "- 损失函数：$L = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^{C} y_c^{(n)}log(\\hat y_c^{(n)})= - \\sum_{n=1}^N  (y^{(n)})^Tlog(\\hat y^{(n)})$\n",
    "- 梯度：\n",
    "    - $\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{n=1}^N x^{(n)}(\\hat y^{(n)} - y^{(n)})^T$\n",
    "    - $\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^N (\\hat y^{(n)} - y^{(n)})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是先以鸢尾花的例子来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T10:55:39.479225Z",
     "start_time": "2020-10-03T10:55:39.454998Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理数据集\n",
    "iris = load_iris()\n",
    "iris_data = np.hstack((iris.data, np.expand_dims(iris.target, 1)))\n",
    "xtrain, ytrain, xtest, ytest = bootstrap(iris_data[:, :4], iris_data[:, 4])\n",
    "\n",
    "# 处理标签--> (0 --> [1, 0, 0])\n",
    "label_dict = {\n",
    "    0: [1, 0, 0],\n",
    "    1: [0, 1, 0],\n",
    "    2: [0, 0, 1]\n",
    "}\n",
    "\n",
    "data = np.array(list(map(lambda x: label_dict[x], iris_data[:, 4])))\n",
    "ytrain = np.array(list(map(lambda x: label_dict[x], list(ytrain))))\n",
    "ytest = np.array(list(map(lambda x: label_dict[x], list(ytest))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T10:55:50.978207Z",
     "start_time": "2020-10-03T10:55:50.950417Z"
    }
   },
   "outputs": [],
   "source": [
    "class SoftmaxModel(object):\n",
    "    \"\"\"实现softmax\"\"\"\n",
    "    def __init__(self, fea_num, cate_num, alpha=0.01, weight_decay=0):\n",
    "        self.w = np.zeros([fea_num, cate_num])\n",
    "        self.b = np.zeros(cate_num)\n",
    "        self.fea_num = fea_num\n",
    "        self.cate_num = cate_num\n",
    "        self.alpha = alpha\n",
    "        self.weight_decay = weight_decay\n",
    "        self.count = 0\n",
    "        \n",
    "    def linreg(self, X):\n",
    "        return X@self.w + self.b\n",
    "    \n",
    "    def softmax(self, y):\n",
    "        return np.exp(y)/np.expand_dims(np.exp(y).sum(axis = 1), 1)\n",
    "    \n",
    "    def entropy_loss(self, y_pred, y):\n",
    "        loss = -(y*np.log(y_pred)).sum()/len(y)\n",
    "        return loss\n",
    "    \n",
    "    def cal_grad(self, X, y_diff):\n",
    "        result = np.zeros([self.fea_num, self.cate_num])\n",
    "        for i in range(len(X)):\n",
    "            result += np.outer(X.T[:, i], y_diff[i, :])\n",
    "        return result / len(X)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # predict\n",
    "        y_pred = self.predict_prob(X)\n",
    "        \n",
    "        # update_grad\n",
    "        dw = self.cal_grad(X, (y_pred-y)) + self.weight_decay * self.w\n",
    "        db = (y_pred-y).sum(axis=0)/len(y) + self.weight_decay * self.b\n",
    "        self.w -= self.alpha * dw\n",
    "        self.b -= self.alpha * db\n",
    "        self.count += 1\n",
    "        \n",
    "            \n",
    "    def predict_prob(self, X):\n",
    "        y_pred = self.softmax(self.linreg(X))\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        pred_index = np.argmax(y_pred, axis=1)\n",
    "        return pred_index\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        pred_index = np.argmax(y_pred, axis=1)\n",
    "        label_index = np.argmax(y, axis = 1)\n",
    "        acc = (pred_index == label_index).sum()/len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T11:04:55.220651Z",
     "start_time": "2020-10-03T11:04:34.651537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.3278, train_score: 0.00, test_loss: 0.7245, test_score: 0.69\n",
      "\n",
      "Epoch [2/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.2187, train_score: 0.00, test_loss: 0.5962, test_score: 0.69\n",
      "\n",
      "Epoch [3/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.1511, train_score: 0.00, test_loss: 0.5335, test_score: 0.69\n",
      "\n",
      "Epoch [4/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.1013, train_score: 0.00, test_loss: 0.4920, test_score: 0.69\n",
      "\n",
      "Epoch [5/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0615, train_score: 0.00, test_loss: 0.4608, test_score: 0.71\n",
      "\n",
      "Epoch [6/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 1.0283, train_score: 0.00, test_loss: 0.4356, test_score: 0.75\n",
      "\n",
      "Epoch [7/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9998, train_score: 0.00, test_loss: 0.4145, test_score: 0.76\n",
      "\n",
      "Epoch [8/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9749, train_score: 0.00, test_loss: 0.3965, test_score: 0.78\n",
      "\n",
      "Epoch [9/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9530, train_score: 0.00, test_loss: 0.3808, test_score: 0.78\n",
      "\n",
      "Epoch [10/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9335, train_score: 0.00, test_loss: 0.3670, test_score: 0.78\n",
      "\n",
      "Epoch [11/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9160, train_score: 0.00, test_loss: 0.3548, test_score: 0.78\n",
      "\n",
      "Epoch [12/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.9002, train_score: 0.00, test_loss: 0.3439, test_score: 0.78\n",
      "\n",
      "Epoch [13/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8859, train_score: 0.00, test_loss: 0.3341, test_score: 0.80\n",
      "\n",
      "Epoch [14/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8730, train_score: 0.00, test_loss: 0.3253, test_score: 0.82\n",
      "\n",
      "Epoch [15/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8611, train_score: 0.00, test_loss: 0.3173, test_score: 0.86\n",
      "\n",
      "Epoch [16/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8503, train_score: 0.00, test_loss: 0.3100, test_score: 0.86\n",
      "\n",
      "Epoch [17/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8403, train_score: 0.00, test_loss: 0.3034, test_score: 0.86\n",
      "\n",
      "Epoch [18/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8311, train_score: 0.00, test_loss: 0.2974, test_score: 0.88\n",
      "\n",
      "Epoch [19/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8227, train_score: 0.00, test_loss: 0.2918, test_score: 0.88\n",
      "\n",
      "Epoch [20/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8148, train_score: 0.00, test_loss: 0.2867, test_score: 0.90\n",
      "\n",
      "Epoch [21/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8076, train_score: 0.00, test_loss: 0.2820, test_score: 0.92\n",
      "\n",
      "Epoch [22/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.8008, train_score: 0.00, test_loss: 0.2776, test_score: 0.92\n",
      "\n",
      "Epoch [23/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7945, train_score: 0.00, test_loss: 0.2736, test_score: 0.92\n",
      "\n",
      "Epoch [24/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7887, train_score: 0.00, test_loss: 0.2698, test_score: 0.92\n",
      "\n",
      "Epoch [25/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7832, train_score: 0.00, test_loss: 0.2663, test_score: 0.92\n",
      "\n",
      "Epoch [26/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7781, train_score: 0.00, test_loss: 0.2631, test_score: 0.92\n",
      "\n",
      "Epoch [27/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7732, train_score: 0.00, test_loss: 0.2601, test_score: 0.92\n",
      "\n",
      "Epoch [28/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7687, train_score: 0.00, test_loss: 0.2572, test_score: 0.92\n",
      "\n",
      "Epoch [29/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7645, train_score: 0.00, test_loss: 0.2546, test_score: 0.92\n",
      "\n",
      "Epoch [30/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7605, train_score: 0.00, test_loss: 0.2521, test_score: 0.92\n",
      "\n",
      "Epoch [31/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7568, train_score: 0.00, test_loss: 0.2498, test_score: 0.92\n",
      "\n",
      "Epoch [32/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7532, train_score: 0.00, test_loss: 0.2476, test_score: 0.92\n",
      "\n",
      "Epoch [33/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7499, train_score: 0.00, test_loss: 0.2456, test_score: 0.92\n",
      "\n",
      "Epoch [34/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7467, train_score: 0.00, test_loss: 0.2436, test_score: 0.92\n",
      "\n",
      "Epoch [35/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7437, train_score: 0.00, test_loss: 0.2418, test_score: 0.92\n",
      "\n",
      "Epoch [36/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7409, train_score: 0.00, test_loss: 0.2401, test_score: 0.92\n",
      "\n",
      "Epoch [37/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7382, train_score: 0.00, test_loss: 0.2385, test_score: 0.92\n",
      "\n",
      "Epoch [38/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7356, train_score: 0.00, test_loss: 0.2369, test_score: 0.92\n",
      "\n",
      "Epoch [39/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7332, train_score: 0.00, test_loss: 0.2355, test_score: 0.92\n",
      "\n",
      "Epoch [40/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7309, train_score: 0.00, test_loss: 0.2341, test_score: 0.92\n",
      "\n",
      "Epoch [41/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7287, train_score: 0.00, test_loss: 0.2328, test_score: 0.92\n",
      "\n",
      "Epoch [42/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7267, train_score: 0.00, test_loss: 0.2316, test_score: 0.92\n",
      "\n",
      "Epoch [43/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7247, train_score: 0.00, test_loss: 0.2304, test_score: 0.92\n",
      "\n",
      "Epoch [44/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7228, train_score: 0.00, test_loss: 0.2293, test_score: 0.92\n",
      "\n",
      "Epoch [45/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7210, train_score: 0.00, test_loss: 0.2282, test_score: 0.92\n",
      "\n",
      "Epoch [46/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7193, train_score: 0.00, test_loss: 0.2272, test_score: 0.92\n",
      "\n",
      "Epoch [47/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7177, train_score: 0.00, test_loss: 0.2263, test_score: 0.92\n",
      "\n",
      "Epoch [48/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7161, train_score: 0.00, test_loss: 0.2254, test_score: 0.92\n",
      "\n",
      "Epoch [49/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7146, train_score: 0.00, test_loss: 0.2245, test_score: 0.92\n",
      "\n",
      "Epoch [50/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7132, train_score: 0.00, test_loss: 0.2237, test_score: 0.92\n",
      "\n",
      "Epoch [51/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7118, train_score: 0.00, test_loss: 0.2229, test_score: 0.90\n",
      "\n",
      "Epoch [52/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7106, train_score: 0.00, test_loss: 0.2221, test_score: 0.90\n",
      "\n",
      "Epoch [53/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7093, train_score: 0.00, test_loss: 0.2214, test_score: 0.90\n",
      "\n",
      "Epoch [54/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7081, train_score: 0.00, test_loss: 0.2208, test_score: 0.90\n",
      "\n",
      "Epoch [55/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7070, train_score: 0.00, test_loss: 0.2201, test_score: 0.90\n",
      "\n",
      "Epoch [56/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7059, train_score: 0.00, test_loss: 0.2195, test_score: 0.90\n",
      "\n",
      "Epoch [57/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7049, train_score: 0.00, test_loss: 0.2189, test_score: 0.90\n",
      "\n",
      "Epoch [58/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7039, train_score: 0.00, test_loss: 0.2183, test_score: 0.90\n",
      "\n",
      "Epoch [59/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7029, train_score: 0.00, test_loss: 0.2178, test_score: 0.90\n",
      "\n",
      "Epoch [60/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7020, train_score: 0.00, test_loss: 0.2173, test_score: 0.90\n",
      "\n",
      "Epoch [61/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7011, train_score: 0.00, test_loss: 0.2168, test_score: 0.90\n",
      "\n",
      "Epoch [62/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.7003, train_score: 0.00, test_loss: 0.2163, test_score: 0.90\n",
      "\n",
      "Epoch [63/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6995, train_score: 0.00, test_loss: 0.2158, test_score: 0.90\n",
      "\n",
      "Epoch [64/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6987, train_score: 0.00, test_loss: 0.2154, test_score: 0.90\n",
      "\n",
      "Epoch [65/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6979, train_score: 0.00, test_loss: 0.2150, test_score: 0.90\n",
      "\n",
      "Epoch [66/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6972, train_score: 1.00, test_loss: 0.2146, test_score: 0.90\n",
      "\n",
      "Epoch [67/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6966, train_score: 1.00, test_loss: 0.2142, test_score: 0.90\n",
      "\n",
      "Epoch [68/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6959, train_score: 1.00, test_loss: 0.2138, test_score: 0.90\n",
      "\n",
      "Epoch [69/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6953, train_score: 1.00, test_loss: 0.2135, test_score: 0.90\n",
      "\n",
      "Epoch [70/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6947, train_score: 1.00, test_loss: 0.2131, test_score: 0.90\n",
      "\n",
      "Epoch [71/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6941, train_score: 1.00, test_loss: 0.2128, test_score: 0.90\n",
      "\n",
      "Epoch [72/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6935, train_score: 1.00, test_loss: 0.2125, test_score: 0.90\n",
      "\n",
      "Epoch [73/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6930, train_score: 1.00, test_loss: 0.2122, test_score: 0.90\n",
      "\n",
      "Epoch [74/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6925, train_score: 1.00, test_loss: 0.2119, test_score: 0.92\n",
      "\n",
      "Epoch [75/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6920, train_score: 1.00, test_loss: 0.2116, test_score: 0.92\n",
      "\n",
      "Epoch [76/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6915, train_score: 1.00, test_loss: 0.2114, test_score: 0.92\n",
      "\n",
      "Epoch [77/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6910, train_score: 1.00, test_loss: 0.2111, test_score: 0.92\n",
      "\n",
      "Epoch [78/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6906, train_score: 1.00, test_loss: 0.2109, test_score: 0.92\n",
      "\n",
      "Epoch [79/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6902, train_score: 1.00, test_loss: 0.2106, test_score: 0.92\n",
      "\n",
      "Epoch [80/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6897, train_score: 1.00, test_loss: 0.2104, test_score: 0.92\n",
      "\n",
      "Epoch [81/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6893, train_score: 1.00, test_loss: 0.2102, test_score: 0.92\n",
      "\n",
      "Epoch [82/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6890, train_score: 1.00, test_loss: 0.2100, test_score: 0.92\n",
      "\n",
      "Epoch [83/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6886, train_score: 1.00, test_loss: 0.2098, test_score: 0.92\n",
      "\n",
      "Epoch [84/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6882, train_score: 1.00, test_loss: 0.2096, test_score: 0.92\n",
      "\n",
      "Epoch [85/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6879, train_score: 1.00, test_loss: 0.2094, test_score: 0.92\n",
      "\n",
      "Epoch [86/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6876, train_score: 1.00, test_loss: 0.2092, test_score: 0.92\n",
      "\n",
      "Epoch [87/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6873, train_score: 1.00, test_loss: 0.2090, test_score: 0.92\n",
      "\n",
      "Epoch [88/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6870, train_score: 1.00, test_loss: 0.2089, test_score: 0.92\n",
      "\n",
      "Epoch [89/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6867, train_score: 1.00, test_loss: 0.2087, test_score: 0.92\n",
      "\n",
      "Epoch [90/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6864, train_score: 1.00, test_loss: 0.2086, test_score: 0.92\n",
      "\n",
      "Epoch [91/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6861, train_score: 1.00, test_loss: 0.2084, test_score: 0.92\n",
      "\n",
      "Epoch [92/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6858, train_score: 1.00, test_loss: 0.2083, test_score: 0.92\n",
      "\n",
      "Epoch [93/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6856, train_score: 1.00, test_loss: 0.2081, test_score: 0.92\n",
      "\n",
      "Epoch [94/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6854, train_score: 1.00, test_loss: 0.2080, test_score: 0.92\n",
      "\n",
      "Epoch [95/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6851, train_score: 1.00, test_loss: 0.2079, test_score: 0.92\n",
      "\n",
      "Epoch [96/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6849, train_score: 1.00, test_loss: 0.2077, test_score: 0.92\n",
      "\n",
      "Epoch [97/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6847, train_score: 1.00, test_loss: 0.2076, test_score: 0.92\n",
      "\n",
      "Epoch [98/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6845, train_score: 1.00, test_loss: 0.2075, test_score: 0.92\n",
      "\n",
      "Epoch [99/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6843, train_score: 1.00, test_loss: 0.2074, test_score: 0.92\n",
      "\n",
      "Epoch [100/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.6841, train_score: 1.00, test_loss: 0.2073, test_score: 0.92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    \"model\": SoftmaxModel(fea_num=4, cate_num=3, alpha=0.01, weight_decay=0),\n",
    "    \"epoch_num\": 100,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "process_bar = sqdm()\n",
    "def train(model, epoch_num, batch_size):\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, xtrain, ytrain):\n",
    "            model.fit(xdata, ydata)\n",
    "            \n",
    "            # train\n",
    "            train_pred = model.predict_prob(xdata)\n",
    "            train_loss = model.entropy_loss(train_pred, ydata.reshape(train_pred.shape))\n",
    "            train_acc = model.score(xdata, ydata)\n",
    "            \n",
    "            # test\n",
    "            test_pred = model.predict_prob(xtest)\n",
    "            test_loss = model.entropy_loss(test_pred, ytest.reshape(test_pred.shape))\n",
    "            test_acc = model.score(xtest, ytest)\n",
    "\n",
    "            process_bar.show_process(len(ytrain), batch_size, train_loss=train_loss, \n",
    "                                    test_loss=test_loss, train_score=train_acc, test_score=test_acc)\n",
    "            \n",
    "        print(\"\\n\")\n",
    "    return model\n",
    "    \n",
    "model = train(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T11:05:00.975842Z",
     "start_time": "2020-10-03T11:05:00.967102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集结果\n",
    "model.predict(iris_data[:, :4])\n",
    "model.score(iris_data[:, :4], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
