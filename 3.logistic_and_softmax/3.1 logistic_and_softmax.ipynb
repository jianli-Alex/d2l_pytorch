{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:51:39.221408Z",
     "start_time": "2020-10-03T12:51:39.199581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic是一种处理2分类的线性模型，其中，$x \\in R^D, y \\in {0, 1}$，我们希望找到一个非线性函数$g()$，使得$R^D --> {0, 1}$，以此来预测后验概率$P(Y=1|X)$\n",
    "- 模型：$P(y=1|X) = \\frac{1}{1+e^{-w^Tx}}$，$P(y=0|X) = \\frac{e^{-w^Tx}}{1+e^{-w^Tx}}$\n",
    "- 损失函数：$L = -\\frac{1}{N} \\sum_{n=1}^N y^{(n)} log(\\hat y^{(n)}) + (1 - y^{(n)}) log(1 - \\hat y^{(n)})$\n",
    "- 梯度：\n",
    "    - $\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{n=1}^Nx^{n} (\\hat y^{(n)} - y^{(n)})$\n",
    "    - $\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^N (\\hat y^{(n)} - y^{(n)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下使用sklearn的鸢尾花数据集来实现logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:51:41.136660Z",
     "start_time": "2020-10-03T12:51:40.097785Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import data_iter\n",
    "from sqdm import sqdm\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:51:41.146055Z",
     "start_time": "2020-10-03T12:51:41.139350Z"
    }
   },
   "outputs": [],
   "source": [
    "def bootstrap(x, y):\n",
    "    \"\"\"自助法实现\"\"\"\n",
    "    data_num = len(y)\n",
    "    \n",
    "    # 训练集的index和测试集的index\n",
    "    batch_index = np.random.choice(data_num, size=data_num, replace=True)\n",
    "    out_index = np.array(list(set(range(data_num)).difference(set(batch_index))))\n",
    "    \n",
    "    # 训练集\n",
    "    xtrain, ytrain = x[batch_index], y[batch_index]\n",
    "    # 测试集\n",
    "    xtest, ytest = x[out_index], y[out_index]\n",
    "    \n",
    "    return xtrain, ytrain, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:51:41.158883Z",
     "start_time": "2020-10-03T12:51:41.148131Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理数据集\n",
    "iris = load_iris()\n",
    "iris_data = np.hstack((iris.data, np.expand_dims(iris.target, 1)))\n",
    "iris_data = iris_data[iris.target < 2]\n",
    "xtrain, ytrain, xtest, ytest = bootstrap(iris_data[:, :4], iris_data[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:51:41.184198Z",
     "start_time": "2020-10-03T12:51:41.161455Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticModel(object):\n",
    "    def __init__(self, alpha=0.01, weight_decay=0):\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "        self.alpha = alpha\n",
    "        self.weight_decay = weight_decay\n",
    "        self.count = 0\n",
    "        \n",
    "    def linreg(self, X):\n",
    "        return X@self.w + self.b\n",
    "    \n",
    "    def sigmoid(self, y):\n",
    "        return 1 / (1 + np.exp(-y))\n",
    "    \n",
    "    def entropy_loss(self, y_pred, y):\n",
    "        y_pred = np.where(y==0, 1-y_pred, y_pred)\n",
    "        loss = -(np.log(y_pred).sum())/len(y)\n",
    "        return loss\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        fea_num = int(X.size / len(y))\n",
    "        if self.count == 0:\n",
    "            self.w = np.zeros(fea_num)\n",
    "        \n",
    "        # reshape X and y\n",
    "        X = X.reshape(len(y), fea_num)\n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        # predict\n",
    "        y_pred = self.predict_prob(X)\n",
    "        \n",
    "        # update grad\n",
    "        dw = (X.T@(y_pred - y)).sum()/len(y) + self.weight_decay*self.w\n",
    "        db = (y_pred - y).sum()/len(y) + self.weight_decay*self.b\n",
    "        self.w -= self.alpha * dw\n",
    "        self.b -= self.alpha * db\n",
    "        self.count += 1\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        y_pred = self.sigmoid(self.linreg(X)).reshape(-1)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        y_pred = np.where(y_pred>0.5, 1, 0)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        acc = (y_pred == y).sum()/len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:51:53.846599Z",
     "start_time": "2020-10-03T12:51:41.186172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1339, train_score: 1.00, test_loss: 0.9609, test_score: 0.41\n",
      "\n",
      "Epoch [2/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1317, train_score: 1.00, test_loss: 0.9433, test_score: 0.41\n",
      "\n",
      "Epoch [3/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1296, train_score: 1.00, test_loss: 0.9258, test_score: 0.41\n",
      "\n",
      "Epoch [4/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1276, train_score: 1.00, test_loss: 0.9085, test_score: 0.41\n",
      "\n",
      "Epoch [5/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1257, train_score: 1.00, test_loss: 0.8913, test_score: 0.41\n",
      "\n",
      "Epoch [6/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1240, train_score: 1.00, test_loss: 0.8744, test_score: 0.41\n",
      "\n",
      "Epoch [7/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1224, train_score: 1.00, test_loss: 0.8576, test_score: 0.41\n",
      "\n",
      "Epoch [8/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1209, train_score: 1.00, test_loss: 0.8410, test_score: 0.41\n",
      "\n",
      "Epoch [9/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1195, train_score: 1.00, test_loss: 0.8247, test_score: 0.41\n",
      "\n",
      "Epoch [10/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1182, train_score: 1.00, test_loss: 0.8085, test_score: 0.41\n",
      "\n",
      "Epoch [11/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1171, train_score: 1.00, test_loss: 0.7926, test_score: 0.41\n",
      "\n",
      "Epoch [12/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1160, train_score: 1.00, test_loss: 0.7768, test_score: 0.41\n",
      "\n",
      "Epoch [13/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1150, train_score: 1.00, test_loss: 0.7613, test_score: 0.41\n",
      "\n",
      "Epoch [14/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1141, train_score: 1.00, test_loss: 0.7461, test_score: 0.41\n",
      "\n",
      "Epoch [15/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1133, train_score: 1.00, test_loss: 0.7310, test_score: 0.41\n",
      "\n",
      "Epoch [16/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1125, train_score: 1.00, test_loss: 0.7162, test_score: 0.41\n",
      "\n",
      "Epoch [17/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1119, train_score: 1.00, test_loss: 0.7017, test_score: 0.41\n",
      "\n",
      "Epoch [18/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1113, train_score: 1.00, test_loss: 0.6874, test_score: 0.41\n",
      "\n",
      "Epoch [19/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1107, train_score: 1.00, test_loss: 0.6733, test_score: 0.41\n",
      "\n",
      "Epoch [20/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1103, train_score: 1.00, test_loss: 0.6595, test_score: 0.41\n",
      "\n",
      "Epoch [21/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1099, train_score: 1.00, test_loss: 0.6460, test_score: 0.46\n",
      "\n",
      "Epoch [22/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1095, train_score: 1.00, test_loss: 0.6327, test_score: 0.46\n",
      "\n",
      "Epoch [23/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1093, train_score: 1.00, test_loss: 0.6198, test_score: 0.46\n",
      "\n",
      "Epoch [24/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1090, train_score: 1.00, test_loss: 0.6070, test_score: 0.49\n",
      "\n",
      "Epoch [25/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1088, train_score: 1.00, test_loss: 0.5946, test_score: 0.49\n",
      "\n",
      "Epoch [26/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1087, train_score: 1.00, test_loss: 0.5825, test_score: 0.49\n",
      "\n",
      "Epoch [27/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1086, train_score: 1.00, test_loss: 0.5706, test_score: 0.49\n",
      "\n",
      "Epoch [28/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1085, train_score: 1.00, test_loss: 0.5590, test_score: 0.51\n",
      "\n",
      "Epoch [29/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1085, train_score: 1.00, test_loss: 0.5477, test_score: 0.54\n",
      "\n",
      "Epoch [30/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1085, train_score: 1.00, test_loss: 0.5367, test_score: 0.56\n",
      "\n",
      "Epoch [31/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1085, train_score: 1.00, test_loss: 0.5259, test_score: 0.56\n",
      "\n",
      "Epoch [32/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1086, train_score: 1.00, test_loss: 0.5155, test_score: 0.62\n",
      "\n",
      "Epoch [33/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1087, train_score: 1.00, test_loss: 0.5053, test_score: 0.64\n",
      "\n",
      "Epoch [34/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1088, train_score: 1.00, test_loss: 0.4954, test_score: 0.64\n",
      "\n",
      "Epoch [35/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1089, train_score: 1.00, test_loss: 0.4857, test_score: 0.64\n",
      "\n",
      "Epoch [36/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1091, train_score: 1.00, test_loss: 0.4764, test_score: 0.69\n",
      "\n",
      "Epoch [37/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1092, train_score: 1.00, test_loss: 0.4672, test_score: 0.69\n",
      "\n",
      "Epoch [38/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1094, train_score: 1.00, test_loss: 0.4584, test_score: 0.72\n",
      "\n",
      "Epoch [39/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1096, train_score: 1.00, test_loss: 0.4498, test_score: 0.77\n",
      "\n",
      "Epoch [40/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1098, train_score: 1.00, test_loss: 0.4415, test_score: 0.77\n",
      "\n",
      "Epoch [41/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1100, train_score: 1.00, test_loss: 0.4334, test_score: 0.82\n",
      "\n",
      "Epoch [42/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1102, train_score: 1.00, test_loss: 0.4255, test_score: 0.82\n",
      "\n",
      "Epoch [43/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1104, train_score: 1.00, test_loss: 0.4179, test_score: 0.82\n",
      "\n",
      "Epoch [44/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1106, train_score: 1.00, test_loss: 0.4106, test_score: 0.85\n",
      "\n",
      "Epoch [45/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1108, train_score: 1.00, test_loss: 0.4034, test_score: 0.85\n",
      "\n",
      "Epoch [46/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1110, train_score: 1.00, test_loss: 0.3965, test_score: 0.85\n",
      "\n",
      "Epoch [47/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1112, train_score: 1.00, test_loss: 0.3898, test_score: 0.85\n",
      "\n",
      "Epoch [48/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1114, train_score: 1.00, test_loss: 0.3833, test_score: 0.95\n",
      "\n",
      "Epoch [49/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1116, train_score: 1.00, test_loss: 0.3770, test_score: 0.95\n",
      "\n",
      "Epoch [50/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1118, train_score: 1.00, test_loss: 0.3709, test_score: 0.95\n",
      "\n",
      "Epoch [51/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1120, train_score: 1.00, test_loss: 0.3649, test_score: 0.95\n",
      "\n",
      "Epoch [52/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1122, train_score: 1.00, test_loss: 0.3592, test_score: 0.95\n",
      "\n",
      "Epoch [53/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1124, train_score: 1.00, test_loss: 0.3537, test_score: 0.95\n",
      "\n",
      "Epoch [54/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1125, train_score: 1.00, test_loss: 0.3483, test_score: 0.95\n",
      "\n",
      "Epoch [55/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1127, train_score: 1.00, test_loss: 0.3431, test_score: 0.95\n",
      "\n",
      "Epoch [56/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1128, train_score: 1.00, test_loss: 0.3381, test_score: 0.95\n",
      "\n",
      "Epoch [57/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1130, train_score: 1.00, test_loss: 0.3332, test_score: 0.95\n",
      "\n",
      "Epoch [58/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1131, train_score: 1.00, test_loss: 0.3285, test_score: 0.95\n",
      "\n",
      "Epoch [59/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1132, train_score: 1.00, test_loss: 0.3239, test_score: 0.95\n",
      "\n",
      "Epoch [60/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1133, train_score: 1.00, test_loss: 0.3195, test_score: 0.95\n",
      "\n",
      "Epoch [61/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1134, train_score: 1.00, test_loss: 0.3152, test_score: 0.97\n",
      "\n",
      "Epoch [62/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1135, train_score: 1.00, test_loss: 0.3110, test_score: 0.97\n",
      "\n",
      "Epoch [63/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1135, train_score: 1.00, test_loss: 0.3070, test_score: 0.97\n",
      "\n",
      "Epoch [64/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1136, train_score: 1.00, test_loss: 0.3031, test_score: 0.97\n",
      "\n",
      "Epoch [65/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1136, train_score: 1.00, test_loss: 0.2993, test_score: 0.97\n",
      "\n",
      "Epoch [66/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1137, train_score: 1.00, test_loss: 0.2956, test_score: 0.97\n",
      "\n",
      "Epoch [67/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1137, train_score: 1.00, test_loss: 0.2920, test_score: 0.97\n",
      "\n",
      "Epoch [68/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1137, train_score: 1.00, test_loss: 0.2886, test_score: 0.97\n",
      "\n",
      "Epoch [69/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1137, train_score: 1.00, test_loss: 0.2852, test_score: 0.97\n",
      "\n",
      "Epoch [70/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1136, train_score: 1.00, test_loss: 0.2820, test_score: 0.97\n",
      "\n",
      "Epoch [71/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1136, train_score: 1.00, test_loss: 0.2788, test_score: 0.97\n",
      "\n",
      "Epoch [72/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1135, train_score: 1.00, test_loss: 0.2757, test_score: 0.97\n",
      "\n",
      "Epoch [73/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1135, train_score: 1.00, test_loss: 0.2728, test_score: 1.00\n",
      "\n",
      "Epoch [74/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1134, train_score: 1.00, test_loss: 0.2699, test_score: 1.00\n",
      "\n",
      "Epoch [75/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1133, train_score: 1.00, test_loss: 0.2671, test_score: 1.00\n",
      "\n",
      "Epoch [76/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1132, train_score: 1.00, test_loss: 0.2643, test_score: 1.00\n",
      "\n",
      "Epoch [77/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1131, train_score: 1.00, test_loss: 0.2617, test_score: 1.00\n",
      "\n",
      "Epoch [78/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1130, train_score: 1.00, test_loss: 0.2591, test_score: 0.97\n",
      "\n",
      "Epoch [79/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1129, train_score: 1.00, test_loss: 0.2566, test_score: 0.97\n",
      "\n",
      "Epoch [80/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1127, train_score: 1.00, test_loss: 0.2542, test_score: 0.97\n",
      "\n",
      "Epoch [81/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1125, train_score: 1.00, test_loss: 0.2518, test_score: 0.97\n",
      "\n",
      "Epoch [82/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1124, train_score: 1.00, test_loss: 0.2495, test_score: 0.97\n",
      "\n",
      "Epoch [83/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1122, train_score: 1.00, test_loss: 0.2473, test_score: 0.97\n",
      "\n",
      "Epoch [84/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1120, train_score: 1.00, test_loss: 0.2451, test_score: 0.97\n",
      "\n",
      "Epoch [85/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1118, train_score: 1.00, test_loss: 0.2429, test_score: 0.97\n",
      "\n",
      "Epoch [86/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1116, train_score: 1.00, test_loss: 0.2409, test_score: 0.97\n",
      "\n",
      "Epoch [87/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1114, train_score: 1.00, test_loss: 0.2389, test_score: 0.97\n",
      "\n",
      "Epoch [88/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1111, train_score: 1.00, test_loss: 0.2369, test_score: 0.97\n",
      "\n",
      "Epoch [89/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1109, train_score: 1.00, test_loss: 0.2350, test_score: 0.97\n",
      "\n",
      "Epoch [90/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1106, train_score: 1.00, test_loss: 0.2331, test_score: 0.97\n",
      "\n",
      "Epoch [91/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1104, train_score: 1.00, test_loss: 0.2313, test_score: 0.97\n",
      "\n",
      "Epoch [92/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1101, train_score: 1.00, test_loss: 0.2295, test_score: 0.97\n",
      "\n",
      "Epoch [93/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1098, train_score: 1.00, test_loss: 0.2277, test_score: 0.97\n",
      "\n",
      "Epoch [94/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1095, train_score: 1.00, test_loss: 0.2260, test_score: 0.97\n",
      "\n",
      "Epoch [95/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1093, train_score: 1.00, test_loss: 0.2244, test_score: 0.97\n",
      "\n",
      "Epoch [96/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1090, train_score: 1.00, test_loss: 0.2228, test_score: 0.97\n",
      "\n",
      "Epoch [97/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1086, train_score: 1.00, test_loss: 0.2212, test_score: 0.97\n",
      "\n",
      "Epoch [98/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1083, train_score: 1.00, test_loss: 0.2196, test_score: 0.97\n",
      "\n",
      "Epoch [99/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1080, train_score: 1.00, test_loss: 0.2181, test_score: 0.97\n",
      "\n",
      "Epoch [100/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1077, train_score: 1.00, test_loss: 0.2166, test_score: 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    \"model\": LogisticModel(alpha=0.02, weight_decay=0),\n",
    "    \"epoch_num\": 100,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "process_bar = sqdm()\n",
    "def train(model, epoch_num, batch_size):\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, xtrain, ytrain):\n",
    "            model.fit(xdata, ydata)\n",
    "            \n",
    "            # train\n",
    "            train_pred = model.predict_prob(xdata)\n",
    "            train_loss = round(model.entropy_loss(train_pred, ydata.reshape(train_pred.shape)), 5)\n",
    "            train_acc = model.score(xdata, ydata)\n",
    "            \n",
    "            # test\n",
    "            test_pred = model.predict_prob(xtest)\n",
    "            test_loss = round(model.entropy_loss(test_pred, ytest.reshape(test_pred.shape)), 5)\n",
    "            test_acc = model.score(xtest, ytest)\n",
    "\n",
    "            process_bar.show_process(len(ytrain), batch_size, train_loss=train_loss, \n",
    "                                    test_loss=test_loss, train_score=train_acc, test_score=test_acc)\n",
    "            \n",
    "        print(\"\\n\")\n",
    "    return model\n",
    "    \n",
    "model = train(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:51:53.859130Z",
     "start_time": "2020-10-03T12:51:53.848339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集结果\n",
    "model.predict(iris_data[:, :4])\n",
    "model.score(iris_data[:, :4], iris_data[:, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax可以看成是一个多项的logistic，实际上softmax是一种条件最大熵模型\n",
    "- 对于某个样本属于第c类的概率为：\n",
    "    - $P(y=c|x) = \\frac{exp(w^T_cx)}{\\sum_{c'=1}^{C}exp(w^T_{c'}x)}$\n",
    "    - 决策函数为\n",
    "        - $\\hat y = \\underset{c}{arg min} \\  P(y=c|x) = \\underset{c}{arg min}\\  w^T_cx$\n",
    "- 损失函数：$L = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^{C} y_c^{(n)}log(\\hat y_c^{(n)})= - \\sum_{n=1}^N  (y^{(n)})^Tlog(\\hat y^{(n)})$\n",
    "- 梯度：\n",
    "    - $\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{n=1}^N x^{(n)}(\\hat y^{(n)} - y^{(n)})^T$\n",
    "    - $\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^N (\\hat y^{(n)} - y^{(n)})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy版"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是先以鸢尾花的例子来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:52:08.336086Z",
     "start_time": "2020-10-03T12:52:08.323390Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理数据集\n",
    "iris = load_iris()\n",
    "iris_data = np.hstack((iris.data, np.expand_dims(iris.target, 1)))\n",
    "xtrain, ytrain, xtest, ytest = bootstrap(iris_data[:, :4], iris_data[:, 4])\n",
    "\n",
    "# 处理标签--> (0 --> [1, 0, 0])\n",
    "label_dict = {\n",
    "    0: [1, 0, 0],\n",
    "    1: [0, 1, 0],\n",
    "    2: [0, 0, 1]\n",
    "}\n",
    "\n",
    "data = np.array(list(map(lambda x: label_dict[x], iris_data[:, 4])))\n",
    "ytrain = np.array(list(map(lambda x: label_dict[x], list(ytrain))))\n",
    "ytest = np.array(list(map(lambda x: label_dict[x], list(ytest))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:52:08.633342Z",
     "start_time": "2020-10-03T12:52:08.612333Z"
    }
   },
   "outputs": [],
   "source": [
    "class SoftmaxModel(object):\n",
    "    \"\"\"实现softmax\"\"\"\n",
    "    def __init__(self, fea_num, cate_num, alpha=0.01, weight_decay=0):\n",
    "        self.w = np.zeros([fea_num, cate_num])\n",
    "        self.b = np.zeros(cate_num)\n",
    "        self.fea_num = fea_num\n",
    "        self.cate_num = cate_num\n",
    "        self.alpha = alpha\n",
    "        self.weight_decay = weight_decay\n",
    "        self.count = 0\n",
    "        \n",
    "    def linreg(self, X):\n",
    "        return X@self.w + self.b\n",
    "    \n",
    "    def softmax(self, y):\n",
    "        return np.exp(y)/np.expand_dims(np.exp(y).sum(axis = 1), 1)\n",
    "    \n",
    "    def entropy_loss(self, y_pred, y):\n",
    "        loss = -(y*np.log(y_pred)).sum()/len(y)\n",
    "        return loss\n",
    "    \n",
    "    def cal_grad(self, X, y_diff):\n",
    "        result = np.zeros([self.fea_num, self.cate_num])\n",
    "        for i in range(len(X)):\n",
    "            result += np.outer(X.T[:, i], y_diff[i, :])\n",
    "        return result / len(X)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # predict\n",
    "        y_pred = self.predict_prob(X)\n",
    "        \n",
    "        # update_grad\n",
    "        dw = self.cal_grad(X, (y_pred-y)) + self.weight_decay * self.w\n",
    "        db = (y_pred-y).sum(axis=0)/len(y) + self.weight_decay * self.b\n",
    "        self.w -= self.alpha * dw\n",
    "        self.b -= self.alpha * db\n",
    "        self.count += 1\n",
    "        \n",
    "            \n",
    "    def predict_prob(self, X):\n",
    "        y_pred = self.softmax(self.linreg(X))\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        pred_index = np.argmax(y_pred, axis=1)\n",
    "        return pred_index\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        pred_index = np.argmax(y_pred, axis=1)\n",
    "        label_index = np.argmax(y, axis = 1)\n",
    "        acc = (pred_index == label_index).sum()/len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:52:32.449519Z",
     "start_time": "2020-10-03T12:52:10.952149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2549, train_score: 1.00, test_loss: 0.6858, test_score: 0.63\n",
      "\n",
      "Epoch [2/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2287, train_score: 1.00, test_loss: 0.5735, test_score: 0.63\n",
      "\n",
      "Epoch [3/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2147, train_score: 1.00, test_loss: 0.5132, test_score: 0.63\n",
      "\n",
      "Epoch [4/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2031, train_score: 1.00, test_loss: 0.4707, test_score: 0.65\n",
      "\n",
      "Epoch [5/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1925, train_score: 1.00, test_loss: 0.4373, test_score: 0.73\n",
      "\n",
      "Epoch [6/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1824, train_score: 1.00, test_loss: 0.4097, test_score: 0.78\n",
      "\n",
      "Epoch [7/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1727, train_score: 1.00, test_loss: 0.3862, test_score: 0.80\n",
      "\n",
      "Epoch [8/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1636, train_score: 1.00, test_loss: 0.3659, test_score: 0.84\n",
      "\n",
      "Epoch [9/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1549, train_score: 1.00, test_loss: 0.3480, test_score: 0.90\n",
      "\n",
      "Epoch [10/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1468, train_score: 1.00, test_loss: 0.3322, test_score: 0.90\n",
      "\n",
      "Epoch [11/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1392, train_score: 1.00, test_loss: 0.3181, test_score: 0.92\n",
      "\n",
      "Epoch [12/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1321, train_score: 1.00, test_loss: 0.3055, test_score: 0.94\n",
      "\n",
      "Epoch [13/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1255, train_score: 1.00, test_loss: 0.2941, test_score: 0.94\n",
      "\n",
      "Epoch [14/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1194, train_score: 1.00, test_loss: 0.2837, test_score: 0.94\n",
      "\n",
      "Epoch [15/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1137, train_score: 1.00, test_loss: 0.2742, test_score: 0.94\n",
      "\n",
      "Epoch [16/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1085, train_score: 1.00, test_loss: 0.2656, test_score: 0.94\n",
      "\n",
      "Epoch [17/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1036, train_score: 1.00, test_loss: 0.2576, test_score: 0.96\n",
      "\n",
      "Epoch [18/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0991, train_score: 1.00, test_loss: 0.2503, test_score: 0.96\n",
      "\n",
      "Epoch [19/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0949, train_score: 1.00, test_loss: 0.2435, test_score: 0.96\n",
      "\n",
      "Epoch [20/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0910, train_score: 1.00, test_loss: 0.2371, test_score: 0.96\n",
      "\n",
      "Epoch [21/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0874, train_score: 1.00, test_loss: 0.2312, test_score: 0.96\n",
      "\n",
      "Epoch [22/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0840, train_score: 1.00, test_loss: 0.2257, test_score: 0.96\n",
      "\n",
      "Epoch [23/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0809, train_score: 1.00, test_loss: 0.2206, test_score: 0.96\n",
      "\n",
      "Epoch [24/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0780, train_score: 1.00, test_loss: 0.2158, test_score: 0.96\n",
      "\n",
      "Epoch [25/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0752, train_score: 1.00, test_loss: 0.2112, test_score: 0.98\n",
      "\n",
      "Epoch [26/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0726, train_score: 1.00, test_loss: 0.2070, test_score: 0.98\n",
      "\n",
      "Epoch [27/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0702, train_score: 1.00, test_loss: 0.2029, test_score: 0.98\n",
      "\n",
      "Epoch [28/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0680, train_score: 1.00, test_loss: 0.1991, test_score: 0.98\n",
      "\n",
      "Epoch [29/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0659, train_score: 1.00, test_loss: 0.1956, test_score: 0.98\n",
      "\n",
      "Epoch [30/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0639, train_score: 1.00, test_loss: 0.1921, test_score: 0.98\n",
      "\n",
      "Epoch [31/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0620, train_score: 1.00, test_loss: 0.1889, test_score: 0.98\n",
      "\n",
      "Epoch [32/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0602, train_score: 1.00, test_loss: 0.1859, test_score: 0.98\n",
      "\n",
      "Epoch [33/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0585, train_score: 1.00, test_loss: 0.1829, test_score: 0.98\n",
      "\n",
      "Epoch [34/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0569, train_score: 1.00, test_loss: 0.1802, test_score: 0.98\n",
      "\n",
      "Epoch [35/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0554, train_score: 1.00, test_loss: 0.1775, test_score: 0.98\n",
      "\n",
      "Epoch [36/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0540, train_score: 1.00, test_loss: 0.1750, test_score: 0.98\n",
      "\n",
      "Epoch [37/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0526, train_score: 1.00, test_loss: 0.1726, test_score: 0.98\n",
      "\n",
      "Epoch [38/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0513, train_score: 1.00, test_loss: 0.1703, test_score: 0.98\n",
      "\n",
      "Epoch [39/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0501, train_score: 1.00, test_loss: 0.1681, test_score: 0.98\n",
      "\n",
      "Epoch [40/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0489, train_score: 1.00, test_loss: 0.1661, test_score: 0.98\n",
      "\n",
      "Epoch [41/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0478, train_score: 1.00, test_loss: 0.1641, test_score: 0.98\n",
      "\n",
      "Epoch [42/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0467, train_score: 1.00, test_loss: 0.1621, test_score: 0.98\n",
      "\n",
      "Epoch [43/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0457, train_score: 1.00, test_loss: 0.1603, test_score: 0.98\n",
      "\n",
      "Epoch [44/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0447, train_score: 1.00, test_loss: 0.1585, test_score: 0.98\n",
      "\n",
      "Epoch [45/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0437, train_score: 1.00, test_loss: 0.1568, test_score: 0.98\n",
      "\n",
      "Epoch [46/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0428, train_score: 1.00, test_loss: 0.1552, test_score: 0.98\n",
      "\n",
      "Epoch [47/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0419, train_score: 1.00, test_loss: 0.1537, test_score: 0.98\n",
      "\n",
      "Epoch [48/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0411, train_score: 1.00, test_loss: 0.1522, test_score: 0.98\n",
      "\n",
      "Epoch [49/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0403, train_score: 1.00, test_loss: 0.1507, test_score: 0.98\n",
      "\n",
      "Epoch [50/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0395, train_score: 1.00, test_loss: 0.1494, test_score: 0.98\n",
      "\n",
      "Epoch [51/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0388, train_score: 1.00, test_loss: 0.1480, test_score: 0.98\n",
      "\n",
      "Epoch [52/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0380, train_score: 1.00, test_loss: 0.1467, test_score: 0.98\n",
      "\n",
      "Epoch [53/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0373, train_score: 1.00, test_loss: 0.1455, test_score: 0.98\n",
      "\n",
      "Epoch [54/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0367, train_score: 1.00, test_loss: 0.1443, test_score: 0.98\n",
      "\n",
      "Epoch [55/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0360, train_score: 1.00, test_loss: 0.1432, test_score: 0.98\n",
      "\n",
      "Epoch [56/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0354, train_score: 1.00, test_loss: 0.1421, test_score: 0.98\n",
      "\n",
      "Epoch [57/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0348, train_score: 1.00, test_loss: 0.1410, test_score: 0.98\n",
      "\n",
      "Epoch [58/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0342, train_score: 1.00, test_loss: 0.1400, test_score: 0.98\n",
      "\n",
      "Epoch [59/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0336, train_score: 1.00, test_loss: 0.1390, test_score: 0.98\n",
      "\n",
      "Epoch [60/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0331, train_score: 1.00, test_loss: 0.1380, test_score: 0.98\n",
      "\n",
      "Epoch [61/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0325, train_score: 1.00, test_loss: 0.1371, test_score: 1.00\n",
      "\n",
      "Epoch [62/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0320, train_score: 1.00, test_loss: 0.1362, test_score: 1.00\n",
      "\n",
      "Epoch [63/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0315, train_score: 1.00, test_loss: 0.1353, test_score: 0.98\n",
      "\n",
      "Epoch [64/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0310, train_score: 1.00, test_loss: 0.1345, test_score: 0.98\n",
      "\n",
      "Epoch [65/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0305, train_score: 1.00, test_loss: 0.1337, test_score: 0.98\n",
      "\n",
      "Epoch [66/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0300, train_score: 1.00, test_loss: 0.1329, test_score: 0.98\n",
      "\n",
      "Epoch [67/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0296, train_score: 1.00, test_loss: 0.1321, test_score: 0.98\n",
      "\n",
      "Epoch [68/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0291, train_score: 1.00, test_loss: 0.1314, test_score: 0.98\n",
      "\n",
      "Epoch [69/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0287, train_score: 1.00, test_loss: 0.1307, test_score: 0.98\n",
      "\n",
      "Epoch [70/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0283, train_score: 1.00, test_loss: 0.1300, test_score: 0.98\n",
      "\n",
      "Epoch [71/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0279, train_score: 1.00, test_loss: 0.1293, test_score: 0.98\n",
      "\n",
      "Epoch [72/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0275, train_score: 1.00, test_loss: 0.1287, test_score: 0.98\n",
      "\n",
      "Epoch [73/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0271, train_score: 1.00, test_loss: 0.1280, test_score: 0.98\n",
      "\n",
      "Epoch [74/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0267, train_score: 1.00, test_loss: 0.1274, test_score: 0.98\n",
      "\n",
      "Epoch [75/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0264, train_score: 1.00, test_loss: 0.1268, test_score: 0.98\n",
      "\n",
      "Epoch [76/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0260, train_score: 1.00, test_loss: 0.1262, test_score: 0.98\n",
      "\n",
      "Epoch [77/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0256, train_score: 1.00, test_loss: 0.1257, test_score: 0.98\n",
      "\n",
      "Epoch [78/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0253, train_score: 1.00, test_loss: 0.1251, test_score: 0.98\n",
      "\n",
      "Epoch [79/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0250, train_score: 1.00, test_loss: 0.1246, test_score: 0.98\n",
      "\n",
      "Epoch [80/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0246, train_score: 1.00, test_loss: 0.1241, test_score: 0.98\n",
      "\n",
      "Epoch [81/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0243, train_score: 1.00, test_loss: 0.1236, test_score: 0.98\n",
      "\n",
      "Epoch [82/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0240, train_score: 1.00, test_loss: 0.1231, test_score: 0.98\n",
      "\n",
      "Epoch [83/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0237, train_score: 1.00, test_loss: 0.1226, test_score: 0.98\n",
      "\n",
      "Epoch [84/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0234, train_score: 1.00, test_loss: 0.1222, test_score: 0.98\n",
      "\n",
      "Epoch [85/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0231, train_score: 1.00, test_loss: 0.1217, test_score: 0.96\n",
      "\n",
      "Epoch [86/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0228, train_score: 1.00, test_loss: 0.1213, test_score: 0.96\n",
      "\n",
      "Epoch [87/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0225, train_score: 1.00, test_loss: 0.1209, test_score: 0.96\n",
      "\n",
      "Epoch [88/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0222, train_score: 1.00, test_loss: 0.1205, test_score: 0.96\n",
      "\n",
      "Epoch [89/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0220, train_score: 1.00, test_loss: 0.1201, test_score: 0.96\n",
      "\n",
      "Epoch [90/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0217, train_score: 1.00, test_loss: 0.1197, test_score: 0.96\n",
      "\n",
      "Epoch [91/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0214, train_score: 1.00, test_loss: 0.1193, test_score: 0.96\n",
      "\n",
      "Epoch [92/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0212, train_score: 1.00, test_loss: 0.1190, test_score: 0.96\n",
      "\n",
      "Epoch [93/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0209, train_score: 1.00, test_loss: 0.1186, test_score: 0.96\n",
      "\n",
      "Epoch [94/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0207, train_score: 1.00, test_loss: 0.1182, test_score: 0.96\n",
      "\n",
      "Epoch [95/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0205, train_score: 1.00, test_loss: 0.1179, test_score: 0.96\n",
      "\n",
      "Epoch [96/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0202, train_score: 1.00, test_loss: 0.1176, test_score: 0.96\n",
      "\n",
      "Epoch [97/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0200, train_score: 1.00, test_loss: 0.1172, test_score: 0.96\n",
      "\n",
      "Epoch [98/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0198, train_score: 1.00, test_loss: 0.1169, test_score: 0.96\n",
      "\n",
      "Epoch [99/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0195, train_score: 1.00, test_loss: 0.1166, test_score: 0.96\n",
      "\n",
      "Epoch [100/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0193, train_score: 1.00, test_loss: 0.1163, test_score: 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    \"model\": SoftmaxModel(fea_num=4, cate_num=3, alpha=0.01, weight_decay=0),\n",
    "    \"epoch_num\": 100,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "process_bar = sqdm()\n",
    "def train(model, epoch_num, batch_size):\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, xtrain, ytrain):\n",
    "            model.fit(xdata, ydata)\n",
    "            \n",
    "            # train\n",
    "            train_pred = model.predict_prob(xdata)\n",
    "            train_loss = model.entropy_loss(train_pred, ydata.reshape(train_pred.shape))\n",
    "            train_acc = model.score(xdata, ydata)\n",
    "            \n",
    "            # test\n",
    "            test_pred = model.predict_prob(xtest)\n",
    "            test_loss = model.entropy_loss(test_pred, ytest.reshape(test_pred.shape))\n",
    "            test_acc = model.score(xtest, ytest)\n",
    "\n",
    "            process_bar.show_process(len(ytrain), batch_size, train_loss=train_loss, \n",
    "                                    test_loss=test_loss, train_score=train_acc, test_score=test_acc)\n",
    "            \n",
    "        print(\"\\n\")\n",
    "    return model\n",
    "    \n",
    "model = train(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T12:52:35.539814Z",
     "start_time": "2020-10-03T12:52:35.534299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集结果\n",
    "model.predict(iris_data[:, :4])\n",
    "model.score(iris_data[:, :4], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
