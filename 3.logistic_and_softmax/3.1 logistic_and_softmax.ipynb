{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T07:46:07.613964Z",
     "start_time": "2020-10-03T07:46:07.579957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic是一种处理2分类的线性模型，其中，$x \\in R^D, y \\in {0, 1}$，我们希望找到一个非线性函数$g()$，使得$R^D --> {0, 1}$，以此来预测后验概率$P(Y=1|X)$\n",
    "- 模型：$P(y=1|X) = \\frac{1}{1+e^{-w^Tx}}$，$P(y=0|X) = \\frac{e^{-w^Tx}}{1+e^{-w^Tx}}$\n",
    "- 损失函数：$L = -\\frac{1}{N} \\sum_{n=1}^N y^{(n)} log(\\hat y^{(n)}) + (1 - y^{(n)}) log(1 - \\hat y^{(n)})$\n",
    "- 梯度：\n",
    "    - $\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{n=1}^Nx^{n} (\\hat y^{(n)} - y^{(n)})$\n",
    "    - $\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^N (\\hat y^{(n)} - y^{(n)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下使用sklearn的鸢尾花数据集来实现logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T11:05:40.742804Z",
     "start_time": "2020-10-03T11:05:39.890768Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../d2l_func/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import data_iter\n",
    "from sqdm import sqdm\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T11:06:16.710875Z",
     "start_time": "2020-10-03T11:06:16.695689Z"
    }
   },
   "outputs": [],
   "source": [
    "def bootstrap(x, y):\n",
    "    \"\"\"自助法实现\"\"\"\n",
    "    data_num = len(y)\n",
    "    \n",
    "    # 训练集的index和测试集的index\n",
    "    batch_index = np.random.choice(data_num, size=data_num, replace=True)\n",
    "    out_index = np.array(list(set(range(data_num)).difference(set(batch_index))))\n",
    "    \n",
    "    # 训练集\n",
    "    xtrain, ytrain = x[batch_index], y[batch_index]\n",
    "    # 测试集\n",
    "    xtest, ytest = x[out_index], y[out_index]\n",
    "    \n",
    "    return xtrain, ytrain, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:52:49.191252Z",
     "start_time": "2020-10-03T09:52:49.185432Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理数据集\n",
    "iris = load_iris()\n",
    "iris_data = np.hstack((iris.data, np.expand_dims(iris.target, 1)))\n",
    "iris_data = iris_data[iris.target < 2]\n",
    "xtrain, ytrain, xtest, ytest = bootstrap(iris_data[:, :4], iris_data[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:52:49.407018Z",
     "start_time": "2020-10-03T09:52:49.396455Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticModel(object):\n",
    "    def __init__(self, alpha=0.01, weight_decay=0):\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "        self.alpha = alpha\n",
    "        self.weight_decay = weight_decay\n",
    "        self.count = 0\n",
    "        \n",
    "    def linreg(self, X):\n",
    "        return X@self.w + self.b\n",
    "    \n",
    "    def sigmoid(self, y):\n",
    "        return 1 / (1 + np.exp(-y))\n",
    "    \n",
    "    def entropy_loss(self, y_pred, y):\n",
    "        y_pred = np.where(y==0, 1-y_pred, y_pred)\n",
    "        loss = -(np.log(y_pred).sum())/len(y)\n",
    "        return loss\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        fea_num = int(X.size / len(y))\n",
    "        if self.count == 0:\n",
    "            self.w = np.zeros(fea_num)\n",
    "        \n",
    "        # reshape X and y\n",
    "        X = X.reshape(len(y), fea_num)\n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        # predict\n",
    "        y_pred = self.predict_prob(X)\n",
    "        \n",
    "        # update grad\n",
    "        dw = (X.T@(y_pred - y)).sum()/len(y) + self.weight_decay*self.w\n",
    "        db = (y_pred - y).sum()/len(y) + self.weight_decay*self.b\n",
    "        self.w -= self.alpha * dw\n",
    "        self.b -= self.alpha * db\n",
    "        self.count += 1\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        y_pred = self.sigmoid(self.linreg(X)).reshape(-1)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        y_pred = np.where(y_pred>0.5, 1, 0)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        acc = (y_pred == y).sum()/len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:53:01.140654Z",
     "start_time": "2020-10-03T09:52:49.681807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1455, train_score: 1.00, test_loss: 0.9576, test_score: 0.44\n",
      "\n",
      "Epoch [2/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1454, train_score: 1.00, test_loss: 0.9418, test_score: 0.44\n",
      "\n",
      "Epoch [3/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1452, train_score: 1.00, test_loss: 0.9263, test_score: 0.44\n",
      "\n",
      "Epoch [4/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1451, train_score: 1.00, test_loss: 0.9111, test_score: 0.44\n",
      "\n",
      "Epoch [5/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1449, train_score: 1.00, test_loss: 0.8963, test_score: 0.44\n",
      "\n",
      "Epoch [6/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1448, train_score: 1.00, test_loss: 0.8817, test_score: 0.44\n",
      "\n",
      "Epoch [7/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1446, train_score: 1.00, test_loss: 0.8675, test_score: 0.44\n",
      "\n",
      "Epoch [8/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1445, train_score: 1.00, test_loss: 0.8535, test_score: 0.44\n",
      "\n",
      "Epoch [9/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1443, train_score: 1.00, test_loss: 0.8399, test_score: 0.44\n",
      "\n",
      "Epoch [10/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1441, train_score: 1.00, test_loss: 0.8265, test_score: 0.44\n",
      "\n",
      "Epoch [11/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1440, train_score: 1.00, test_loss: 0.8135, test_score: 0.44\n",
      "\n",
      "Epoch [12/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1438, train_score: 1.00, test_loss: 0.8007, test_score: 0.44\n",
      "\n",
      "Epoch [13/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1436, train_score: 1.00, test_loss: 0.7882, test_score: 0.44\n",
      "\n",
      "Epoch [14/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1434, train_score: 1.00, test_loss: 0.7760, test_score: 0.44\n",
      "\n",
      "Epoch [15/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1432, train_score: 1.00, test_loss: 0.7641, test_score: 0.44\n",
      "\n",
      "Epoch [16/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1431, train_score: 1.00, test_loss: 0.7525, test_score: 0.44\n",
      "\n",
      "Epoch [17/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1429, train_score: 1.00, test_loss: 0.7411, test_score: 0.44\n",
      "\n",
      "Epoch [18/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1427, train_score: 1.00, test_loss: 0.7300, test_score: 0.44\n",
      "\n",
      "Epoch [19/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1424, train_score: 1.00, test_loss: 0.7191, test_score: 0.44\n",
      "\n",
      "Epoch [20/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1422, train_score: 1.00, test_loss: 0.7085, test_score: 0.44\n",
      "\n",
      "Epoch [21/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1420, train_score: 1.00, test_loss: 0.6982, test_score: 0.44\n",
      "\n",
      "Epoch [22/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1418, train_score: 1.00, test_loss: 0.6881, test_score: 0.44\n",
      "\n",
      "Epoch [23/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1416, train_score: 1.00, test_loss: 0.6782, test_score: 0.44\n",
      "\n",
      "Epoch [24/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1413, train_score: 1.00, test_loss: 0.6686, test_score: 0.44\n",
      "\n",
      "Epoch [25/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1411, train_score: 1.00, test_loss: 0.6592, test_score: 0.46\n",
      "\n",
      "Epoch [26/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1408, train_score: 1.00, test_loss: 0.6501, test_score: 0.46\n",
      "\n",
      "Epoch [27/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1406, train_score: 1.00, test_loss: 0.6411, test_score: 0.46\n",
      "\n",
      "Epoch [28/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1403, train_score: 1.00, test_loss: 0.6324, test_score: 0.46\n",
      "\n",
      "Epoch [29/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1400, train_score: 1.00, test_loss: 0.6239, test_score: 0.49\n",
      "\n",
      "Epoch [30/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1398, train_score: 1.00, test_loss: 0.6156, test_score: 0.49\n",
      "\n",
      "Epoch [31/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1395, train_score: 1.00, test_loss: 0.6075, test_score: 0.49\n",
      "\n",
      "Epoch [32/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1392, train_score: 1.00, test_loss: 0.5996, test_score: 0.49\n",
      "\n",
      "Epoch [33/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1389, train_score: 1.00, test_loss: 0.5919, test_score: 0.49\n",
      "\n",
      "Epoch [34/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1386, train_score: 1.00, test_loss: 0.5844, test_score: 0.51\n",
      "\n",
      "Epoch [35/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1383, train_score: 1.00, test_loss: 0.5771, test_score: 0.51\n",
      "\n",
      "Epoch [36/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1379, train_score: 1.00, test_loss: 0.5700, test_score: 0.54\n",
      "\n",
      "Epoch [37/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1376, train_score: 1.00, test_loss: 0.5630, test_score: 0.54\n",
      "\n",
      "Epoch [38/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1373, train_score: 1.00, test_loss: 0.5562, test_score: 0.54\n",
      "\n",
      "Epoch [39/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1369, train_score: 1.00, test_loss: 0.5496, test_score: 0.56\n",
      "\n",
      "Epoch [40/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1366, train_score: 1.00, test_loss: 0.5431, test_score: 0.56\n",
      "\n",
      "Epoch [41/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1362, train_score: 1.00, test_loss: 0.5368, test_score: 0.62\n",
      "\n",
      "Epoch [42/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1359, train_score: 1.00, test_loss: 0.5306, test_score: 0.62\n",
      "\n",
      "Epoch [43/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1355, train_score: 1.00, test_loss: 0.5246, test_score: 0.62\n",
      "\n",
      "Epoch [44/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1352, train_score: 1.00, test_loss: 0.5188, test_score: 0.62\n",
      "\n",
      "Epoch [45/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1348, train_score: 1.00, test_loss: 0.5130, test_score: 0.62\n",
      "\n",
      "Epoch [46/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1344, train_score: 1.00, test_loss: 0.5074, test_score: 0.64\n",
      "\n",
      "Epoch [47/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1341, train_score: 1.00, test_loss: 0.5020, test_score: 0.64\n",
      "\n",
      "Epoch [48/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1337, train_score: 1.00, test_loss: 0.4966, test_score: 0.64\n",
      "\n",
      "Epoch [49/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1333, train_score: 1.00, test_loss: 0.4914, test_score: 0.64\n",
      "\n",
      "Epoch [50/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1329, train_score: 1.00, test_loss: 0.4863, test_score: 0.64\n",
      "\n",
      "Epoch [51/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1325, train_score: 1.00, test_loss: 0.4813, test_score: 0.67\n",
      "\n",
      "Epoch [52/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1321, train_score: 1.00, test_loss: 0.4765, test_score: 0.67\n",
      "\n",
      "Epoch [53/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1317, train_score: 1.00, test_loss: 0.4717, test_score: 0.67\n",
      "\n",
      "Epoch [54/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1313, train_score: 1.00, test_loss: 0.4671, test_score: 0.72\n",
      "\n",
      "Epoch [55/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1309, train_score: 1.00, test_loss: 0.4625, test_score: 0.72\n",
      "\n",
      "Epoch [56/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1305, train_score: 1.00, test_loss: 0.4581, test_score: 0.72\n",
      "\n",
      "Epoch [57/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1301, train_score: 1.00, test_loss: 0.4537, test_score: 0.72\n",
      "\n",
      "Epoch [58/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1298, train_score: 1.00, test_loss: 0.4494, test_score: 0.72\n",
      "\n",
      "Epoch [59/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1293, train_score: 1.00, test_loss: 0.4453, test_score: 0.72\n",
      "\n",
      "Epoch [60/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1290, train_score: 1.00, test_loss: 0.4412, test_score: 0.72\n",
      "\n",
      "Epoch [61/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1285, train_score: 1.00, test_loss: 0.4372, test_score: 0.72\n",
      "\n",
      "Epoch [62/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1281, train_score: 1.00, test_loss: 0.4333, test_score: 0.74\n",
      "\n",
      "Epoch [63/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1278, train_score: 1.00, test_loss: 0.4294, test_score: 0.74\n",
      "\n",
      "Epoch [64/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1274, train_score: 1.00, test_loss: 0.4257, test_score: 0.74\n",
      "\n",
      "Epoch [65/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1270, train_score: 1.00, test_loss: 0.4220, test_score: 0.74\n",
      "\n",
      "Epoch [66/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1266, train_score: 1.00, test_loss: 0.4184, test_score: 0.77\n",
      "\n",
      "Epoch [67/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1262, train_score: 1.00, test_loss: 0.4149, test_score: 0.77\n",
      "\n",
      "Epoch [68/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1258, train_score: 1.00, test_loss: 0.4114, test_score: 0.77\n",
      "\n",
      "Epoch [69/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1254, train_score: 1.00, test_loss: 0.4080, test_score: 0.77\n",
      "\n",
      "Epoch [70/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1250, train_score: 1.00, test_loss: 0.4047, test_score: 0.77\n",
      "\n",
      "Epoch [71/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1246, train_score: 1.00, test_loss: 0.4014, test_score: 0.77\n",
      "\n",
      "Epoch [72/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1242, train_score: 1.00, test_loss: 0.3982, test_score: 0.77\n",
      "\n",
      "Epoch [73/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1239, train_score: 1.00, test_loss: 0.3951, test_score: 0.77\n",
      "\n",
      "Epoch [74/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1235, train_score: 1.00, test_loss: 0.3920, test_score: 0.77\n",
      "\n",
      "Epoch [75/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1231, train_score: 1.00, test_loss: 0.3890, test_score: 0.77\n",
      "\n",
      "Epoch [76/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1228, train_score: 1.00, test_loss: 0.3860, test_score: 0.77\n",
      "\n",
      "Epoch [77/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1224, train_score: 1.00, test_loss: 0.3831, test_score: 0.87\n",
      "\n",
      "Epoch [78/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1220, train_score: 1.00, test_loss: 0.3802, test_score: 0.87\n",
      "\n",
      "Epoch [79/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1217, train_score: 1.00, test_loss: 0.3774, test_score: 0.87\n",
      "\n",
      "Epoch [80/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1213, train_score: 1.00, test_loss: 0.3746, test_score: 0.87\n",
      "\n",
      "Epoch [81/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1210, train_score: 1.00, test_loss: 0.3719, test_score: 0.87\n",
      "\n",
      "Epoch [82/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1206, train_score: 1.00, test_loss: 0.3692, test_score: 0.87\n",
      "\n",
      "Epoch [83/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1202, train_score: 1.00, test_loss: 0.3665, test_score: 0.87\n",
      "\n",
      "Epoch [84/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1199, train_score: 1.00, test_loss: 0.3640, test_score: 0.90\n",
      "\n",
      "Epoch [85/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1196, train_score: 1.00, test_loss: 0.3614, test_score: 0.90\n",
      "\n",
      "Epoch [86/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1192, train_score: 1.00, test_loss: 0.3589, test_score: 0.90\n",
      "\n",
      "Epoch [87/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1189, train_score: 1.00, test_loss: 0.3565, test_score: 0.90\n",
      "\n",
      "Epoch [88/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1186, train_score: 1.00, test_loss: 0.3540, test_score: 0.90\n",
      "\n",
      "Epoch [89/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1182, train_score: 1.00, test_loss: 0.3517, test_score: 0.90\n",
      "\n",
      "Epoch [90/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1179, train_score: 1.00, test_loss: 0.3493, test_score: 0.90\n",
      "\n",
      "Epoch [91/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1176, train_score: 1.00, test_loss: 0.3470, test_score: 0.90\n",
      "\n",
      "Epoch [92/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1173, train_score: 1.00, test_loss: 0.3447, test_score: 0.92\n",
      "\n",
      "Epoch [93/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1170, train_score: 1.00, test_loss: 0.3425, test_score: 0.92\n",
      "\n",
      "Epoch [94/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1167, train_score: 1.00, test_loss: 0.3403, test_score: 0.92\n",
      "\n",
      "Epoch [95/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1164, train_score: 1.00, test_loss: 0.3381, test_score: 0.92\n",
      "\n",
      "Epoch [96/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1161, train_score: 1.00, test_loss: 0.3360, test_score: 0.92\n",
      "\n",
      "Epoch [97/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1158, train_score: 1.00, test_loss: 0.3339, test_score: 0.92\n",
      "\n",
      "Epoch [98/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1155, train_score: 1.00, test_loss: 0.3318, test_score: 0.92\n",
      "\n",
      "Epoch [99/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1152, train_score: 1.00, test_loss: 0.3298, test_score: 0.92\n",
      "\n",
      "Epoch [100/100]\n",
      "100/100 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1149, train_score: 1.00, test_loss: 0.3278, test_score: 0.92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    \"model\": LogisticModel(alpha=0.02, weight_decay=0),\n",
    "    \"epoch_num\": 100,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "process_bar = sqdm()\n",
    "def train(model, epoch_num, batch_size):\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, xtrain, ytrain):\n",
    "            model.fit(xdata, ydata)\n",
    "            \n",
    "            # train\n",
    "            train_pred = model.predict_prob(xdata)\n",
    "            train_loss = round(model.entropy_loss(train_pred, ydata.reshape(train_pred.shape)), 5)\n",
    "            train_acc = model.score(xdata, ydata)\n",
    "            \n",
    "            # test\n",
    "            test_pred = model.predict_prob(xtest)\n",
    "            test_loss = round(model.entropy_loss(test_pred, ytest.reshape(test_pred.shape)), 5)\n",
    "            test_acc = model.score(xtest, ytest)\n",
    "\n",
    "            process_bar.show_process(len(ytrain), batch_size, train_loss=train_loss, \n",
    "                                    test_loss=test_loss, train_score=train_acc, test_score=test_acc)\n",
    "            \n",
    "        print(\"\\n\")\n",
    "    return model\n",
    "    \n",
    "model = train(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T09:53:01.159438Z",
     "start_time": "2020-10-03T09:53:01.142886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集结果\n",
    "model.predict(iris_data[:, :4])\n",
    "model.score(iris_data[:, :4], iris_data[:, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax可以看成是一个多项的logistic，实际上softmax是一种条件最大熵模型\n",
    "- 对于某个样本属于第c类的概率为：\n",
    "    - $P(y=c|x) = \\frac{exp(w^T_cx)}{\\sum_{c'=1}^{C}exp(w^T_{c'}x)}$\n",
    "    - 决策函数为\n",
    "        - $\\hat y = \\underset{c}{arg min} \\  P(y=c|x) = \\underset{c}{arg min}\\  w^T_cx$\n",
    "- 损失函数：$L = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^{C} y_c^{(n)}log(\\hat y_c^{(n)})= - \\sum_{n=1}^N  (y^{(n)})^Tlog(\\hat y^{(n)})$\n",
    "- 梯度：\n",
    "    - $\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{n=1}^N x^{(n)}(\\hat y^{(n)} - y^{(n)})^T$\n",
    "    - $\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{n=1}^N (\\hat y^{(n)} - y^{(n)})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy版"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是先以鸢尾花的例子来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T11:06:23.156947Z",
     "start_time": "2020-10-03T11:06:23.134877Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理数据集\n",
    "iris = load_iris()\n",
    "iris_data = np.hstack((iris.data, np.expand_dims(iris.target, 1)))\n",
    "xtrain, ytrain, xtest, ytest = bootstrap(iris_data[:, :4], iris_data[:, 4])\n",
    "\n",
    "# 处理标签--> (0 --> [1, 0, 0])\n",
    "label_dict = {\n",
    "    0: [1, 0, 0],\n",
    "    1: [0, 1, 0],\n",
    "    2: [0, 0, 1]\n",
    "}\n",
    "\n",
    "data = np.array(list(map(lambda x: label_dict[x], iris_data[:, 4])))\n",
    "ytrain = np.array(list(map(lambda x: label_dict[x], list(ytrain))))\n",
    "ytest = np.array(list(map(lambda x: label_dict[x], list(ytest))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T11:06:26.553297Z",
     "start_time": "2020-10-03T11:06:26.539955Z"
    }
   },
   "outputs": [],
   "source": [
    "class SoftmaxModel(object):\n",
    "    \"\"\"实现softmax\"\"\"\n",
    "    def __init__(self, fea_num, cate_num, alpha=0.01, weight_decay=0):\n",
    "        self.w = np.zeros([fea_num, cate_num])\n",
    "        self.b = np.zeros(cate_num)\n",
    "        self.fea_num = fea_num\n",
    "        self.cate_num = cate_num\n",
    "        self.alpha = alpha\n",
    "        self.weight_decay = weight_decay\n",
    "        self.count = 0\n",
    "        \n",
    "    def linreg(self, X):\n",
    "        return X@self.w + self.b\n",
    "    \n",
    "    def softmax(self, y):\n",
    "        return np.exp(y)/np.expand_dims(np.exp(y).sum(axis = 1), 1)\n",
    "    \n",
    "    def entropy_loss(self, y_pred, y):\n",
    "        loss = -(y*np.log(y_pred)).sum()/len(y)\n",
    "        return loss\n",
    "    \n",
    "    def cal_grad(self, X, y_diff):\n",
    "        result = np.zeros([self.fea_num, self.cate_num])\n",
    "        for i in range(len(X)):\n",
    "            result += np.outer(X.T[:, i], y_diff[i, :])\n",
    "        return result / len(X)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # predict\n",
    "        y_pred = self.predict_prob(X)\n",
    "        \n",
    "        # update_grad\n",
    "        dw = self.cal_grad(X, (y_pred-y)) + self.weight_decay * self.w\n",
    "        db = (y_pred-y).sum(axis=0)/len(y) + self.weight_decay * self.b\n",
    "        self.w -= self.alpha * dw\n",
    "        self.b -= self.alpha * db\n",
    "        self.count += 1\n",
    "        \n",
    "            \n",
    "    def predict_prob(self, X):\n",
    "        y_pred = self.softmax(self.linreg(X))\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        pred_index = np.argmax(y_pred, axis=1)\n",
    "        return pred_index\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict_prob(X)\n",
    "        pred_index = np.argmax(y_pred, axis=1)\n",
    "        label_index = np.argmax(y, axis = 1)\n",
    "        acc = (pred_index == label_index).sum()/len(y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T11:06:50.876234Z",
     "start_time": "2020-10-03T11:06:29.911624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.3142, train_score: 1.00, test_loss: 0.7527, test_score: 0.57\n",
      "\n",
      "Epoch [2/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.2102, train_score: 1.00, test_loss: 0.6358, test_score: 0.57\n",
      "\n",
      "Epoch [3/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1617, train_score: 1.00, test_loss: 0.5729, test_score: 0.59\n",
      "\n",
      "Epoch [4/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1336, train_score: 1.00, test_loss: 0.5271, test_score: 0.76\n",
      "\n",
      "Epoch [5/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1153, train_score: 1.00, test_loss: 0.4900, test_score: 0.81\n",
      "\n",
      "Epoch [6/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.1026, train_score: 1.00, test_loss: 0.4585, test_score: 0.86\n",
      "\n",
      "Epoch [7/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0931, train_score: 1.00, test_loss: 0.4312, test_score: 0.91\n",
      "\n",
      "Epoch [8/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0858, train_score: 1.00, test_loss: 0.4072, test_score: 0.91\n",
      "\n",
      "Epoch [9/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0799, train_score: 1.00, test_loss: 0.3858, test_score: 0.91\n",
      "\n",
      "Epoch [10/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0749, train_score: 1.00, test_loss: 0.3667, test_score: 0.93\n",
      "\n",
      "Epoch [11/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0708, train_score: 1.00, test_loss: 0.3496, test_score: 0.93\n",
      "\n",
      "Epoch [12/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0671, train_score: 1.00, test_loss: 0.3341, test_score: 0.95\n",
      "\n",
      "Epoch [13/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0639, train_score: 1.00, test_loss: 0.3201, test_score: 0.95\n",
      "\n",
      "Epoch [14/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0610, train_score: 1.00, test_loss: 0.3073, test_score: 0.95\n",
      "\n",
      "Epoch [15/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0584, train_score: 1.00, test_loss: 0.2957, test_score: 0.95\n",
      "\n",
      "Epoch [16/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0560, train_score: 1.00, test_loss: 0.2850, test_score: 0.95\n",
      "\n",
      "Epoch [17/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0539, train_score: 1.00, test_loss: 0.2752, test_score: 0.97\n",
      "\n",
      "Epoch [18/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0519, train_score: 1.00, test_loss: 0.2662, test_score: 0.97\n",
      "\n",
      "Epoch [19/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0500, train_score: 1.00, test_loss: 0.2579, test_score: 0.97\n",
      "\n",
      "Epoch [20/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0483, train_score: 1.00, test_loss: 0.2502, test_score: 0.97\n",
      "\n",
      "Epoch [21/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0466, train_score: 1.00, test_loss: 0.2430, test_score: 0.97\n",
      "\n",
      "Epoch [22/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0451, train_score: 1.00, test_loss: 0.2364, test_score: 0.97\n",
      "\n",
      "Epoch [23/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0437, train_score: 1.00, test_loss: 0.2301, test_score: 0.97\n",
      "\n",
      "Epoch [24/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0423, train_score: 1.00, test_loss: 0.2243, test_score: 0.97\n",
      "\n",
      "Epoch [25/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0411, train_score: 1.00, test_loss: 0.2189, test_score: 0.97\n",
      "\n",
      "Epoch [26/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0399, train_score: 1.00, test_loss: 0.2138, test_score: 0.97\n",
      "\n",
      "Epoch [27/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0387, train_score: 1.00, test_loss: 0.2090, test_score: 0.97\n",
      "\n",
      "Epoch [28/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0377, train_score: 1.00, test_loss: 0.2045, test_score: 0.97\n",
      "\n",
      "Epoch [29/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0366, train_score: 1.00, test_loss: 0.2002, test_score: 0.97\n",
      "\n",
      "Epoch [30/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0357, train_score: 1.00, test_loss: 0.1962, test_score: 0.97\n",
      "\n",
      "Epoch [31/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0347, train_score: 1.00, test_loss: 0.1923, test_score: 0.97\n",
      "\n",
      "Epoch [32/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0339, train_score: 1.00, test_loss: 0.1887, test_score: 0.97\n",
      "\n",
      "Epoch [33/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0330, train_score: 1.00, test_loss: 0.1853, test_score: 0.97\n",
      "\n",
      "Epoch [34/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0322, train_score: 1.00, test_loss: 0.1820, test_score: 0.97\n",
      "\n",
      "Epoch [35/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0315, train_score: 1.00, test_loss: 0.1789, test_score: 0.97\n",
      "\n",
      "Epoch [36/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0307, train_score: 1.00, test_loss: 0.1760, test_score: 0.97\n",
      "\n",
      "Epoch [37/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0300, train_score: 1.00, test_loss: 0.1731, test_score: 0.97\n",
      "\n",
      "Epoch [38/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0294, train_score: 1.00, test_loss: 0.1704, test_score: 0.97\n",
      "\n",
      "Epoch [39/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0287, train_score: 1.00, test_loss: 0.1678, test_score: 0.97\n",
      "\n",
      "Epoch [40/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0281, train_score: 1.00, test_loss: 0.1654, test_score: 0.97\n",
      "\n",
      "Epoch [41/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0275, train_score: 1.00, test_loss: 0.1630, test_score: 0.97\n",
      "\n",
      "Epoch [42/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0269, train_score: 1.00, test_loss: 0.1607, test_score: 0.97\n",
      "\n",
      "Epoch [43/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0264, train_score: 1.00, test_loss: 0.1586, test_score: 0.97\n",
      "\n",
      "Epoch [44/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0259, train_score: 1.00, test_loss: 0.1565, test_score: 0.97\n",
      "\n",
      "Epoch [45/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0254, train_score: 1.00, test_loss: 0.1545, test_score: 0.97\n",
      "\n",
      "Epoch [46/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0249, train_score: 1.00, test_loss: 0.1525, test_score: 0.97\n",
      "\n",
      "Epoch [47/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0244, train_score: 1.00, test_loss: 0.1507, test_score: 0.97\n",
      "\n",
      "Epoch [48/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0239, train_score: 1.00, test_loss: 0.1489, test_score: 0.97\n",
      "\n",
      "Epoch [49/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0235, train_score: 1.00, test_loss: 0.1471, test_score: 0.97\n",
      "\n",
      "Epoch [50/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0231, train_score: 1.00, test_loss: 0.1455, test_score: 0.97\n",
      "\n",
      "Epoch [51/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0227, train_score: 1.00, test_loss: 0.1438, test_score: 0.97\n",
      "\n",
      "Epoch [52/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0223, train_score: 1.00, test_loss: 0.1423, test_score: 0.97\n",
      "\n",
      "Epoch [53/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0219, train_score: 1.00, test_loss: 0.1408, test_score: 0.97\n",
      "\n",
      "Epoch [54/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0215, train_score: 1.00, test_loss: 0.1393, test_score: 0.97\n",
      "\n",
      "Epoch [55/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0212, train_score: 1.00, test_loss: 0.1379, test_score: 0.97\n",
      "\n",
      "Epoch [56/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0208, train_score: 1.00, test_loss: 0.1365, test_score: 0.97\n",
      "\n",
      "Epoch [57/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0205, train_score: 1.00, test_loss: 0.1352, test_score: 0.97\n",
      "\n",
      "Epoch [58/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0201, train_score: 1.00, test_loss: 0.1339, test_score: 0.97\n",
      "\n",
      "Epoch [59/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0198, train_score: 1.00, test_loss: 0.1327, test_score: 0.97\n",
      "\n",
      "Epoch [60/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0195, train_score: 1.00, test_loss: 0.1314, test_score: 0.97\n",
      "\n",
      "Epoch [61/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0192, train_score: 1.00, test_loss: 0.1303, test_score: 0.97\n",
      "\n",
      "Epoch [62/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0189, train_score: 1.00, test_loss: 0.1291, test_score: 0.97\n",
      "\n",
      "Epoch [63/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0186, train_score: 1.00, test_loss: 0.1280, test_score: 0.97\n",
      "\n",
      "Epoch [64/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0184, train_score: 1.00, test_loss: 0.1269, test_score: 0.97\n",
      "\n",
      "Epoch [65/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0181, train_score: 1.00, test_loss: 0.1259, test_score: 0.97\n",
      "\n",
      "Epoch [66/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0178, train_score: 1.00, test_loss: 0.1249, test_score: 0.97\n",
      "\n",
      "Epoch [67/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0176, train_score: 1.00, test_loss: 0.1239, test_score: 0.97\n",
      "\n",
      "Epoch [68/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0173, train_score: 1.00, test_loss: 0.1229, test_score: 0.97\n",
      "\n",
      "Epoch [69/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0171, train_score: 1.00, test_loss: 0.1219, test_score: 0.97\n",
      "\n",
      "Epoch [70/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0169, train_score: 1.00, test_loss: 0.1210, test_score: 0.97\n",
      "\n",
      "Epoch [71/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0166, train_score: 1.00, test_loss: 0.1201, test_score: 0.97\n",
      "\n",
      "Epoch [72/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0164, train_score: 1.00, test_loss: 0.1192, test_score: 0.97\n",
      "\n",
      "Epoch [73/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0162, train_score: 1.00, test_loss: 0.1184, test_score: 0.97\n",
      "\n",
      "Epoch [74/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0160, train_score: 1.00, test_loss: 0.1175, test_score: 0.97\n",
      "\n",
      "Epoch [75/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0158, train_score: 1.00, test_loss: 0.1167, test_score: 0.97\n",
      "\n",
      "Epoch [76/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0156, train_score: 1.00, test_loss: 0.1159, test_score: 0.97\n",
      "\n",
      "Epoch [77/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0154, train_score: 1.00, test_loss: 0.1151, test_score: 0.97\n",
      "\n",
      "Epoch [78/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0152, train_score: 1.00, test_loss: 0.1144, test_score: 0.97\n",
      "\n",
      "Epoch [79/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0150, train_score: 1.00, test_loss: 0.1136, test_score: 0.97\n",
      "\n",
      "Epoch [80/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0148, train_score: 1.00, test_loss: 0.1129, test_score: 0.97\n",
      "\n",
      "Epoch [81/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0146, train_score: 1.00, test_loss: 0.1122, test_score: 0.97\n",
      "\n",
      "Epoch [82/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0145, train_score: 1.00, test_loss: 0.1115, test_score: 0.97\n",
      "\n",
      "Epoch [83/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0143, train_score: 1.00, test_loss: 0.1108, test_score: 0.97\n",
      "\n",
      "Epoch [84/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0141, train_score: 1.00, test_loss: 0.1101, test_score: 0.97\n",
      "\n",
      "Epoch [85/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0140, train_score: 1.00, test_loss: 0.1095, test_score: 0.97\n",
      "\n",
      "Epoch [86/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0138, train_score: 1.00, test_loss: 0.1088, test_score: 0.97\n",
      "\n",
      "Epoch [87/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0136, train_score: 1.00, test_loss: 0.1082, test_score: 0.97\n",
      "\n",
      "Epoch [88/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0135, train_score: 1.00, test_loss: 0.1076, test_score: 0.97\n",
      "\n",
      "Epoch [89/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0133, train_score: 1.00, test_loss: 0.1070, test_score: 0.97\n",
      "\n",
      "Epoch [90/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0132, train_score: 1.00, test_loss: 0.1064, test_score: 0.97\n",
      "\n",
      "Epoch [91/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0131, train_score: 1.00, test_loss: 0.1058, test_score: 0.97\n",
      "\n",
      "Epoch [92/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0129, train_score: 1.00, test_loss: 0.1053, test_score: 0.97\n",
      "\n",
      "Epoch [93/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0128, train_score: 1.00, test_loss: 0.1047, test_score: 0.97\n",
      "\n",
      "Epoch [94/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0126, train_score: 1.00, test_loss: 0.1042, test_score: 0.97\n",
      "\n",
      "Epoch [95/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0125, train_score: 1.00, test_loss: 0.1036, test_score: 0.97\n",
      "\n",
      "Epoch [96/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0124, train_score: 1.00, test_loss: 0.1031, test_score: 0.97\n",
      "\n",
      "Epoch [97/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0122, train_score: 1.00, test_loss: 0.1026, test_score: 0.97\n",
      "\n",
      "Epoch [98/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0121, train_score: 1.00, test_loss: 0.1021, test_score: 0.97\n",
      "\n",
      "Epoch [99/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0120, train_score: 1.00, test_loss: 0.1016, test_score: 0.97\n",
      "\n",
      "Epoch [100/100]\n",
      "150/150 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] - train_loss: 0.0119, train_score: 1.00, test_loss: 0.1011, test_score: 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    \"model\": SoftmaxModel(fea_num=4, cate_num=3, alpha=0.01, weight_decay=0),\n",
    "    \"epoch_num\": 100,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "process_bar = sqdm()\n",
    "def train(model, epoch_num, batch_size):\n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"Epoch [{epoch+1}/{epoch_num}]\")\n",
    "        for xdata, ydata in data_iter(batch_size, xtrain, ytrain):\n",
    "            model.fit(xdata, ydata)\n",
    "            \n",
    "            # train\n",
    "            train_pred = model.predict_prob(xdata)\n",
    "            train_loss = model.entropy_loss(train_pred, ydata.reshape(train_pred.shape))\n",
    "            train_acc = model.score(xdata, ydata)\n",
    "            \n",
    "            # test\n",
    "            test_pred = model.predict_prob(xtest)\n",
    "            test_loss = model.entropy_loss(test_pred, ytest.reshape(test_pred.shape))\n",
    "            test_acc = model.score(xtest, ytest)\n",
    "\n",
    "            process_bar.show_process(len(ytrain), batch_size, train_loss=train_loss, \n",
    "                                    test_loss=test_loss, train_score=train_acc, test_score=test_acc)\n",
    "            \n",
    "        print(\"\\n\")\n",
    "    return model\n",
    "    \n",
    "model = train(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-03T11:07:01.190533Z",
     "start_time": "2020-10-03T11:07:01.180981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集结果\n",
    "model.predict(iris_data[:, :4])\n",
    "model.score(iris_data[:, :4], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
